{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd110d0",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 16 â€“ CS 189, Fall 2025</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c629ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "colors = px.colors.qualitative.Plotly\n",
    "px.defaults.width = 800\n",
    "# from ipywidgets import HBox\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace174b",
   "metadata": {},
   "source": [
    "For this notebook we will use the CPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ff8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch._dynamo.disable() # fixing a bug with apple silicon support\n",
    "if torch.accelerator.is_available():\n",
    "    device = torch.accelerator.current_accelerator()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "print(\"Congratulations! You have access to a\", device, \"device.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6021d24",
   "metadata": {},
   "source": [
    "You can ignore the following visualization code.  This was added to track and update the loss and decision surface during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionBoundaryVisualizer:\n",
    "    def __init__(self, pred_fig, x1_range, x2_range, num_points=50, plot_probs=True):\n",
    "        self.pred_fig = pred_fig\n",
    "        self.plot_probs = plot_probs\n",
    "        x1_min, x1_max = x1_range\n",
    "        x2_min, x2_max = x2_range\n",
    "        margin_x1 = 0.5 * (x1_max - x1_min)\n",
    "        margin_x2 = 0.5 * (x2_max - x2_min)\n",
    "        x1_min -= margin_x1\n",
    "        x1_max += margin_x1\n",
    "        x2_min -= margin_x2\n",
    "        x2_max += margin_x2\n",
    "        # Setup the grid of test points for decision boundary plotting\n",
    "        x1, x2 = torch.meshgrid(torch.linspace(x1_min, x1_max, num_points),\n",
    "                                torch.linspace(x2_min, x2_max, num_points), indexing='ij')\n",
    "        self.grid = torch.cat([x1.reshape(-1, 1), x2.reshape(-1, 1)], dim=1)\n",
    "        self.x1 = to_numpy(x1)\n",
    "        self.x2 = to_numpy(x2)\n",
    "\n",
    "    def plot_decision_boundary(self, model):\n",
    "        with torch.no_grad():\n",
    "            preds = F.softmax(model(self.grid), dim=1)\n",
    "        num_classes = preds.shape[1]\n",
    "        if num_classes > 2:  # support for multiclass\n",
    "            preds = torch.argmax(preds, axis=1).reshape(self.x1.shape).T\n",
    "            preds = to_numpy(preds)\n",
    "            return go.Contour(x=self.x1[:, 0], y=self.x2[0], z=preds,\n",
    "                            #   contours=dict(start=0, end=num_classes, size=1),\n",
    "                              colorscale=px.colors.qualitative.Plotly[:num_classes],  \n",
    "                              contours=dict(start=-0.5, end=num_classes-0.5, size=1, coloring='fill'),\n",
    "                              opacity=0.5, showscale=False)\n",
    "        else:  # Binary classification case (red/blue)\n",
    "            if self.plot_probs:\n",
    "                preds = preds[:, 1].reshape(self.x1.shape).T\n",
    "            else:\n",
    "                preds = (preds[:, 1] > 0.5).astype(float).reshape(self.x1.shape).T\n",
    "            preds = to_numpy(preds)\n",
    "            return go.Contour(x=self.x1[:, 0], y=self.x2[0], z=preds,\n",
    "                              colorscale=[[0, 'blue'], [1, 'red']], \n",
    "                              #colorscale='Matter_r',\n",
    "                              opacity = 0.5, showscale=False)\n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "    def __call__(self, epoch, model, loss_fn):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            boundary = self.plot_decision_boundary(model)\n",
    "        # plotly batch update\n",
    "        with self.pred_fig.batch_update():\n",
    "            self.pred_fig.data[-1].z = boundary.z\n",
    "        model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7332e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossVisualizer:\n",
    "    def __init__(self, training_data, validation_data, loss_fig):\n",
    "        self.x_val, self.t_val = validation_data[:]\n",
    "        self.x_train, self.t_train = training_data[:]\n",
    "        self.loss_fig = loss_fig\n",
    "        self.epochs = []\n",
    "        self.losses_val = []\n",
    "        self.errors_val = []\n",
    "        self.losses_tr = []\n",
    "    def reset(self):\n",
    "        self.epochs = []\n",
    "        self.losses_val = []\n",
    "        self.errors_val = []\n",
    "        self.losses_tr = []\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = []\n",
    "            self.loss_fig.data[0].y = []\n",
    "            self.loss_fig.data[1].x = []\n",
    "            self.loss_fig.data[1].y = []\n",
    "    \n",
    "    def __call__(self, epoch, model, loss_fn):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = loss_fn(model(self.x_val), self.t_val).item()\n",
    "            loss_tr = loss_fn(model(self.x_train), self.t_train).item()\n",
    "            err_val = (model(self.x_val).argmax(dim=1) != self.t_val).float().mean().item()\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses_val.append(loss_val)\n",
    "        self.losses_tr.append(loss_tr)\n",
    "        self.errors_val.append(err_val)\n",
    "        # Visualization Code\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = self.epochs\n",
    "            self.loss_fig.data[0].y = self.losses_val\n",
    "            self.loss_fig.data[1].x = self.epochs\n",
    "            self.loss_fig.data[1].y = self.losses_tr\n",
    "            self.loss_fig.data[2].x = self.epochs\n",
    "            self.loss_fig.data[2].y = self.errors_val\n",
    "        model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3979e",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Weight Initialization</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9831635",
   "metadata": {},
   "source": [
    "The following depicts the sweet spot of each of the common activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ebe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-7, 7, 1000)\n",
    "df = pd.DataFrame(\n",
    "    {\"x\": x, \n",
    "     \"sigmoid(x)\": 1 / (1 + np.exp(-x)), \n",
    "     \"tanh(x)\": np.tanh(x),\n",
    "     \"Relu(x)\": np.maximum(0, x)})\n",
    "df = df.melt(id_vars=[\"x\"], \n",
    "            value_vars=[\"sigmoid(x)\", \"tanh(x)\", \"Relu(x)\"],\n",
    "                    var_name=\"activation\", value_name=\"value\")\n",
    "\n",
    "fig = px.line(df, x=\"x\", y=\"value\", facet_col=\"activation\")\n",
    "fig.update_layout(yaxis_range=[-1.5, 1.5])\n",
    "fig.update_traces(line=dict(width=5))\n",
    "# increase the subplot title font size\n",
    "fig.update_layout(\n",
    "    font=dict(size=24),\n",
    "    width=900, height=400,\n",
    "    # margin=dict(l=10, r=10, t=10, b=10)\n",
    ")\n",
    "# add the regions of transition\n",
    "fig.add_vrect(x0=-4, x1=4, line_width=0, fillcolor=\"LightGreen\", opacity=0.3,\n",
    "              annotation_text=\"Sweet Spot\", annotation_position=\"top left\",\n",
    "              row=1, col=1)\n",
    "fig.add_vrect(x0=-2, x1=2, line_width=0, fillcolor=\"LightGreen\", opacity=0.3,\n",
    "              annotation_text=\"Sweet Spot\", annotation_position=\"top left\",\n",
    "              row=1, col=2)\n",
    "fig.add_vrect(x0=0, x1=7, line_width=0, fillcolor=\"LightGreen\", opacity=0.3,\n",
    "              annotation_text=\"Sweet Spot\", annotation_position=\"top left\",\n",
    "              row=1, col=3)\n",
    "# remove the activate= from the titles\n",
    "for i in range(1, 4):\n",
    "    fig.layout.annotations[i-1].text = fig.layout.annotations[i-1].text.split(\"=\")[1] \n",
    "# fig.write_image(\"activation_functions.pdf\", width=900, height=400, scale=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e80d",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Implementing Xavier and He Initialization</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, dims, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n",
    "        )\n",
    "        self.act = nn.functional.relu if activation == \"relu\" else torch.tanh\n",
    "        self._reset_parameters()  \n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # Initialize weights according to activation function\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if not isinstance(layer, nn.Linear): \n",
    "                continue\n",
    "            if self.activation == \"relu\":\n",
    "                # He/Kaiming for ReLU\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                # Xavier for tanh\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers) - 1: # no activation on last layer\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5908c7",
   "metadata": {},
   "source": [
    "Let's try making a deep neural network and see if the initialization makes a difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fe7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(189)\n",
    "model = MLPModel(dims=[1] + 30 * [10] + [1], activation=\"tanh\")\n",
    "\n",
    "x_test = torch.linspace(-10, 10, 1000).unsqueeze(1)\n",
    "y = model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ddba2a",
   "metadata": {},
   "source": [
    "Notice small pre-activation values (close to 0) and the non-linearity centered around 0 for tanh and sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab71e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=x_test.squeeze().detach().numpy(), \n",
    "              y=y.squeeze().detach().numpy(), \n",
    "              title=\"Deep MLP Output with ReLU Activation and He Initialization\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3530d6a1",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Layer and Batch Normalization</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnorm = nn.BatchNorm1d(2)\n",
    "bnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnorm(torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca714e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bnorm.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bnorm.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnorm = nn.LayerNorm(2)\n",
    "lnorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf63245",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnorm(torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a465be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lnorm.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(lnorm.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998290d9",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Dropout Regularization</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb618852",
   "metadata": {},
   "source": [
    "PyTorch implements dropout as a layer that can be added to your model. During training, it randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. \n",
    "\n",
    "However, unlike the original paper description PyTorch implements inverted dropout, which scales the activations during training by $1/(1-\\rho)$ so that no scaling is needed at test time.  Here $\\rho$ is the probability of an element to be zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918358a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "dropout.train()\n",
    "display(dropout(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])))\n",
    "\n",
    "dropout.eval()\n",
    "display(dropout(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2962fe35",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Everything MLP Network</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_normalization_layer(norm_type, dim):\n",
    "    \"\"\"Creates a normalization layer based on the specified type.\"\"\"\n",
    "    norm_type = (norm_type or \"\").lower()\n",
    "    if norm_type == \"batch\":\n",
    "        return nn.BatchNorm1d(dim)\n",
    "    elif norm_type == \"layer\":\n",
    "        return nn.LayerNorm(dim)\n",
    "    else:\n",
    "        return nn.Identity()\n",
    "\n",
    "def make_activation_layer(activation):\n",
    "    \"\"\"Creates an activation layer based on the specified type.\"\"\"\n",
    "    activation = (activation or \"\").lower()\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif activation == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        return nn.Identity()\n",
    "\n",
    "def make_dropout_layer(dropout_prob):\n",
    "    \"\"\"Creates a dropout layer if dropout_prob > 0.\"\"\"\n",
    "    if dropout_prob is not None and dropout_prob > 0.0:\n",
    "        return nn.Dropout(p=dropout_prob)\n",
    "    return nn.Identity()\n",
    "\n",
    "def make_projection_layer(dim_in, dim_out):\n",
    "    \"\"\"Creates a projection layer if necessary to address dimension missmatch.\"\"\"\n",
    "    if dim_in != dim_out:\n",
    "        return nn.Linear(dim_in, dim_out, bias=False)\n",
    "    return nn.Identity()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, dims, activation=\"relu\", normalization=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        # Build the networks as a sequence of layers\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(len(dims)-2):\n",
    "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            self.layers.append(make_normalization_layer(normalization, dims[i+1]))\n",
    "            self.layers.append(make_activation_layer(activation))\n",
    "            self.layers.append(make_dropout_layer(dropout))\n",
    "        # final layer without activation or normalization\n",
    "        self.head = nn.Linear(dims[-2], dims[-1])\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for layer in (m for m in self.layers if isinstance(m, nn.Linear)):\n",
    "            if self.activation == \"relu\":\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "            else:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.xavier_uniform_(self.head.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, activation=\"relu\", normalization=\"none\", dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.activation = (activation or \"\").lower()\n",
    "        self.ff_net = nn.Sequential(\n",
    "            make_normalization_layer(normalization, dim_in),\n",
    "            make_activation_layer(activation),\n",
    "            make_dropout_layer(dropout),\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "        )\n",
    "        self.projection = make_projection_layer(dim_in, dim_out)\n",
    "        self._reset_parameters() \n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # init residual branch linear(s)\n",
    "        for m in self.ff_net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if self.activation == \"relu\":\n",
    "                    nn.init.kaiming_uniform_(m.weight, a=0.0, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                else:\n",
    "                    gain = nn.init.calculate_gain(\"tanh\") if self.activation == \"tanh\" else 1.0\n",
    "                    nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        # init projection (linear skip) if present\n",
    "        if isinstance(self.projection, nn.Linear):\n",
    "            nn.init.xavier_uniform_(self.projection.weight, gain=1.0)   # linear mapping\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x) + self.ff_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualNetwork(nn.Module):\n",
    "    def __init__(self, dims, activation=\"relu\", normalization=None, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.dims = dims        \n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.drop_out = dropout\n",
    "        # Build the networks as a sequence of layers\n",
    "        self.layers = nn.Sequential()\n",
    "        for i in range(len(dims)-2):\n",
    "            self.layers.append(ResidualBlock(dims[i], dims[i+1], activation, normalization, dropout))\n",
    "        # final layer without activation or normalization\n",
    "        self.head = nn.Linear(dims[-2], dims[-1])\n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.head.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51ada0",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Playing with the Network</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaea4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets_from_numpy(x, t, split_ratio=0.8, seed=189):\n",
    "    \"\"\"Create training and test datasets from numpy data arrays.\"\"\"\n",
    "    from torch.utils.data import random_split, TensorDataset\n",
    "    data = TensorDataset(\n",
    "        torch.tensor(x, dtype=torch.float32, device=device), \n",
    "        torch.tensor(t, dtype=torch.long, device=device),\n",
    "    )\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    train_data, test_data = random_split(\n",
    "        data, [split_ratio, 1 - split_ratio], \n",
    "        generator=generator) \n",
    "    return train_data, test_data\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"Convert a PyTorch tensor to a NumPy array.\"\"\"\n",
    "    return tensor.cpu().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74229863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "n = 128\n",
    "# centers = 5 # number of classes\n",
    "# x, t = make_blobs(n_samples=n, centers=centers, cluster_std=1.5, random_state=36)\n",
    "# centers = 2\n",
    "# x, t = make_moons(n_samples=n, noise=0.2, random_state=42)\n",
    "centers = 2\n",
    "x, t = make_circles(n_samples=n, noise=0.2, factor=0.5, random_state=42)\n",
    "\n",
    "training_data, validation_data = make_datasets_from_numpy(x, t, split_ratio=0.5, seed=189)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[[\"x1\", \"x2\"]] = to_numpy(torch.vstack([training_data[:][0], validation_data[:][0]]))\n",
    "data[\"t\"] = to_numpy(torch.hstack([training_data[:][1], validation_data[:][1]])).astype(str)\n",
    "data[\"kind\"] = [\"train\"] * len(training_data) + [\"val\"] * len(validation_data)\n",
    "data_fig = px.scatter(data, x=\"x1\", y=\"x2\", color=\"t\", symbol=\"kind\",\n",
    "                 category_orders={'t': [str(i) for i in range(centers)]},\n",
    "                 labels={'x1': 'Feature 1', 'x2': 'Feature 2', 'color': 'Class'},\n",
    "                 width=500, height=500)\n",
    "data_fig.update_traces(marker=dict(size=10, line=dict(width=1, color=\"DarkSlateGrey\")))\n",
    "data_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa29816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gd(model, loss_fn, \n",
    "                 training_data,\n",
    "                 batch_size, \n",
    "                 nepochs, \n",
    "                 learning_rate,\n",
    "                 visualizer=None,\n",
    "                 weight_decay=1e-4):\n",
    "    \n",
    "    # Create a dataloader for training\n",
    "    from torch.utils.data import DataLoader\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(189)\n",
    "    loader = DataLoader(training_data, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, # shuffles each epoch\n",
    "                        generator=generator)\n",
    "    \n",
    "    # Define the optimizer (this is the update rule)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "    # Alternatively, you can use Adam optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Loop through the epochs\n",
    "    for epoch in range(nepochs):\n",
    "        # Loop through all the batches\n",
    "        for (x, t) in loader:\n",
    "            # Zero the gradients to start the next step\n",
    "            optimizer.zero_grad()\n",
    "            # Compute prediction and loss\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, t)\n",
    "            # Backpropagation (compute the gradient)\n",
    "            loss.backward()\n",
    "            # Update the parameters using the optimizer's update rule\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Visualize the model (if a visualizer function is provided)\n",
    "        if visualizer is not None:\n",
    "            model.eval() # disable dropout/batchnorm\n",
    "            with torch.no_grad():\n",
    "                visualizer(epoch, model, loss_fn)\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403db1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive, IntSlider, FloatSlider, Dropdown, HBox, VBox, Layout\n",
    "\n",
    "model = MLPModel(dims=[2, centers], activation=\"relu\")\n",
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "# hide the color scale for the contour plot\n",
    "pred_fig.update_layout(coloraxis_showscale=False, \n",
    "                       margin=dict(l=10, r=10, t=10, b=10),\n",
    "                       height=400, width=500)\n",
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Val. Loss'))\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train. Loss'))\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Val. Accuracy'))\n",
    "loss_fig.update_layout(\n",
    "    height=300, xaxis_title='Epochs', \n",
    "    margin=dict(l=10, r=10, t=10, b=10),\n",
    "    yaxis_title='Loss (Cross Entropy)')\n",
    "x_val, t_val = validation_data[:]\n",
    "loss_visualizer = LossVisualizer(\n",
    "    training_data=training_data,\n",
    "    validation_data=validation_data,\n",
    "    loss_fig=loss_fig)\n",
    "boundary_visualizer = DecisionBoundaryVisualizer(\n",
    "    pred_fig=pred_fig,\n",
    "    x1_range=(to_numpy(x_val[:, 0]).min(), to_numpy(x_val[:, 0]).max()),\n",
    "    x2_range=(to_numpy(x_val[:, 1]).min(), to_numpy(x_val[:, 1]).max()),\n",
    "    num_points=50,\n",
    "    plot_probs=True)\n",
    "pred_fig.add_trace(boundary_visualizer.plot_decision_boundary(model))\n",
    "\n",
    "\n",
    "def update_model(\n",
    "        model_type,\n",
    "        n_layers, neurons_per_layer, activation_fn, normalization, dropout, learning_rate, \n",
    "        weight_decay, batch_size, epochs):\n",
    "    # setup the model\n",
    "    layers = [2] + ([neurons_per_layer] * n_layers) + [centers]\n",
    "    if model_type == 'MLP':\n",
    "        model = MLPModel(dims=layers, activation=activation_fn, normalization=normalization, dropout=dropout)\n",
    "    elif model_type == 'ResNet':\n",
    "        model = ResidualNetwork(dims=layers, activation=activation_fn, normalization=normalization, dropout=dropout)\n",
    "    loss_visualizer.reset()\n",
    "    minibatch_gd(\n",
    "        model=model, \n",
    "        loss_fn=nn.CrossEntropyLoss(), \n",
    "        training_data=training_data,\n",
    "        batch_size=batch_size, \n",
    "        nepochs=epochs, \n",
    "        learning_rate=learning_rate,\n",
    "        visualizer=lambda epoch, model, loss_fn: [\n",
    "            boundary_visualizer(epoch, model, loss_fn),\n",
    "            loss_visualizer(epoch, model, loss_fn)\n",
    "        ],\n",
    "        weight_decay=weight_decay)\n",
    "\n",
    "controls = interactive(\n",
    "    update_model, \n",
    "    model_type=Dropdown(options=['MLP', 'ResNet'], value='MLP', description=\"Model Type\", continuous_update=False),\n",
    "    n_layers=IntSlider(min=1, max=20, step=1, value=5, description=\"Layers\", continuous_update=False),\n",
    "    neurons_per_layer=IntSlider(min=4, max=128, step=1, value=10, description=\"Neurons/Layer\", continuous_update=False),\n",
    "    activation_fn=Dropdown(options=['relu', 'tanh',], value='tanh', description=\"Activation\", continuous_update=False),\n",
    "    normalization=Dropdown(options=['batch', 'layer', 'none'], value='none', description=\"Normalization\", continuous_update=False),\n",
    "    dropout=FloatSlider(min=0, max=0.9, step=0.1, value=0.0, description=\"Dropout\", continuous_update=False),\n",
    "    learning_rate=FloatSlider(min=0.00001, max=.1, step=0.0005, value=0.001, description=\"Learning Rate\", continuous_update=False),\n",
    "    weight_decay=FloatSlider(min=1e-7, max=1, step=1e-5, value=1e-4, description=\"Weight Decay\", continuous_update=False),\n",
    "    batch_size=IntSlider(min=1, max=128, step=8, value=32, description=\"Batch Size\", continuous_update=False),\n",
    "    epochs=IntSlider(min=10, max=500, step=10, value=100, description=\"Epochs\", continuous_update=False))\n",
    "display(VBox([HBox([controls, pred_fig,], layout=Layout(align_items=\"center\")), loss_fig]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f77fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
