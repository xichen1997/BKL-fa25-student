{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd110d0",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 14 – CS 189, Fall 2025</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c629ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "colors = px.colors.qualitative.Plotly\n",
    "px.defaults.width = 800\n",
    "# from ipywidgets import HBox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042c400",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Basics of Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2e336",
   "metadata": {},
   "source": [
    "Neurons pass information from one to another using action potentials. They connect with one another at synapses, which are junctions between one neuron's axon and another's dendrite. Information flows from:\n",
    "\n",
    "1.  The dendrites,\n",
    "2.  To the cell body,\n",
    "3.  Through the axons,\n",
    "4.  To a synapse connecting the axon to the dendrite of the next neuron.\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w02/imgs/neuron.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e57c2cd",
   "metadata": {},
   "source": [
    "\n",
    "An Artificial Neuron is a mathematical function with the following elements:\n",
    "\n",
    "\n",
    "1.   Input\n",
    "2.   Weighted summation of inputs\n",
    "3.   Processing unit of activation function\n",
    "4.   Output\n",
    "\n",
    "<img src=\"https://www.cs.toronto.edu/~lczhang/360/lec/w02/imgs/neuron_model.jpeg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d7413",
   "metadata": {},
   "source": [
    "The mathematical equation for an artificial is as follows:\n",
    "\n",
    "\\begin{align}\n",
    "        \\hat{y} = f(\\vec{\\mathbf{\\theta}} \\cdot \\vec{\\mathbf{x}}) &= f\\left(\\sum_{i=0}^D w_i x_i\\right) \\\\\n",
    "        &= f(w_0 + w_1 x_1 + ... + w_Dx_D).\n",
    "\\end{align}\n",
    "\n",
    "Assuming that function $f$ is the logistic or sigmoid function, the output of the neuron has a probability value ($0 \\leq p \\leq 1$). This probability value can then be used for a binary classification task where $p < 0.5$ is an indication of class $0$, and $p \\geq 0.5$ assigns data to class 1. Re-writing the equation above with a sigmoid activation function would give us the following:\n",
    "\n",
    "\\begin{align}\n",
    "        \\hat{y} = σ(\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}}) &= \\sigma\\left(\\sum_{i=0}^d w_i x_i\\right) \\\\\n",
    "        &= σ(w_0 + w_1 x_1 + ... + w_Dx_D).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The code below contains an implementation of AND, OR, and XOR gates. You will be able to generate data for each of the functions and add the desired noise level to the data. Familiarize yourself with the code and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ef6651",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">The Logic Gate Problem</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d623a4d",
   "metadata": {},
   "source": [
    "\n",
    "We'll start by generating synthetic data for a logic gate (e.g., AND, OR, XOR) with Gaussian noise. This data will be used for training and testing the logistic regression model.\n",
    "The function `generate_data_with_noise` allows customization of the number of samples, the logic gate, and the noise level.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6911e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a dataset with multiple samples per gate location\n",
    "def generate_data_with_noise(num_samples = 500, gate = \"AND\", noise_level = 0.05):\n",
    "    \"\"\"\n",
    "    Generate multiple samples per logic gate configuration with added noise.\n",
    "    \"\"\"\n",
    "    if gate == 'AND':\n",
    "        base_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        base_t = np.array([0, 0, 0, 1])\n",
    "    elif gate == 'OR':\n",
    "        base_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        base_t = np.array([0, 1, 1, 1])\n",
    "    elif gate == 'XOR':\n",
    "        base_X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "        base_t = np.array([0, 1, 1, 0])\n",
    "    else:\n",
    "        raise ValueError(\"Gate must be 'AND', 'OR', or 'XOR'.\")\n",
    "    \n",
    "    # Repeat each base configuration to create multiple samples\n",
    "    X = np.repeat(base_X, num_samples // len(base_X), axis=0)\n",
    "    t = np.repeat(base_t, num_samples // len(base_t), axis=0)\n",
    "    \n",
    "    # Add Gaussian noise to the inputs\n",
    "    X = X + np.random.normal(0, noise_level, X.shape)\n",
    "    \n",
    "    # Shuffle the dataset to avoid ordered samples\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    t = t[indices]\n",
    "    \n",
    "    return X, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34846f6a",
   "metadata": {},
   "source": [
    "In this lecture we will use interactive visualizations.  These require a python environment so if you are viewing this notebook through the static HTML version you won't be able to use the interactive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c45f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, Dropdown, Checkbox\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "X, t = generate_data_with_noise(500, 'AND', 0.05)\n",
    "# Make an interactive plot\n",
    "data_fig = go.FigureWidget()\n",
    "data_fig.add_trace(go.Scatter(x=X[t == 0, 0], y=X[t == 0, 1], mode='markers', \n",
    "                              marker=dict(color='red', opacity=0.7, size=7, \n",
    "                                          line=dict(width=1, color='DarkSlateGrey')), \n",
    "                              name='0'))   \n",
    "data_fig.add_trace(go.Scatter(x=X[t == 1, 0], y=X[t == 1, 1], mode='markers', \n",
    "                              marker=dict(color='blue', opacity=0.7, size=7, \n",
    "                                          line=dict(width=1, color='DarkSlateGrey')), \n",
    "                              name='1'))  \n",
    "data_fig.update_layout(width=800, height=500,\n",
    "                       xaxis_range=[-1, 2], yaxis_range=[-1, 2])\n",
    "# The following code defines a set of interactive widgets (sliders)\n",
    "# and binds them to an update function that will be run whenever\n",
    "# a slider is changed.\n",
    "@interact(num_samples=FloatSlider(min=100, max=1000, step=100, value=500, description='Samples'),\n",
    "          gate=Dropdown(options=['AND', 'OR', 'XOR'], value='AND', description='Gate'),\n",
    "          noise_level=FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05, description='Noise Level'))\n",
    "def update_data_plot(num_samples, gate, noise_level):\n",
    "    X, t = generate_data_with_noise(num_samples, gate, noise_level)\n",
    "    with data_fig.batch_update():\n",
    "        data_fig.data[0].x = X[t == 0, 0]\n",
    "        data_fig.data[0].y = X[t == 0, 1]\n",
    "        data_fig.data[1].x = X[t == 1, 0]\n",
    "        data_fig.data[1].y = X[t == 1, 1]   \n",
    "        data_fig.update_layout(title=f\"Dataset for {gate} Gate with Noise Level {noise_level}\")\n",
    "display(data_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d524ad",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Solving with Logistic Regression</h3>\n",
    "\n",
    "This section demonstrates how to perform logistic regression using the scikit-learn library. The dataset is divided into training and testing subsets using an 80%-20% ratio with the train_test_split function. A logistic regression model is instantiated and trained on the training dataset using the `.fit()` method. The model's performance is evaluated on both the training and testing data using the `.score()` method, which computes accuracy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c57ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Logistic Regression Using Scikit-learn\n",
    "def perform_logistic_regression(X, t):\n",
    "    # Split the data\n",
    "    x_train, x_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=140)\n",
    "    \n",
    "    # Train a logistic regression model\n",
    "    model = LogisticRegression().fit(x_train, t_train)\n",
    "\n",
    "    # Print training and testing scores\n",
    "    train_error = model.score(x_train, t_train)\n",
    "    test_error = model.score(x_test, t_test)\n",
    "    \n",
    "    return model, train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d8789",
   "metadata": {},
   "source": [
    "We can plot a decision boundary or even a decision surface by plotting predictions on a regular grid of points.  This is accomplished using the meshgrid function from numpy.  We can then use the model to predict the class of each point on the grid and plot the results.  This is a useful way to visualize the decision boundary of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f128bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, x1_range, x2_range, num_points=100, probs=True):\n",
    "    # Generate a grid of points\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1_range[0], x1_range[1], num_points),\n",
    "                         np.linspace(x2_range[0], x2_range[1], num_points))\n",
    "    grid = np.c_[x1.ravel(), x2.ravel()]\n",
    "    # Get predictions for the grid\n",
    "    if probs:\n",
    "        preds = model.predict_proba(grid)[:,1].reshape(x1.shape)\n",
    "    else:\n",
    "        preds = model.predict(grid).reshape(x1.shape)\n",
    "    return go.Contour(x=x1[0], y=x2[:, 0], z=preds, colorscale=[[0, 'red'], [1, 'blue']], \n",
    "                      opacity = 0.5, showscale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc37b2a",
   "metadata": {},
   "source": [
    "Again we create an interactive visualization plot:            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "\n",
    "model, train_test, test_error = perform_logistic_regression(X, t)\n",
    "boundary = plot_decision_boundary(model, [-1, 2], [-1, 2], probs=False)\n",
    "pred_fig.add_trace(boundary)\n",
    "\n",
    "@interact(num_samples=FloatSlider(min=100, max=1000, step=100, value=500, description='Samples'),\n",
    "          gate=Dropdown(options=['AND', 'OR', 'XOR'], value='AND', description='Gate'),\n",
    "          noise_level=FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05, description='Noise Level'),\n",
    "          show_probs=Checkbox(value=False, description='Show Probabilities'))\n",
    "def update_pred_fig(num_samples, gate, noise_level, show_probs):\n",
    "    np.random.seed(42)\n",
    "    X, y = generate_data_with_noise(num_samples, gate, noise_level)\n",
    "    model, train_error, test_error = perform_logistic_regression(X, y)\n",
    "    boundary = plot_decision_boundary(model, [-1, 2], [-1, 2], probs=show_probs)\n",
    "    with pred_fig.batch_update():\n",
    "        pred_fig.data[0].x = X[y == 0, 0]\n",
    "        pred_fig.data[0].y = X[y == 0, 1]\n",
    "        pred_fig.data[1].x = X[y == 1, 0]\n",
    "        pred_fig.data[1].y = X[y == 1, 1]   \n",
    "    pred_fig.data[2].update(z=boundary.z)\n",
    "    title = f\"Predictions for {gate} Gate with Noise Level {noise_level} (Train: {train_error:.2f}, Test: {test_error:.2f})\"\n",
    "    pred_fig.update_layout(title=title)\n",
    "display(pred_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7eaad2",
   "metadata": {},
   "source": [
    "Let's focus on the XOR data and develop a neural network to classify it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = generate_data_with_noise(500, 'XOR', 0.15)\n",
    "\n",
    "data = pd.DataFrame({'x1': X[:, 0], 'x2': X[:, 1], 't': t})\n",
    "\n",
    "data_fig = go.Figure()\n",
    "data_fig.add_trace(go.Scatter(x=X[t == 0, 0], y=X[t == 0, 1], mode='markers', \n",
    "                              marker=dict(color='red', opacity=0.7, size=7, \n",
    "                                          line=dict(width=1, color='DarkSlateGrey')), \n",
    "                              name='0'))   \n",
    "data_fig.add_trace(go.Scatter(x=X[t == 1, 0], y=X[t == 1, 1], mode='markers', \n",
    "                              marker=dict(color='blue', opacity=0.7, size=7, \n",
    "                                          line=dict(width=1, color='DarkSlateGrey')), \n",
    "                              name='1'))  \n",
    "data_fig.update_layout(width=600, height=500,\n",
    "                       xaxis_range=[-1, 2], yaxis_range=[-1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecaa07",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Using PyTorch</h2>\n",
    "\n",
    "PyTorch provides a flexible and efficient platform for building and training neural networks. PyTorch is known for its intuitive python centric approach to defining neural networks.\n",
    "\n",
    "There are a few core concepts that are important to understand when using PyTorch:\n",
    "\n",
    "1. **Tensors**: Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with additional capabilities for GPU acceleration. They can be used to represent inputs, outputs, and parameters of neural networks.\n",
    "1. **DataSets and DataLoaders**: PyTorch provides utilities to handle data loading and batching through `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
    "1. **nn.Module**: This is the base class for all neural network modules in PyTorch. You can define your own models by subclassing `nn.Module` and defining the layers and forward pass.\n",
    "1. **Optimizers**: PyTorch provides various optimization algorithms (like SGD, Adam, etc.) in the `torch.optim` module to update the model parameters based on the computed gradients.\n",
    "1. **Autograd**: PyTorch's automatic differentiation library that tracks operations on tensors to compute gradients for backpropagation. (Focus of next lecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb3524",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Importing Torch</h3>\n",
    "\n",
    "When we import PyTorch (called torch for historical reasons), we typically import the main torch library and the `torch.nn` module, which contains classes and functions for building neural networks. We often alias `torch.nn` as `nn` for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc032a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706ae16",
   "metadata": {},
   "source": [
    "One of the new features in torch is the `torch._dynamo` module, which is used to optimize the execution of PyTorch models. However, there are some known issues with `torch._dynamo` on certain hardware, such as Apple Silicon (M1/M2 chips). To avoid potential bugs or performance issues when running on Apple Silicon, we can disable `torch._dynamo` using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.disable() # fixing a bug with apple silicon support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc1a4b",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Tensors</h3>\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with additional capabilities for GPU acceleration. They can be used to represent inputs, outputs, and parameters of neural networks.\n",
    "\n",
    "We can create tensors in various ways, such as from lists or NumPy arrays, or by using built-in functions to create tensors filled with zeros, ones, or random values. Here are some examples of creating tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c27e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor from a list\n",
    "torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29344b6e",
   "metadata": {},
   "source": [
    "We can set the data type of a tensor using the `dtype` argument when creating it. Common data types include `torch.float32`, `torch.int64`, etc.  Because we will work with hardware accelerators, it is important to be explicit about the data type of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ea5e7",
   "metadata": {},
   "source": [
    "Most accelerators will operate in float32 precision, so we will use `torch.float32` for our input data and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139783aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((2, 3)).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8df211",
   "metadata": {},
   "source": [
    "Tensors generally inherit the data type of the input data, but it is good practice to be explicit about the data type when creating tensors, especially when working with hardware accelerators like GPUs or TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(data['t'])\n",
    "print(t.dtype)\n",
    "t = t.to(torch.float32)\n",
    "print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914294aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor(data['t'], dtype=torch.float32)\n",
    "print(t.dtype)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8d87d",
   "metadata": {},
   "source": [
    "We can create data using other constructors, such as `torch.zeros`, `torch.ones`, and `torch.rand`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor filled with zeros\n",
    "torch.zeros((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor filled with ones\n",
    "torch.ones((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb982734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tensor with random values\n",
    "torch.rand((2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b69c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "torch.rand((2, 2), generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae47ec",
   "metadata": {},
   "source": [
    "We can manipulate tensors using various operations, such as addition, multiplication, and matrix multiplication. Here are some examples of tensor operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "t = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "x @ t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b02e5",
   "metadata": {},
   "source": [
    "We can slice and index tensors similarly to NumPy arrays. Here are some examples of slicing and indexing tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10117882",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9712511",
   "metadata": {},
   "source": [
    "Moving between numpy arrays and torch tensors is easy. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4443b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.random.randn(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd9ea8",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Working with Hardware Accelerators</h4>\n",
    "\n",
    "Pytorch allows us to easily work with hardware accelerators like GPUs. This is particularly useful for training large neural networks, as it can significantly speed up the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    device = torch.accelerator.current_accelerator()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Congratulations! You have access to a\", device, \"device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee0a45",
   "metadata": {},
   "source": [
    "We can move tensors to the appropriate device (CPU or GPU) using the `.to(device)` method. Here is an example of how to check for available hardware accelerators and set the default device accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df58372",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "x = torch.randn(3,2, device=device, generator=generator)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5ad54",
   "metadata": {},
   "source": [
    "We can also move back to the CPU if needed using the same `.to(\"cpu\")` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a29591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea3e18",
   "metadata": {},
   "source": [
    "You cannot perform numpy operations on tensors that are on a device. You must first move them back to the CPU using the `.cpu()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc00296",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94aa53f",
   "metadata": {},
   "source": [
    "Here is a simple function to convert tensors to numpy arrays safely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "882ce366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    \"\"\"Convert a PyTorch tensor to a NumPy array.\"\"\"\n",
    "    return tensor.cpu().numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96875ee9",
   "metadata": {},
   "source": [
    "You can also set the default device for all tensors using `torch.set_default_device(device)`. This way, any new tensor created will automatically be on the specified device. This is particularly useful when you want to ensure that all tensors are on the same device without having to specify the device each time you create a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddfe0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446a22e",
   "metadata": {},
   "source": [
    "Let's convert our synthetic xor data to a tensor and move it to the appropriate device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83cf5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(data[['x1','x2']].values, dtype=torch.float32, device=device)\n",
    "t = torch.tensor(data['t'].values, dtype=torch.float32, device=device)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b933adc",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Datasets</h3>\n",
    "\n",
    "PyTorch uses Datasets to represent the collections of training data.  PyTorch provides a convenient way to create datasets using the `torch.utils.data.TensorDataset` class. This class allows us to combine our input features and labels into a single dataset object, which can then be used for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(data[['x1','x2']].values, dtype=torch.float32), \n",
    "    torch.tensor(data['t'].values, dtype=torch.float32))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb6ae2",
   "metadata": {},
   "source": [
    "We can access the dataset entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b8ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t = dataset[1:5]\n",
    "x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd1f4c",
   "metadata": {},
   "source": [
    "There are useful utilities to manipulate datasets, such as constructing the train-validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b797670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(189)\n",
    "\n",
    "train_data, validation_data = random_split(\n",
    "    dataset, [.8, .2], \n",
    "    generator=generator) \n",
    "\n",
    "len(train_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598d3e1",
   "metadata": {},
   "source": [
    "Here we make a dataset builder function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91396d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets_from_numpy(x, t, split_ratio=0.8, seed=189):\n",
    "    \"\"\"Create training and test datasets from numpy data arrays.\"\"\"\n",
    "    from torch.utils.data import random_split, TensorDataset\n",
    "    data = TensorDataset(\n",
    "        torch.tensor(x, dtype=torch.float32, device=device), \n",
    "        torch.tensor(t, dtype=torch.long, device=device),\n",
    "    )\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    train_data, test_data = random_split(\n",
    "        data, [split_ratio, 1 - split_ratio], \n",
    "        generator=generator) \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aecb685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = make_datasets_from_numpy(\n",
    "    data[['x1','x2']].values, data['t'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d537d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, t) = training_data[:]\n",
    "x, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c073f",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Defining the Logistic Regression Model (Neural Network)</h2>\n",
    "\n",
    "Now that we have our data in a suitable format, we can define our logistic regression model using PyTorch's `nn.Module` class. This class provides a convenient way to define the layers and forward pass of our model.\n",
    "\n",
    "To build a new model (or model component), we create a new class that inherits from `nn.Module`. In the `__init__` method, we define the layers of our model. For logistic regression, we typically have a single linear layer that maps the input features to the output. We also need to implement a `forward` method that defines how the input data flows through the model to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e29c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(input_dim, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(x @ self.w + self.b)\n",
    "        return torch.cat([1 - p, p], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3108d04",
   "metadata": {},
   "source": [
    "When defining the `__init__` method of our model, we create nn.Parameters for the weights and biases of the linear layer. These parameters are automatically registered with the model, and PyTorch will handle their initialization and optimization during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f31037",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter(torch.zeros(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e31be",
   "metadata": {},
   "source": [
    "We can then create an new instance of our model and print its architecture to verify that it has been defined correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18168e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.w)\n",
    "print(model.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e263f",
   "metadata": {},
   "source": [
    "We make predictions by either invoking forward or simply applying the model instance to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695caeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(x=x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adf3ab",
   "metadata": {},
   "source": [
    "Moving a model, moves its parameters to the specified device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu().w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2972e",
   "metadata": {},
   "source": [
    "Linear models are so common that PyTorch provides a built-in `nn.Linear` layer that encapsulates the weight and bias parameters, as well as the forward pass. We could use this built-in layer instead of manually defining the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "614c2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        p = torch.sigmoid(self.linear(x))\n",
    "        return torch.cat([1 - p, p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_dim=2)\n",
    "print(model)\n",
    "print(model.linear.weight)\n",
    "print(model.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a9587",
   "metadata": {},
   "source": [
    "Notice that the model weights are not initialized to zero. This is important for breaking symmetry during training and ensuring that the model can learn effectively. We will discuss weight initialization strategies in more detail in a future lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4853de60",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Defining the Loss Function</h2>\n",
    "\n",
    "The Loss function quantifies how well the model's predictions match the true labels. For binary classification tasks like logistic regression, we typically use the binary cross-entropy loss function. PyTorch provides a built-in loss function for this purpose called `nn.BCELoss`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "422720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f42538",
   "metadata": {},
   "source": [
    "The binary cross-entropy loss function computes the loss between the predicted probabilities of class 1 and the true binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model(x[0:1])[:,1], t[0:1].float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e702a",
   "metadata": {},
   "source": [
    "The more general cross entropy loss works for multi-class classification problems, where there are more than two classes. It is implemented in PyTorch as `nn.CrossEntropyLoss`. \n",
    "\n",
    "**Important:** This loss function combines the softmax activation and the negative log-likelihood loss into a single function, making it suitable for multi-class classification tasks.  This behavior of combining steps is actually common with neural network components in PyTorch.  This enables more efficient computation and numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86b95bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b7e58",
   "metadata": {},
   "source": [
    "We can redefine our linear model to return raw logits instead of probabilities, which is what `nn.CrossEntropyLoss` expects as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83c0e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef638b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassLinearModel(input_dim=2, output_dim=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bef782",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(model(x[:5]), t[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2628fb",
   "metadata": {},
   "source": [
    "This should be the same as computing the softmax and then applying the negative log likelihood loss separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53980bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(model(x[:5]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdb14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = nn.NLLLoss()\n",
    "nll(F.log_softmax(model(x[:5]), dim=1), t[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edefad",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Automatic Differentiation</h3>\n",
    "\n",
    "Automatic differentiation (AD) is a set of techniques to evaluate the derivative of a function specified by a computer program. PyTorch provides a powerful automatic differentiation library called Autograd, which allows us to compute gradients of tensors with respect to some scalar value (usually the loss) automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "pred = model(x)\n",
    "loss = loss_fn(pred, t)\n",
    "\n",
    "print(model.linear.weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd1f49f",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Optimizing the Loss Function</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a433dcc",
   "metadata": {},
   "source": [
    "We can now used minibatch stochastic gradient descent (SGD) to minimize the loss function. PyTorch provides various optimization algorithms in the `torch.optim` module. These algorithms implement the logic of a single step.  Here we will use the Adam optimizer we introduced in the last lecture.\n",
    "In PyTorch, you have to create your own iterative training loop.  Before the training loop, you typically setup a **DataLoader** to handle batching and shuffling of the dataset, and you create an **optimizer** to update the model parameters.\n",
    "\n",
    "\n",
    "Then, within the training loop, you perform the following steps for each batch of data:\n",
    "1. **Zero the gradients**: Before computing the gradients for the current batch, we need to zero out the gradients from the previous batch. This is done using the `optimizer.zero_grad()` method.\n",
    "1. **Forward pass**: We pass the input data through the model to obtain the predictions.\n",
    "1. **Compute the loss**: We compute the loss between the model's predictions and the true labels using the loss function.\n",
    "1. **Backward pass**: We compute the gradients of the loss with respect to the model parameters using the `loss.backward()` method. This populates the `.grad` attributes of the model parameters.\n",
    "1. **Update the parameters**: We update the model parameters using the optimizer's `step()` method, which applies the optimization algorithm to adjust the parameters based on the computed gradients.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ac9dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gd(model, loss_fn, \n",
    "                 training_data,\n",
    "                 batch_size, nepochs, learning_rate,\n",
    "                 visualizer=None):\n",
    "    \n",
    "    # Create a dataloader for training\n",
    "    from torch.utils.data import DataLoader\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(189)\n",
    "    loader = DataLoader(training_data, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, # shuffles each epoch\n",
    "                        generator=generator)\n",
    "    \n",
    "    # Define the optimizer (this is the update rule)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
    "    # Alternatively, you can use Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    \n",
    "    # Loop through the epochs\n",
    "    for epoch in range(nepochs):\n",
    "        # Loop through all the batches\n",
    "        for (x, t) in loader:\n",
    "            # Zero the gradients to start the next step\n",
    "            optimizer.zero_grad()\n",
    "            # Compute prediction and loss\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, t)\n",
    "            # Backpropagation (compute the gradient)\n",
    "            loss.backward()\n",
    "            # Update the parameters using the optimizer's update rule\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Visualize the model (if a visualizer function is provided)\n",
    "        if visualizer is not None:\n",
    "            with torch.no_grad():\n",
    "                visualizer(epoch, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1199fcc",
   "metadata": {},
   "source": [
    "You can ignore the following visualization code.  This was added to track and update the loss and decision surface during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fad2a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossVisualizer:\n",
    "    def __init__(self, validation_data, pred_fig, loss_fig, \n",
    "                 num_points=50, plot_probs=True, sleep_time=0):\n",
    "        self.x_val, self.t_val = validation_data[:]\n",
    "        self.pred_fig = pred_fig\n",
    "        self.loss_fig = loss_fig\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        self.plot_probs = plot_probs\n",
    "        self.sleep_time = sleep_time\n",
    "        x1_min, x1_max = to_numpy(self.x_val[:,0]).min(), to_numpy(self.x_val[:,0]).max()\n",
    "        x2_min, x2_max = to_numpy(self.x_val[:,1]).min(), to_numpy(self.x_val[:,1]).max()\n",
    "        margin_x1 = 0.5 * (x1_max - x1_min)\n",
    "        margin_x2 = 0.5 * (x2_max - x2_min)\n",
    "        x1_min -= margin_x1\n",
    "        x1_max += margin_x1\n",
    "        x2_min -= margin_x2\n",
    "        x2_max += margin_x2\n",
    "        # Setup the grid of test points for decision boundary plotting\n",
    "        x1, x2 = torch.meshgrid(torch.linspace(x1_min, x1_max, num_points),\n",
    "                                torch.linspace(x2_min, x2_max, num_points), indexing='ij')\n",
    "        self.grid = torch.cat([x1.reshape(-1, 1), x2.reshape(-1, 1)], dim=1)\n",
    "        self.x1 = to_numpy(x1)\n",
    "        self.x2 = to_numpy(x2)\n",
    "\n",
    "    def plot_decision_boundary(self, model):\n",
    "        with torch.no_grad():\n",
    "            preds = F.softmax(model(self.grid), dim=1)\n",
    "        num_classes = preds.shape[1]\n",
    "        if num_classes > 2:  # support for multiclass\n",
    "            preds = torch.argmax(preds, axis=1).reshape(self.x1.shape).T\n",
    "            preds = to_numpy(preds)\n",
    "            return go.Contour(x=self.x1[:, 0], y=self.x2[0], z=preds,\n",
    "                            #   contours=dict(start=0, end=num_classes, size=1),\n",
    "                              colorscale=px.colors.qualitative.Plotly[:num_classes],  \n",
    "                              contours=dict(start=-0.5, end=num_classes-0.5, size=1, coloring='fill'),\n",
    "                              opacity=0.5, showscale=False)\n",
    "        else:  # Binary classification case (red/blue)\n",
    "            if self.plot_probs:\n",
    "                preds = preds[:, 1].reshape(self.x1.shape).T\n",
    "            else:\n",
    "                preds = (preds[:, 1] > 0.5).astype(float).reshape(self.x1.shape).T\n",
    "            preds = to_numpy(preds)\n",
    "            return go.Contour(x=self.x1[:, 0], y=self.x2[0], z=preds,\n",
    "                              colorscale=[[0, 'red'], [1, 'blue']], \n",
    "                              #colorscale='Matter_r',\n",
    "                              opacity = 0.5, showscale=False)\n",
    "    def reset(self):\n",
    "        self.epochs = []\n",
    "        self.losses = []\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = []\n",
    "            self.loss_fig.data[0].y = []\n",
    "    \n",
    "    def __call__(self, epoch, model, loss_fn):\n",
    "        with torch.no_grad():\n",
    "            loss = loss_fn(model(self.x_val), self.t_val).item()\n",
    "        self.epochs.append(epoch)\n",
    "        self.losses.append(loss)\n",
    "        # Visualization Code\n",
    "        boundary = self.plot_decision_boundary(model)\n",
    "        # plotly batch update\n",
    "        with self.pred_fig.batch_update():\n",
    "            self.pred_fig.data[-1].z = boundary.z\n",
    "        with self.loss_fig.batch_update():\n",
    "            self.loss_fig.data[0].x = self.epochs\n",
    "            self.loss_fig.data[0].y = self.losses\n",
    "        if(self.sleep_time > 0):\n",
    "            import time\n",
    "            time.sleep(self.sleep_time)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa9161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HBox\n",
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train Loss'))\n",
    "loss_fig.update_layout(title='Validation Loss', xaxis_title='Epochs', yaxis_title='Validation Loss (CE)')\n",
    "\n",
    "\n",
    "model = MultiClassLinearModel(input_dim=2)\n",
    "visualizer = LossVisualizer(\n",
    "    validation_data=validation_data,\n",
    "    pred_fig=pred_fig, loss_fig=loss_fig, \n",
    "    num_points=50, plot_probs=True, sleep_time=0)\n",
    "\n",
    "pred_fig.add_trace(visualizer.plot_decision_boundary(model))\n",
    "display(HBox([pred_fig,loss_fig]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c1fa2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiClassLinearModel(input_dim=2)\n",
    "visualizer.reset()\n",
    "minibatch_gd(\n",
    "    model=model, loss_fn=loss_fn, \n",
    "    training_data=training_data,\n",
    "    batch_size=32, nepochs=50, learning_rate=0.01,\n",
    "    visualizer=visualizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc5fb87",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Building a Neural Network</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e4d74",
   "metadata": {},
   "source": [
    "Now that we have a working logistic regression model, we can extend it to a simple neural network by adding hidden layers and non-linear activation functions. This will allow the model to learn more complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbf4e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, dims, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        # Notice here I create a ModuleList of layers so that PyTorch can track them    \n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n",
    "        )\n",
    "        # Store activation function\n",
    "        if activation == \"relu\": self.act = nn.functional.relu\n",
    "        else: self.act = torch.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i < len(self.layers)-1:  # no activation on final layer\n",
    "                x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ea9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPModel(dims=[2, 4, 4, 2], activation=\"tanh\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ed250",
   "metadata": {},
   "source": [
    "The following code creates an interactive visualization to track the decision boundary and loss during training.  Unfortunately, this visualization will not work in the static HTML version of this notebook and doesn't appear to work in Google Colab.  You will need to run this code in a local Jupyter notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HBox\n",
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train Loss'))\n",
    "loss_fig.update_layout(title='Validation Loss', xaxis_title='Epochs', \n",
    "                       yaxis_title='Validation Loss (CE)')\n",
    "visualizer = LossVisualizer(\n",
    "    validation_data=validation_data,\n",
    "    pred_fig=pred_fig, loss_fig=loss_fig, \n",
    "    num_points=50, plot_probs=True, sleep_time=0)\n",
    "pred_fig.add_trace(visualizer.plot_decision_boundary(model))\n",
    "display(HBox([pred_fig,loss_fig]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ba0363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPModel(dims=[2, 10, 2], activation=\"tanh\")\n",
    "visualizer.reset()\n",
    "minibatch_gd(\n",
    "    model=model, loss_fn=loss_fn, \n",
    "    training_data=training_data,\n",
    "    batch_size=32, nepochs=20, learning_rate=0.01,\n",
    "    visualizer=visualizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506cc624",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">More Complex Classification Dataset</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc32b2",
   "metadata": {},
   "source": [
    "Now that we have a working multi-layer neural network, we can test it on a more complex dataset. The `make_moons` function generates a two-dimensional dataset with a crescent moon shape, which is a common benchmark for testing classification algorithms. We can add some noise to the data to make the classification task more challenging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons\n",
    "# x, t = make_circles(n_samples=1000, noise=0.2, factor=0.5, random_state=42)\n",
    "x, t = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "data_fig = go.Figure()\n",
    "data_fig.add_trace(go.Scatter(x=x[t == 0, 0], y=x[t == 0, 1], \n",
    "                              mode='markers', marker=dict(color='red'), \n",
    "                              name='0'))   \n",
    "data_fig.add_trace(go.Scatter(x=x[t == 1, 0], y=x[t == 1, 1], \n",
    "                              mode='markers', marker=dict(color='blue'), \n",
    "                              name='1'))  \n",
    "data_fig.update_layout(width=500, height=500,\n",
    "                       xaxis_range=[-2, 3], yaxis_range=[-1.5, 2],\n",
    "                       xaxis_title='Feature 1',\n",
    "                       yaxis_title='Feature 2',\n",
    "                       title='make_circles Dataset')\n",
    "print(\"The first 5 training datapoints:\", X[:5])\n",
    "print(\"The labels for the first 5 datapoints:\", t[:5])\n",
    "data_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8f0a6",
   "metadata": {},
   "source": [
    "Convert the data into PyTorch Tensor format and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6575f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data = make_datasets_from_numpy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06738738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider, Dropdown, Checkbox\n",
    "\n",
    "model = model = MLPModel(dims=[2, 4, 4, 2], activation=\"relu\")\n",
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train Loss'))\n",
    "loss_fig.update_layout(title='Validation Loss', xaxis_title='Epochs', yaxis_title='Validation Loss (BC)')\n",
    "\n",
    "visualizer = LossVisualizer(\n",
    "    validation_data=validation_data,\n",
    "    pred_fig=pred_fig, loss_fig=loss_fig, \n",
    "    num_points=50, plot_probs=True, sleep_time=0)\n",
    "pred_fig.add_trace(visualizer.plot_decision_boundary(model))\n",
    "\n",
    "display(HBox([pred_fig,loss_fig]))\n",
    "\n",
    "@interact(n_layers=IntSlider(min=1, max=5, step=1, value=2, description=\"Layers\", continuous_update=False),\n",
    "         neurons_per_layer=IntSlider(min=4, max=64, step=4, value=8, description=\"Neurons/Layer\", continuous_update=False),\n",
    "         activation_fn=Dropdown(options=['relu', 'tanh',], value='tanh', description=\"Activation\", continuous_update=False),\n",
    "         learning_rate=FloatSlider(min=0.001, max=0.1, step=0.001, value=0.01, description=\"Learning Rate\", continuous_update=False),\n",
    "         batch_size=IntSlider(min=1, max=128, step=8, value=32, description=\"Batch Size\", continuous_update=False),\n",
    "         epochs=IntSlider(min=10, max=200, step=10, value=10, description=\"Epochs\", continuous_update=False))\n",
    "def update_model(n_layers, neurons_per_layer, activation_fn, learning_rate, batch_size, epochs):\n",
    "    # setup the model\n",
    "    layers = [2] + ([neurons_per_layer] * n_layers) + [2]\n",
    "    model = MLPModel(dims=layers, activation=activation_fn)\n",
    "    visualizer.reset()\n",
    "    minibatch_gd(\n",
    "        model=model, loss_fn=loss_fn, \n",
    "        training_data=training_data,\n",
    "        batch_size=batch_size, nepochs=epochs, learning_rate=learning_rate,\n",
    "        visualizer=visualizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c3990",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Multi-Class Classification Dataset</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30cea03",
   "metadata": {},
   "source": [
    "We can try one more extension to the multi-class setting.  The `make_blobs` function generates isotropic Gaussian blobs for clustering. We can use this function to create a dataset with multiple classes and test our neural network's ability to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5954da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "centers = 5 # number of classes\n",
    "x, t = make_blobs(n_samples=1000, centers=centers, cluster_std=1.0, random_state=189)\n",
    "\n",
    "data_blobs = pd.DataFrame({'x1': x[:, 0], 'x2': x[:, 1], 't': t.astype(str)})\n",
    "data_blobs.sort_values('t', inplace=True)\n",
    "\n",
    "data_fig = px.scatter(data_blobs, x='x1', y='x2', color='t', \n",
    "                      title='make_blobs Dataset', \n",
    "                      category_orders={'t': [str(i) for i in range(centers)]},\n",
    "                      labels={'x1': 'Feature 1', 'x2': 'Feature 2', 'color': 'Class'},\n",
    "                      width=500, height=500)\n",
    "print(\"The first 5 training datapoints:\", X[:5])\n",
    "print(\"The labels for the first 5 datapoints:\", t[:5])\n",
    "data_fig.show()\n",
    "\n",
    "training_data, validation_data = make_datasets_from_numpy(x, t, split_ratio=0.8, seed=189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider, Dropdown, Checkbox\n",
    "\n",
    "model = model = MLPModel(dims=[2, 4, 4, centers], activation=\"relu\")\n",
    "pred_fig = go.FigureWidget(data=data_fig.data, layout=data_fig.layout)\n",
    "# hide the color scale for the contour plot\n",
    "pred_fig.update_layout(coloraxis_showscale=False)\n",
    "loss_fig = go.FigureWidget()\n",
    "loss_fig.add_trace(go.Scatter(x=[], y=[], mode='lines', name='Train Loss'))\n",
    "loss_fig.update_layout(title='Validation Loss', xaxis_title='Epochs', yaxis_title='Validation Loss (BC)')\n",
    "\n",
    "visualizer = LossVisualizer(\n",
    "    validation_data=validation_data,\n",
    "    pred_fig=pred_fig, loss_fig=loss_fig, \n",
    "    num_points=50, plot_probs=True, sleep_time=0)\n",
    "pred_fig.add_trace(visualizer.plot_decision_boundary(model))\n",
    "\n",
    "display(HBox([pred_fig,loss_fig]))\n",
    "\n",
    "@interact(n_layers=IntSlider(min=1, max=5, step=1, value=2, description=\"Layers\", continuous_update=False),\n",
    "         neurons_per_layer=IntSlider(min=4, max=64, step=4, value=8, description=\"Neurons/Layer\", continuous_update=False),\n",
    "         activation_fn=Dropdown(options=['relu', 'tanh',], value='tanh', description=\"Activation\", continuous_update=False),\n",
    "         learning_rate=FloatSlider(min=0.001, max=0.1, step=0.001, value=0.01, description=\"Learning Rate\", continuous_update=False),\n",
    "         batch_size=IntSlider(min=1, max=128, step=8, value=32, description=\"Batch Size\", continuous_update=False),\n",
    "         epochs=IntSlider(min=10, max=200, step=10, value=10, description=\"Epochs\", continuous_update=False))\n",
    "def update_model(n_layers, neurons_per_layer, activation_fn, learning_rate, batch_size, epochs):\n",
    "    # setup the model\n",
    "    layers = [2] + ([neurons_per_layer] * n_layers) + [centers]\n",
    "    model = MLPModel(dims=layers, activation=activation_fn)\n",
    "    visualizer.reset()\n",
    "    minibatch_gd(\n",
    "        model=model, loss_fn=loss_fn, \n",
    "        training_data=training_data,\n",
    "        batch_size=batch_size, nepochs=epochs, learning_rate=learning_rate,\n",
    "        visualizer=visualizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
