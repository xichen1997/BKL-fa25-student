{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f55c784",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 12: Gradient Descent â€“ CS 189, Fall 2025</h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae09b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import figure_factory as ff\n",
    "colors = px.colors.qualitative.Plotly\n",
    "px.defaults.width = 800\n",
    "from ipywidgets import HBox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the images folder if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc0ccb",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Plotting Code you Can Safely Ignore (but need to run).</h2>\n",
    "\n",
    "This lecture has many complex visualizations and to keep the rest of the code short I have put the visualization code here.  Much of this code is out-of-scope for the course, but feel free to look through it if you are interested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd69551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_grid(figs, rows, cols):\n",
    "    \"\"\"Create a grid of figures with Plotly figures.\"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "    def get_trace_type(fig):\n",
    "        for trace in fig.data:\n",
    "            if trace.type == 'surface':\n",
    "                return 'surface'  # required for go.Surface\n",
    "            elif trace.type.startswith('scatter3d') or trace.type.startswith('mesh3d'):\n",
    "                return 'scene'  # 3D scene\n",
    "        return 'xy'  # default 2D\n",
    "    specs = [[{'type': get_trace_type(fig)} for fig in figs[i:i+cols]] for i in range(0, len(figs), cols)]\n",
    "    fig_grid = make_subplots(rows=rows, cols=cols, specs=specs,\n",
    "                             subplot_titles=[fig.layout.title.text for fig in figs])\n",
    "    for i, fig in enumerate(figs):\n",
    "        fig_grid.add_traces(fig.data, rows=(i//cols) + 1, cols=(i%cols) + 1 )\n",
    "    return fig_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_predictions(w, cancer):\n",
    "    \"\"\"Plot predictions of the logistic model.\"\"\"\n",
    "    cancer = cancer.copy()\n",
    "    cancer['logistic_pred'] = np.where(\n",
    "        logistic_model(w, cancer[['mean radius', 'mean texture']].values) > 0.5,\n",
    "        'Pred Malignant', 'Pred Benign'\n",
    "    )\n",
    "    # Create a scatter plot with logistic predictions\n",
    "    fig = px.scatter(cancer, x='mean radius', y='mean texture', \n",
    "                     symbol='logistic_pred', color='target',\n",
    "                     symbol_sequence=[ \"circle-open\", \"cross\"])\n",
    "    for (i,t) in enumerate(fig.data): t.legendgroup = str(i)\n",
    "    # decision boundary\n",
    "    xs = np.linspace(cancer['mean radius'].min(), cancer['mean radius'].max(), 100)\n",
    "    decision_boundary = -(w[0] * xs ) / w[1]\n",
    "    fig.add_scatter(x=xs, y=decision_boundary, mode='lines', \n",
    "                    name='Decision Boundary', legendgroup='Decision Boundary',\n",
    "                    line=dict(color='black', width=2, dash='dash', ))\n",
    "    # probability surface\n",
    "    ys = np.linspace(cancer['mean texture'].min(), cancer['mean texture'].max(), 100)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = logistic_model(w, np.c_[X.ravel(), Y.ravel()]).reshape(X.shape)\n",
    "    fig.add_contour(x=xs, y=ys, z=Z, \n",
    "                    colorscale='Matter_r', opacity=0.5,\n",
    "                    name='Probability Surface', \n",
    "                    colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "\n",
    "    fig.update_layout(title=f'w=({w[0]:.2f}, {w[1]:.2f})',\n",
    "                      xaxis_range=[xs.min(), xs.max()], yaxis_range=[ys.min(), ys.max()],\n",
    "                      xaxis_title='Mean Radius (scaled)', yaxis_title='Mean Texture (scaled)',\n",
    "                      width=800, height=600)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(w1, w2, error, ncontours=50):\n",
    "    surf_fig = go.Figure()\n",
    "    surf_fig.add_surface(z=error, x=w1, y=w2, \n",
    "                         colorscale='Viridis_r', opacity=0.7, showscale=False,\n",
    "                         contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                          start=error.min(), end=error.max(), \n",
    "                                          size=(error.max()-error.min())/ncontours)))\n",
    "    surf_fig.update_layout(title=\"Loss Surface\")\n",
    "    contour_fig = go.Figure()\n",
    "    contour_fig.add_contour(x=w1.flatten(), y=w2.flatten(), z=error.flatten(),\n",
    "                            colorscale='Viridis_r', opacity=0.7,\n",
    "                            contours=dict(start=error.min(), end=error.max(), \n",
    "                                          size=(error.max()-error.min())/ncontours),\n",
    "                            colorbar=dict(x=1.05, y=0.35, len=0.75))\n",
    "    contour_fig.update_layout(title=\"Loss Contours\")\n",
    "    fig = make_plot_grid([surf_fig, contour_fig], 1, 2).update_layout(height=800)\n",
    "    fig.update_layout(scene=dict(xaxis_title='w1', yaxis_title='w2', zaxis_title='Loss', aspectmode='cube'))\n",
    "    fig.update_layout(xaxis_range=[w1.min(), w1.max()], yaxis_range=[w2.min(), w2.max()],\n",
    "                      xaxis_title='w1', yaxis_title='w2')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(w1, w2, error, dw1, dw2, scale=1.0):\n",
    "    fig = plot_loss(w1, w2, error)\n",
    "    fig.add_trace(\n",
    "        go.Cone(\n",
    "            x=w1.flatten(), y=w2.flatten(), z=np.zeros_like(error).flatten(),  # Ground plane\n",
    "            u=dw1.flatten(), v=dw2.flatten(), w=np.zeros_like(error).flatten(),  # No vertical component\n",
    "            sizeref=2, anchor=\"tail\", showscale=False\n",
    "        ), 1,1)\n",
    "    contour_fig = ff.create_quiver(\n",
    "        x=w1.flatten(), y=w2.flatten(), u=dw1.flatten(), v=dw2.flatten(), \n",
    "        line_width=2, line_color=\"white\",\n",
    "        scale = scale, arrow_scale=.2, showlegend=False)\n",
    "    fig.add_traces(contour_fig.data, rows=1, cols=2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_solution_path(fig, errors, ws):\n",
    "    s = np.linspace(0, 1, len(ws))\n",
    "    fig.add_scatter3d(x=ws[:, 0], y=ws[:, 1], z=errors, marker_color=s, marker_size=5,\n",
    "                    mode='lines+markers', line=dict(color='black', width=2), opacity=0.5,\n",
    "                    name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                    row=1, col=1)\n",
    "    fig.add_scatter(x=ws[:, 0], y=ws[:, 1], marker_color=s,\n",
    "                    mode='lines+markers', line=dict(color='black', width=2), opacity=0.5,\n",
    "                    name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                    showlegend=False,\n",
    "                    row=1, col=2)\n",
    "    fig.add_scatter3d(x=[ws[-1, 0]], y=[ws[-1, 1]], z=[errors[-1]],\n",
    "                       mode='markers', marker=dict(color='red', size=10),\n",
    "                       name='Final Solution', legendgroup='Final Solution',\n",
    "                       row=1, col=1)\n",
    "    fig.add_scatter(x=[ws[-1, 0]], y=[ws[-1, 1]],\n",
    "                       mode='markers', marker=dict(color='red', size=20),\n",
    "                       name='Final Solution', legendgroup='Final Solution',\n",
    "                       showlegend=False,\n",
    "                       row=1, col=2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc4828",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Optimization Basics</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5f07e",
   "metadata": {},
   "source": [
    "Consider the following optimization problems?  What is the solution? How would you find it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b7d1",
   "metadata": {},
   "source": [
    "**Problem 1**: Minimize the function \n",
    "$$\n",
    "\\arg \\min_{w \\in \\mathbb{R}}w^2 - 3w + 4 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-2,7,100)\n",
    "f = lambda w: w**2 - 3 * w + 4\n",
    "fig = px.line(x=w, y=f(w), labels={'x': 'w', 'y': 'f(w)'})\n",
    "fig.update_traces(line_width=5)\n",
    "# fig.write_image(\"images/function1.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0a224",
   "metadata": {},
   "source": [
    "**Problem 2**: Minimize the function this time with integer contraints\n",
    "$$\n",
    "\\arg \\min_{w \\in \\mathbb{Z}}w^2 - 3w + 4 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.arange(-2, 7)\n",
    "f = lambda w: w**2 - 3 * w + 4\n",
    "fig = px.scatter(x=w, y=f(w), labels={'x': 'w', 'y': 'f(w)'})\n",
    "fig.update_traces(marker_size=7)\n",
    "# fig.write_image(\"images/function2.pdf\",) \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eec76c",
   "metadata": {},
   "source": [
    "**Problem 3**: Minimize the function \n",
    "\\begin{align}\n",
    "\\arg \\min_{w \\in \\mathbb{R}} w^4 - 5 w^2 + w + 4 \n",
    "\\end{align}\n",
    "\n",
    "<!--- \n",
    "$$\n",
    "\\arg \\min_{w \\in \\mathbb{R}} (1-w)(1+w)(2-w)(2+w) + w \n",
    "$$\n",
    "\n",
    "```python\n",
    "import sympy\n",
    "w = sympy.Symbol('w')\n",
    "exp = sympy.factor( (1-w)*(1+w)*(2-w)*(2+w) + w)\n",
    "display(exp)\n",
    "# latex for expression\n",
    "sympy.latex(exp)\n",
    "```\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-3,3,100)\n",
    "f = lambda w: w**4 - 5 * w**2 + w + 4\n",
    "fig = px.line(x=w, y=f(w), labels={'x': 'w', 'y': 'f(w)'})\n",
    "#increase the line width\n",
    "fig.update_traces(line_width=5)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "# fig.write_image(\"images/function3.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dce642",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Convexity</h3>\n",
    "\n",
    "In this section, we will discuss convex optimization problems and how to solve them.\n",
    "\n",
    "**Todo** add definitions of convexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_secant(w, f, x1, x2):\n",
    "    \"\"\"Plot the secant line between two points on the function.\"\"\"\n",
    "    y1, y2 = f(x1), f(x2)\n",
    "    fig = px.line(x=w, y=f(w), labels={'x': 'w', 'y': 'f(w)'})\n",
    "    fig.add_scatter(x=[x1, x2], y=[f(x1), f(x2)], \n",
    "                mode='markers+lines', name='Secant Line',\n",
    "                marker=dict(size=20, color='green'),\n",
    "                line=dict(color='green', dash=\"dash\"))\n",
    "    fig.update_traces(line_width=5)\n",
    "    fig.update_layout(showlegend=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.linspace(-2,7,100)\n",
    "f = lambda w: w**2 - 3 * w + 4\n",
    "fig_convex = plot_secant(w, f, -1, 5)\n",
    "fig_convex.update_layout(title=\"Convex Function\")\n",
    "# fig_convex.write_image(\"images/convex_function.pdf\")\n",
    "\n",
    "w = np.linspace(-3,3,100)\n",
    "f = lambda w: w**4 - 5 * w**2 + w + 4\n",
    "fig_nonconvex = plot_secant(w, f, -1, 1)\n",
    "fig_nonconvex.update_layout(title=\"Non-convex Function\")\n",
    "# fig_nonconvex.write_image(\"images/non_convex_function.pdf\")\n",
    "\n",
    "fig = make_plot_grid([fig_convex, fig_nonconvex], 1, 2)\n",
    "fig.update_layout(showlegend=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618368d",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Understanding the Error Surface</h2>\n",
    "\n",
    "In this part of the lecture we will visualize the error surface for a few problems.  This will help us understand why gradient descent works and how it finds the optimal solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbc5e1",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Cross Entropy Loss Surface for Logistic Regression</h3>\n",
    "\n",
    "In the previous lecture we studied the Logistic Regression model.  The cross entropy loss function is convex.  Here we will visualize a simple two dimensional case of the loss surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd647c12",
   "metadata": {},
   "source": [
    "For this demo, we will use the breast cancer dataset. The dataset is available in the `datasets` module of `sklearn`. To keep things simple, we will focus on just two features of the dataset `\"mean radius\"`, `\"mean texture\"`.  We will normalize these features to keep the model simple.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cancer_dict = datasets.load_breast_cancer(as_frame=True)\n",
    "cancer_df = pd.DataFrame(cancer_dict.data, columns=cancer_dict.feature_names)\n",
    "cancer_df['target'] = cancer_dict.target.astype(str)\n",
    "cancer_df = cancer_df[['mean radius', 'mean texture', 'target']].dropna()\n",
    "scaler = StandardScaler()\n",
    "cancer_df[['mean radius', 'mean texture']] = scaler.fit_transform(cancer_df[['mean radius', 'mean texture']])\n",
    "print(\"The dataset:\", cancer_df.shape)\n",
    "display(cancer_df.head())\n",
    "px.scatter(cancer_df, x='mean radius', y='mean texture', color='target', opacity=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7114fd43",
   "metadata": {},
   "source": [
    "Here we see there is likely a simple linear decision boundary that separates the two classes.  Let's visualize the decision boundary for a few model parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc9ddf",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Logistic Regression Model</h4>\n",
    "\n",
    "Recall that the logistic regression model is given by:\n",
    "\n",
    "\\begin{align}\n",
    "p(t=1|x) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}\n",
    "\\end{align}\n",
    "\n",
    "To be able to plot the loss surface we will drop the bias term and consider just two weights $w_1$ and $w_2$.  The resulting model is:\n",
    "\n",
    "\\begin{align}\n",
    "p(t=1|x) = \\sigma(w_1 x_1 + w_2 x_2) = \\frac{1}{1 + e^{-(w_1 x_1 + w_2 x_2)}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f44912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_model(w, x):\n",
    "    \"\"\"Logistic model for binary classification.\"\"\"\n",
    "    z = w @ x.T\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d5da3",
   "metadata": {},
   "source": [
    "The following code plots the decision boundary for a few different values of $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses = np.array([[-1., -1.],[-2., -4.]])\n",
    "figs = [plot_lr_predictions(w, cancer_df) for w in guesses]\n",
    "figs[1].update_traces(showlegend=False)\n",
    "fig = make_plot_grid(figs, 1, 2)\n",
    "# minor fixup of plots \n",
    "fig.update_layout(height=600, xaxis_range=figs[0].layout.xaxis.range, \n",
    "                  yaxis_range=figs[0].layout.yaxis.range)\n",
    "fig.update_layout(height=600, xaxis2_range=figs[1].layout.xaxis.range, \n",
    "                  yaxis2_range=figs[1].layout.yaxis.range)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b066e",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Plotting the Cross Entropy Loss Surface</h4>\n",
    "\n",
    "Recall that the average negative log likelihood loss function (cross entropy loss) for logistic regression is given by:\n",
    "\n",
    "\\begin{align}\n",
    "L(w) = -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\log(p(t_i|x_i)) + (1 - t_i) \\log(1 - p(t_i|x_i)) \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c628db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(w):\n",
    "    \"\"\"Negative log-likelihood for logistic regression.\"\"\"\n",
    "    x = cancer['x']\n",
    "    t = cancer['t']\n",
    "    z = w @ x.T\n",
    "    # return -np.mean(t * np.log(logistic_model(w, x)) + (1 - t) * np.log(1 - logistic_model(w, x)))\n",
    "    # more numerically stable version\n",
    "    # np.mean(np.log(1+ np.exp(z)) - t * z) \n",
    "    # stable softplus: log(1+exp(z))\n",
    "    softplus_z = np.logaddexp(0, z)\n",
    "    return np.mean(softplus_z - t * z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4fa54",
   "metadata": {},
   "source": [
    "For visualization purposes I am going to create a dictionary to track all the variables I am creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eafb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = dict()\n",
    "cancer['npts'] = 30\n",
    "cancer['w1'], cancer['w2'] = np.meshgrid(\n",
    "    np.linspace(-10, 1, cancer['npts']), \n",
    "    np.linspace(-5, 1.3, cancer['npts'])\n",
    ")\n",
    "cancer['ws'] = np.stack([cancer['w1'].flatten(), cancer['w2'].flatten()]).T\n",
    "cancer['x'] = cancer_df[['mean radius', 'mean texture']].values\n",
    "cancer['t'] = cancer_df['target'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b113d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer['error'] = np.array([neg_log_likelihood(w) \n",
    "                for w in cancer['ws']])\n",
    "cancer['error'] = cancer['error'].reshape(cancer['w1'].shape)\n",
    "fig = plot_loss(cancer['w1'], cancer['w2'], cancer['error'])\n",
    "for i, g in enumerate(guesses):\n",
    "    fig.add_scatter3d(x=[g[0]], y=[g[1]], z=[neg_log_likelihood(g)],\n",
    "                      mode='markers', marker=dict(size=10, color=colors[i + 2]),\n",
    "                      name=f'Weight {g}', legendgroup=str(i), row=1, col=1)\n",
    "    fig.add_scatter(x=[g[0]], y=[g[1]], mode='markers',\n",
    "                    marker=dict(size=10, color=colors[i + 2]),\n",
    "                    name=f'Weight {g}', legendgroup=str(i), showlegend=False,\n",
    "                    row=1, col=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07787b7d",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Choosing the Best Parameters</h4>\n",
    "\n",
    "We have computed the negative log likelihood loss surface for the logistic regression model for many parameter values.  Here we can simply choose the parameters with the lowest loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e79083",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ind = np.argmin(cancer['error'])\n",
    "cancer['grid_best'] = np.array([cancer['w1'].flatten()[best_ind], \n",
    "                                cancer['w2'].flatten()[best_ind]])\n",
    "\n",
    "fig = plot_loss(cancer['w1'], cancer['w2'], cancer['error'])\n",
    "for i, g in enumerate(guesses):\n",
    "    fig.add_scatter3d(x=[g[0]], y=[g[1]], z=[neg_log_likelihood(g)],\n",
    "                      mode='markers', marker=dict(size=10, color=colors[i + 2]),\n",
    "                      name=f'Weight {g}', legendgroup=str(i), row=1, col=1)\n",
    "    fig.add_scatter(x=[g[0]], y=[g[1]], mode='markers',\n",
    "                    marker=dict(size=10, color=colors[i + 2]),\n",
    "                    name=f'Weight {g}', legendgroup=str(i), showlegend=False,\n",
    "                    row=1, col=2)\n",
    "fig.add_scatter3d(x=[cancer['grid_best'][0]], y=[cancer['grid_best'][1]], z=[cancer['error'].min()],\n",
    "    mode='markers', marker=dict(size=10, color='red'),\n",
    "    name=f\"Best Weight [{cancer['grid_best'][0]:0.2f}, {cancer['grid_best'][1]:0.2f}]\", \n",
    "    legendgroup=f\"Best\",\n",
    "    row=1, col=1)\n",
    "fig.add_scatter(x=[cancer['grid_best'][0]], y=[cancer['grid_best'][1]],\n",
    "    mode='markers', marker=dict(size=10, color='red'),\n",
    "    name=f\"Best Weight [{cancer['grid_best'][0]:0.2f}, {cancer['grid_best'][1]:0.2f}]\", \n",
    "    legendgroup=f\"Best\",\n",
    "    showlegend=False, row=1, col=2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lr_predictions(cancer['grid_best'], cancer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52715bc",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Squared Error Loss Squared Loss on a Non-Linear Model</h3>\n",
    "\n",
    "In the following we will visualize a more complex loss surface.  We will use a non-linear model and the squared error loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine = dict()\n",
    "np.random.seed(42)\n",
    "sine['n'] = 200\n",
    "# Generate some random data\n",
    "sine['x'] = np.random.rand(sine['n']) * 2.5 * np.pi\n",
    "sine['x'] = np.sort(sine['x']) # sort for easier plotting\n",
    "sine['y'] = np.sin(1.1 + 2.5 * sine['x']) + 0.5 * np.random.randn(sine['n'])\n",
    "sine_df = pd.DataFrame({'x': sine['x'], 'y': sine['y']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59ecdd",
   "metadata": {},
   "source": [
    "Because we made this data we know that true underlying function that determines the data is a sine wave.\n",
    "\n",
    "$$\n",
    "\\hat{y} = f_w(x) = \\sin\\left(1.1 + 2.5 x \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537358e",
   "metadata": {},
   "source": [
    "We can visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(sine_df, x='x', y='y')\n",
    "fig.update_traces(marker_color='black') \n",
    "data_trace = fig.data[0]\n",
    "# fig.write_image(\"images/sine_data.pdf\", width=800, height=400)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe405000",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Non-linear Sine Regression Model</h4>\n",
    "\n",
    "Here we will use a model of the form:\n",
    "\\begin{align}\n",
    "y = f(x) = \\sin(w_0  + w_1 x)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22381025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_model(w, x):\n",
    "    return np.sin(w[0] + x * w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ceed18",
   "metadata": {},
   "source": [
    "Let's try a few different parameter values and see how this model looks on our data.  We will try the following three possible models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine['guesses'] = np.array([[0, 2], [2, 3], [0, 3.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8ada8",
   "metadata": {},
   "source": [
    "Here we make predictions at 100 test points for each model and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0165f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine['xhat'] = np.linspace(sine_df['x'].min(), sine_df['x'].max(), 100)\n",
    "sine['pred_df'] = pd.DataFrame({'x': sine['xhat']})\n",
    "for w in sine['guesses']:\n",
    "    sine['pred_df'][f'yhat(w={w})'] = sine_model(w, sine['xhat'])\n",
    "sine['pred_df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i, w in enumerate(sine['pred_df'].columns[1:]):\n",
    "    fig.add_trace(go.Scatter(x=sine['pred_df']['x'], y=sine['pred_df'][w], \n",
    "                             mode='lines', name=w, \n",
    "                             line=dict(width=4, color=colors[i+2])))\n",
    "fig.update_traces(line_width=4)\n",
    "fig.add_trace(data_trace)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), \n",
    "                  xaxis_title='x', yaxis_title='y')\n",
    "# fig.write_image(\"images/sine_with_3_models.pdf\", width=800, height=400)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679354d",
   "metadata": {},
   "source": [
    "None of the above really match the data.  We would like to find a parameterization that is closer to the data. To do this we need a loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f7d2c",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Visualizing Squared Loss</h4>\n",
    "\n",
    "To really fit the data we need a measure of how good our model is relative to the data.  This is a loss function.  For this exercise we will use the **average squared loss** which is often just called the squared loss.\n",
    "\n",
    "$$\n",
    "E(w;\\mathcal{D}) = L\\left(f_w; \\mathcal{D} = \\left\\{(x_i, y_i \\right\\}_{i=1}^n\\right) = \\frac{1}{n} \\sum_{i=1}^n\\left(y_i - f_w\\left(x_i\\right)\\right)^2\n",
    "= \\frac{1}{n} \\sum_{i=1}^n\\left(y_i - \\sin\\left(w_0 + w_1 x_i \\right)\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be344fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_MSE(w):\n",
    "    x = sine['x']\n",
    "    y = sine['y']\n",
    "    y_hat = sine_model(w, x)\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825005e4",
   "metadata": {},
   "source": [
    "Here, $w_0$ and $w_1$ are the parameters of our model.  We can visualize this loss function as a surface in 3D.  To do this we will create a grid of points in the parameter space and evaluate the loss function at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cac444",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine['npts'] = 30\n",
    "sine['w0'], sine['w1'] = np.meshgrid(\n",
    "    np.linspace(-1.5, 3, sine['npts']), np.linspace(1, 4, sine['npts']))\n",
    "# combine w1 and w2 into a single tensor\n",
    "sine['ws'] = np.stack([sine['w0'].flatten(), sine['w1'].flatten()]).T\n",
    "\n",
    "sine['error'] = np.array([sine_MSE(w) for w in sine['ws']]).reshape(sine['w0'].shape)\n",
    "\n",
    "fig = plot_loss(sine['w0'], sine['w1'], sine['error'])\n",
    "for i, w in enumerate(sine['guesses']):\n",
    "    fig.add_trace(go.Scatter3d(x=[w[0]], y=[w[1]], z=[sine_MSE(w)],\n",
    "                               mode='markers', marker=dict(size=5, color=colors[i+2]),\n",
    "                               name=f'w=({w[0]}, {w[1]})', legendgroup=str(i)),\n",
    "                  row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[w[0]], y=[w[1]],\n",
    "                               mode='markers', marker=dict(size=20, color=colors[i+2]),\n",
    "                               name=f'w=({w[0]}, {w[1]})', legendgroup=str(i), showlegend=False),\n",
    "                  row=1, col=2) \n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c42d3",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Choosing the Best Parameters</h4>\n",
    "\n",
    "Just as before, we can simply choose the parameters with the lowest loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(sine['error'])\n",
    "sine['grid_best'] = sine['ws'][ind,:]\n",
    "sine['grid_best_error'] = sine['error'].flatten()[ind]\n",
    "print(f\"Best weights: {sine['grid_best']}, with error: {sine['grid_best_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_loss(sine['w0'], sine['w1'], sine['error'])\n",
    "for i, w in enumerate(sine['guesses']):\n",
    "    fig.add_trace(go.Scatter3d(x=[w[0]], y=[w[1]], z=[sine_MSE(w)],\n",
    "                               mode='markers', marker=dict(size=5, color=colors[i+2]),\n",
    "                               name=f'w=({w[0]}, {w[1]})', legendgroup=str(i)),\n",
    "                  row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=[w[0]], y=[w[1]],\n",
    "                               mode='markers', marker=dict(size=20, color=colors[i+2]),\n",
    "                               name=f'w=({w[0]}, {w[1]})', legendgroup=str(i), showlegend=False),\n",
    "                  row=1, col=2) \n",
    "fig.add_scatter3d(x=[sine['grid_best'][0]], y=[sine['grid_best'][1]], z=[sine['grid_best_error']],\n",
    "    mode='markers', marker=dict(size=10, color='red'),\n",
    "    name=f\"Best Weight [{sine['grid_best'][0]:0.2f}, {sine['grid_best'][1]:0.2f}]\", legendgroup=f\"Best\",\n",
    "    row=1, col=1)\n",
    "fig.add_scatter(x=[sine['grid_best'][0]], y=[sine['grid_best'][1]],\n",
    "    mode='markers', marker=dict(size=20, color='red'),\n",
    "    name=f\"Best Weight [{sine['grid_best'][0]:0.2f}, {sine['grid_best'][1]:0.2f}]\", legendgroup=f\"Best\",\n",
    "    showlegend=False, row=1, col=2)\n",
    "    \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc8e49",
   "metadata": {},
   "source": [
    "Plotting the best fit model on the data we see that it is a much better fit than our previous guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i, w in enumerate(sine['pred_df'].columns[1:]):\n",
    "    fig.add_trace(go.Scatter(x=sine['pred_df']['x'], y=sine['pred_df'][w], mode='lines', name=w, \n",
    "                             line=dict(width=4, color=colors[i+2])))\n",
    "fig.update_traces(line_width=4)\n",
    "fig.add_trace(data_trace)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), \n",
    "                  xaxis_title='x', yaxis_title='y')\n",
    "sine['pred_df']['grid_best'] = sine_model(sine['grid_best'], sine['xhat'])  \n",
    "fig.add_scatter(x=sine['pred_df']['x'], y=sine['pred_df']['grid_best'], \n",
    "                mode='lines', \n",
    "                name=f\"Best w=({sine['grid_best'][0]:.2f}, {sine['grid_best'][1]:.2f})\",\n",
    "                line=dict(width=8, color='red')\n",
    ")\n",
    "# fig.write_image(\"images/sine_with_3_models.pdf\", width=800, height=400)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce22f6",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Slido Visualization</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3825004",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.scatter(x=cancer['x'][:,0], \n",
    "                  y=cancer['t'] + 0.01*np.random.normal(size=cancer['t'].shape), \n",
    "                  color=cancer['t'].astype(str), title=\"Cancer Data (Jittered)\")\n",
    "\n",
    "ws = np.linspace(-20, 3, 100)\n",
    "nll = np.array([\n",
    "    np.mean(np.log1p(np.exp(-w*cancer['x'][:,0])) - cancer['t'] * (w*cancer['x'][:,0]))\n",
    "    for w in ws])\n",
    "\n",
    "fig2 = px.line(x=ws, y=nll, labels={'x': 'w', 'y': 'Negative Log-Likelihood'})\n",
    "ind = np.argmin(nll)\n",
    "best_w = ws[ind]\n",
    "fig2.add_vline(x=best_w, line=dict(color='red', dash='dash'), \n",
    "               annotation_text=f\"Best w={best_w:.2f}\", \n",
    "               annotation_position=\"top right\")\n",
    "fig2.add_scatter(x=[best_w], y=[nll[ind]], mode='markers', \n",
    "                 marker=dict(color='red', size=10),\n",
    "                 name=\"Best w\")\n",
    "xtest = np.linspace(cancer['x'][:,0].min(), cancer['x'][:,0].max(), 100)\n",
    "fig1.add_scatter(x=xtest, \n",
    "                 y=sigmoid(best_w * xtest), \n",
    "                 mode='lines', line=dict(color='black', width=4),\n",
    "                 name=f'Logistic Model (w={best_w:.2f})')\n",
    "fig = make_plot_grid([fig1, fig2], 1, 2)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.scatter(x=cancer['x'][:,0], \n",
    "                  y=cancer['x'][:,1], title=\"Cancer Data\")\n",
    "ws = np.linspace(-5, 5, 100)\n",
    "sqloss = [np.mean((cancer['x'][:,1] - (w * cancer['x'][:,0]))**2) for w in ws]\n",
    "fig2 = px.line(x=ws, y=sqloss, labels={'x': 'w', 'y': 'Squared Loss'})\n",
    "ind = np.argmin(sqloss)\n",
    "best_w = ws[ind] \n",
    "xtest = np.linspace(cancer['x'][:,0].min(), cancer['x'][:,0].max(), 100)\n",
    "fig1.add_scatter(x=xtest, \n",
    "                 y=best_w * xtest, \n",
    "                 mode='lines', line=dict(color='black', width=4),\n",
    "                 name=f'Linear Model (w={best_w:.2f})')\n",
    "fig2.add_vline(x=best_w, line=dict(color='red', dash='dash'), \n",
    "               annotation_text=f\"Best w={best_w:.2f}\", \n",
    "               annotation_position=\"top right\")\n",
    "fig2.add_scatter(x=[best_w], y=[sqloss[ind]], mode='markers', \n",
    "                 marker=dict(color='red', size=10),\n",
    "                 name=\"Best w\")\n",
    "fig = make_plot_grid([fig1, fig2], 1, 2)\n",
    "fig.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1ce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 3\n",
    "fig1 = px.scatter(x=cancer['x'][:,0]+offset, \n",
    "                  y=cancer['t'] + 0.01*np.random.normal(size=cancer['t'].shape), \n",
    "                  color=cancer['t'].astype(str), title=\"Cancer Data (Jittered)\")\n",
    "#fig1.show()\n",
    "w1,w2 = np.meshgrid(np.linspace(5,15, 40), np.linspace(-8, 0, 40))\n",
    "ws = np.stack([w1.flatten(), w2.flatten()]).T\n",
    "nll = np.array([\n",
    "    np.mean(np.mean(np.logaddexp(0, w[1]*(cancer['x'][:,0]+offset) + w[0])) - \n",
    "    cancer['t'] * (w[1]*(cancer['x'][:,0]+offset) + w[0]))\n",
    "    for w in ws]).reshape(w1.shape)\n",
    "\n",
    "fig2 = plot_loss(w1, w2, nll)\n",
    "ind = np.argmin(nll)\n",
    "best_w = ws[ind,:]\n",
    "fig2.add_scatter3d(x=[best_w[0]], y=[best_w[1]], z=[nll.flatten()[ind]],\n",
    "                   mode='markers', marker=dict(size=10, color='red'),\n",
    "                   name=f'Best w=({best_w[0]:.2f}, {best_w[1]:.2f})', legendgroup='Best',\n",
    "                   row=1, col=1)\n",
    "fig2.add_scatter(x=[best_w[0]], y=[best_w[1]],\n",
    "                   mode='markers', marker=dict(size=20, color='red'),\n",
    "                   name=f'Best w=({best_w[0]:.2f}, {best_w[1]:.2f})', legendgroup='Best',\n",
    "                   showlegend=False,\n",
    "                   row=1, col=2)\n",
    "fig2.show()\n",
    "xtest = np.linspace(cancer['x'][:,0].min()+offset, cancer['x'][:,0].max()+offset, 100)\n",
    "fig1.add_scatter(x=xtest, \n",
    "                 y=sigmoid(best_w[1] * xtest + best_w[0]), \n",
    "                 mode='lines', line=dict(color='black', width=4),\n",
    "                 name=f'Logistic Model (w=({best_w[0]:.2f}, {best_w[1]:.2f}))')\n",
    "fig1.update_layout(height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74267539",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Visualizing the Gradient of the Error Function</h2>\n",
    "\n",
    "In the following, we will visualize the gradient of the error function on top of the error surface.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32241b34",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Gradient of the Cross Entropy Loss for Logistic Regression</h3>\n",
    "\n",
    "Recall that the logistic regression model is given by (here we drop the bias term for simplicity):\n",
    "$$\n",
    "p(t=1|x) = \\sigma(w^T x) = \\frac{1}{1 + e^{-(w^\\top x )}}\n",
    "$$\n",
    "To compute the gradient of the negative log likelihood loss function (cross entropy loss), we need to first define the loss function. To simplify some of the future steps we will use the **average** negative log likelihood loss function.\n",
    "\n",
    "\\begin{align*}\n",
    "L(w) &= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\log(p(t_i|w, x_i)) + (1 - t_i) \\log(1 - p(t_i|w, x_i)) \\right) \\\\\n",
    "L(w) &= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\log(\\sigma(w^\\top x_i)) + (1 - t_i) \\log(1 - \\sigma(w^\\top x_i)) \\right) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8a385",
   "metadata": {},
   "source": [
    "There is a useful identity that we can use to simplify the computation of the gradient and Hessian.  The derivative of the sigmoid function is given by:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z} \\sigma(z) = \\sigma(z)(1 - \\sigma(z)) = \\sigma(z)\\sigma(-z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce1c7b",
   "metadata": {},
   "source": [
    "So we can compute the $j^{th}$ term of the gradient as follows:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_j} \n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\frac{\\partial }{\\partial w_j}\\log(\\sigma(w^T x_i)) + (1 - t_i) \\frac{\\partial }{\\partial w_j}\\log(1 - \\sigma(w^T x_i)) \\right) \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\frac{1}{\\sigma(w^T x_i)} \\frac{\\partial }{\\partial w_j}\\sigma(w^T x_i) + (1 - t_i) \\frac{1}{1 - \\sigma(w^T x_i)} \\frac{\\partial }{\\partial w_j}(1 - \\sigma(w^T x_i)) \\right)\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i \\frac{1}{\\sigma(w^T x_i)} \\sigma(w^T x_i)(1 - \\sigma(w^T x_i)) x_i + (1 - t_i) \\frac{1}{1 - \\sigma(w^T x_i)} (-\\sigma(w^T x_i)(1 - \\sigma(w^T x_i))) x_{ij} \\right)\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i (1 - \\sigma(w^T x_i)) x_{ij} - (1 - t_i) \\sigma(w^T x_i) x_{ij} \\right)\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i x_{ij} - \\sigma(w^T x_i) x_{ij} \\right)\\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^N \\left( t_i x_{ij} - \\sigma(w^T x_i) x_{ij} \\right)\\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\left(\\sigma(w^T x_i) - t_i \\right) x_{ij}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d35026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_NLL(w):\n",
    "    \"\"\"Compute the gradient of the negative log-likelihood.\"\"\"\n",
    "    p = logistic_model(w, cancer['x'])\n",
    "    grad = np.mean((p - cancer['t']).reshape(-1, 1) * cancer['x'], 0)\n",
    "    return grad\n",
    "\n",
    "grad_NLL(np.array([-1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de847b98",
   "metadata": {},
   "source": [
    "We can now visualize the gradient of the loss function for this more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b6f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "(cancer['dw1'], cancer['dw2']) = np.array(\n",
    "    [grad_NLL(w) for w in cancer['ws']]).T\n",
    "cancer['dw1'] = cancer['dw1'].reshape(cancer['w1'].shape)\n",
    "cancer['dw2'] = cancer['dw2'].reshape(cancer['w1'].shape)\n",
    "# fig.write_image(\"images/loss_surface_3d_with_gradients.pdf\", width=800, height=800)\n",
    "fig = plot_gradient(cancer['w1'], cancer['w2'], cancer['error'], cancer['dw1'], cancer['dw2'], scale=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d3aa3",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Gradient Squared Error for our Sine Model</h3>\n",
    "\n",
    "Recall that our sine model is given by:\n",
    "\\begin{align}\n",
    "y = f(x) = \\sin(w_0  + w_1 x)\n",
    "\\end{align}\n",
    "and our squared loss function is given by:\n",
    "\n",
    "$$\n",
    "E(w;\\mathcal{D}) = L\\left(f_w; \\mathcal{D} = \\left\\{(x_n, y_n) \\right\\}_{n=1}^N\\right) = \\frac{1}{N} \\sum_{n=1}^N\\left(y_n - f_w\\left(x_n\\right)\\right)^2\n",
    "= \\frac{1}{N} \\sum_{n=1}^N\\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right)^2\n",
    "$$\n",
    "\n",
    "Taking the gradient we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla E(w;\\mathcal{D}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial E}{\\partial w_0} \\\\\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Calculating each term we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_0}\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N 2\\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\left(-\\cos\\left(w_0 + w_1 x_n \\right)\\right) \\\\\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_  1 x_n \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "&= \\frac{1}{N} \\sum_{n=1}^N 2\\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\left(-\\cos\\left(w_0 + w_1 x_n \\right)\\right) x_n \\\\\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_1 x_n \\right) x_n\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_sine_MSE(w):\n",
    "    \"\"\"Compute the gradient of the negative log-likelihood.\"\"\"\n",
    "    x = sine['x']\n",
    "    y = sine['y']\n",
    "    y_hat = sine_model(w, x)\n",
    "    grad_w0 = -2 * np.mean((y - y_hat) * np.cos(w[0] + w[1] * x))\n",
    "    grad_w1 = -2 * np.mean((y - y_hat) * x * np.cos(w[0] + w[1] * x))\n",
    "    return np.array([grad_w0, grad_w1])\n",
    "\n",
    "grad_sine_MSE(np.array([0, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6081f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sine['dw0'], sine['dw1']) = np.array(\n",
    "    [grad_sine_MSE(w) for w in sine['ws']]).T\n",
    "sine['dw0'] = sine['dw0'].reshape(sine['w1'].shape)\n",
    "sine['dw1'] = sine['dw1'].reshape(sine['w1'].shape)\n",
    "fig = plot_gradient(sine['w0'], sine['w1'], sine['error'], sine['dw0'], sine['dw1'], scale=0.1)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73316cae",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">The Gradient Descent Algorithm</h2>\n",
    "\n",
    "Here we implement the most basic version of gradient descent.  This version uses a fixed step size and does not use any fancy tricks like momentum or adaptive step sizes (which we will see soon).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_0, gradient, \n",
    "    learning_rate=1, nepochs=10, epsilon=1e-6):\n",
    "    \"\"\"Basic gradient descent algorithm.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        learning_rate: Step size for each iteration.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w_old = w_0\n",
    "    path = [w_old]\n",
    "    for e in range(nepochs):\n",
    "        w = w_old - learning_rate * gradient(w_old)\n",
    "        path.append(w)\n",
    "        if np.linalg.norm(w - w_old) < epsilon: break\n",
    "        w_old = w\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52134dd6",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Gradient Descent Applied to Logistic Regression</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706338ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0 = np.array([-10., -5.])\n",
    "# w0 = np.array([-1., 2.])\n",
    "w0 = np.array([0., 0])\n",
    "path = gradient_descent(w0, \n",
    "                     grad_NLL,\n",
    "                     learning_rate=10, \n",
    "                     nepochs=100)\n",
    "errors = [neg_log_likelihood(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    cancer['w1'], cancer['w2'], cancer['error'], \n",
    "    cancer['dw1'], cancer['dw2'], scale=2)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c983877",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Gradient Descent Applied to Sine Model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.2, 2.])\n",
    "w0 = np.array([.5, 2.5])\n",
    "w0 = np.array([1, 3])\n",
    "path = gradient_descent(w0, \n",
    "                        grad_sine_MSE,\n",
    "                        learning_rate=.2, \n",
    "                        nepochs=20)\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca55003",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Slido Visualization</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff11210",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = x**2 + 1\n",
    "fig = px.line(x=x, y=y, labels={'x': 'x', 'y': 'f(x)'})\n",
    "fig.update_traces(line_width=5)\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * x\n",
    "w0 = 2\n",
    "path = gradient_descent(w0, grad_f, learning_rate=1.08, nepochs=10)\n",
    "fig.add_scatter(x=path, y=path**2 + 1, mode='markers+lines',\n",
    "                marker=dict(size=10, color='red'),\n",
    "                line=dict(color='black', width=2, dash=\"dash\"),\n",
    "                name='Gradient Descent Path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241b82e",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">The Second Order Structure</h2>\n",
    "\n",
    "Here we examine the second order structure of the loss function through a Taylor expansion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521d4e4",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Hessian of the Logistic Regression Model</h3>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{H} &= \\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} & \\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 E}{\\partial w_1^2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We can start with the gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_j} \n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\left( p(t_i|x_i) - t_i  \\right) x_{ij} \n",
    "= \\frac{1}{N} \\sum_{i=1}^N \\left(  \\sigma(w^T x_i) - t_i \\right) x_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "Using the product rule and chain rule, we can compute the second derivatives of the loss function with respect to the parameters $w_k$:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 }{\\partial w_k} \\frac{\\partial L}{\\partial w_j}   \n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial^2 }{\\partial w_k}\\left(  \\sigma(w^T x_i) - t_i \\right) x_{ij}\\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N  x_{ij} \\frac{\\partial^2 }{\\partial w_k}\\sigma(w^T x_i) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N  x_{ij} \\sigma(w^T x_i)(1 - \\sigma(w^T x_i)) x_{ik}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd45192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_NLL(w):\n",
    "    \"\"\"Compute the Hessian of the negative log-likelihood.\"\"\"\n",
    "    x = cancer['x']\n",
    "    p = logistic_model(w, x)\n",
    "    hessian = (x.T @ np.diag(p * (1 - p)) @ x ) / len(p)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414d3c5",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Hessian of the Sine Regression Model</h3>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{H} &= \\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} & \\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 E}{\\partial w_1^2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We can start with the gradient:\n",
    "Calculating each term we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_0}\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_  1 x_n \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_1 x_n \\right) x_n\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Using the product rule and chain rule, we can compute the second derivatives of the loss function with respect to the parameters $w_0$ and $w_1$.\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) \\right]\\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1^2} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) x_n^2 + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) x_n^2 \\right]\\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) x_n + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) x_n \\right] \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b98f4",
   "metadata": {},
   "source": [
    "We can also derive the gradient using a symbolic algebra library.  This is a powerful technique that allows us to compute the gradient of a function without having to derive it by hand.  We will use the `sympy` library to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ca7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "# define our symbols\n",
    "w0, w1, x, y = sp.symbols('w0 w1 x y')\n",
    "# Define a symbolic expression for the error\n",
    "E = (y - sp.sin(w0 + w1 * x))**2\n",
    "# Compute the gradient of E with respect to w0 and w1\n",
    "gE = [sp.diff(E, var) for var in (w0, w1)]\n",
    "gE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_E = sp.hessian(E, (w0, w1))\n",
    "hessian_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_sine_MSE(w):\n",
    "    \"\"\"Compute the Hessian of the negative log-likelihood.\"\"\"\n",
    "    x = sine['x']\n",
    "    y = sine['y']\n",
    "    dw0dw0 = 2 * np.mean(np.cos(w[0] + w[1] * x)**2 + \n",
    "                (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    dw0dw1 = 2 * np.mean(x * np.cos(w[0] + w[1] * x)**2 +\n",
    "                x * (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    dw1dw1 = 2 * np.mean(x**2 * np.cos(w[0] + w[1] * x)**2 + \n",
    "                (x**2) * (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    hessian = np.array([[dw0dw0, dw0dw1], [dw0dw1, dw1dw1]])\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_sine_MSE(np.array([1.1, 2.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d965af6",
   "metadata": {},
   "source": [
    "Using sympy to do everything algebraically we get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sp.symbols('n', integer=True)    # number of data points (symbolic length)\n",
    "i = sp.Idx('i', n)                   # index variable for summation\n",
    "w0, w1 = sp.symbols('w0 w1')         # weights\n",
    "x = sp.IndexedBase('x', shape=(n,))  # indexed variable for x data\n",
    "y = sp.IndexedBase('y', shape=(n,))  # indexed variable for y data\n",
    "\n",
    "E_i = (y[i] - sp.sin(w0 + w1 * x[i]))**2\n",
    "E = sp.summation(E_i, (i, 0, n - 1)) / n\n",
    "H = sp.hessian(E, (w0, w1))\n",
    "\n",
    "Hfun = sp.lambdify((w0, w1, x, y, n), H)\n",
    "def hessian_sine_MSE2(w):\n",
    "    return np.array(Hfun(w[0], w[1], sine['x'], sine['y'], len(sine['x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e533aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_sine_MSE2(np.array([1.1, 2.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84553c",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Taylor Expansion of the Loss</h3>\n",
    "\n",
    "The Taylor expansion of a function $f(x)$ around a point $x=a$ is given by:\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + \\nabla f(a)^T (x - a) + \\frac{1}{2} (x - a)^T H(a) (x - a) + \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_loss(w, w_star, L_star, g_star, H_star):\n",
    "    delta = w - w_star\n",
    "    return (L_star + delta @ g_star + 1/2 * delta.T @ H_star @ delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7e293",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Applied to Logistic Regression</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_star = np.array([-2., -2.])\n",
    "w_star = cancer['grid_best']\n",
    "L_star = neg_log_likelihood(w_star)\n",
    "g_star = grad_NLL(w_star)\n",
    "H_star = hessian_NLL(w_star)\n",
    "s,u = np.linalg.eigh(H_star)\n",
    "print(\"w_star:\", w_star)\n",
    "print(\"L_star:\", L_star)\n",
    "print(\"g_star:\", g_star)\n",
    "print(\"H_star:\\n\", H_star)\n",
    "print(\"Eigenvalues of H_star:\", s)\n",
    "print(\"Eigegenvectors of H_star:\\n\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c2f06",
   "metadata": {},
   "source": [
    "Visualizing the Hessian at the optimal point we see that it is positive definite (both eigenvalues are positive).  This is consistent with the fact that the loss function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0789160",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gradient(cancer['w1'], cancer['w2'], \n",
    "                    cancer['error'], cancer['dw1'], cancer['dw2'])\n",
    "\n",
    "cancer['taylor_loss'] = np.array([\n",
    "    taylor_loss(w, w_star, L_star, g_star, H_star)\n",
    "for w in cancer['ws']]).reshape(cancer['w1'].shape)\n",
    "\n",
    "fig.add_surface(z=cancer['taylor_loss'], x=cancer['w1'], y=cancer['w2'], \n",
    "                 colorscale='plasma_r', opacity=0.5, showscale=False,\n",
    "                 contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                      start=cancer['taylor_loss'].min(), end=cancer['taylor_loss'].max(), \n",
    "                                      size=(cancer['taylor_loss'].max()-cancer['taylor_loss'].min())/50)), \n",
    "                                      row=1, col=1)\n",
    "fig.update_layout(scene=dict(zaxis=dict(range=[0, 3])))\n",
    "# fig.write_image(\"images/loss_surface_3d_with_gradients.pdf\", width=800, height=800)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc514da",
   "metadata": {},
   "source": [
    "We can visualize the eigenvectors of the Hessian at the optimal point on the quadratic approximation.  The eigenvectors give us the directions of curvature of the loss function.  The eigenvalues give us the amount of curvature in each direction.\n",
    "\n",
    "The contours of the ellipse of the quadratic approximation are given by the equation:\n",
    "$$\n",
    "\\sum_i \\lambda_i \\alpha_i^2 = \\text{const}.\n",
    "$$\n",
    "If we solve for the direction of the $i^{th}$ eigenvector we get:\n",
    "$$\n",
    "\\alpha_i = \\pm \\sqrt{\\frac{\\text{const}}{\\lambda_i}}\n",
    "$$\n",
    "We can visualize the eigenvectors of the Hessian at the optimal point on the quadratic approximation. The eigenvectors give us the directions of curvature of the loss function. The eigenvalues give us the amount of curvature in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86763384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_contour(z=cancer['taylor_loss'].flatten(), x=cancer['w1'].flatten(), \n",
    "                y=cancer['w2'].flatten(),\n",
    "                colorscale='viridis_r', opacity=0.5,\n",
    "                contours=dict(start=cancer['taylor_loss'].min(), end=cancer['taylor_loss'].max(), \n",
    "                            size=(cancer['taylor_loss'].max()-cancer['taylor_loss'].min())/50),\n",
    "                colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "scaling = 0.5\n",
    "lam, U = np.linalg.eigh(H_star)\n",
    "scale = scaling / np.sqrt(np.abs(lam))\n",
    "\n",
    "cx, cy = cancer['grid_best']\n",
    "for i, (lami, ui, si) in enumerate(zip(lam, U.T, scale), start=1):\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx + si*ui[0]],\n",
    "        y=[cy, cy + si*ui[1]],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        name=f'u{i} (Î»={lami:.3g})'\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx - si*ui[0]],\n",
    "        y=[cy, cy - si*ui[1]],\n",
    "        mode='lines',\n",
    "        line=dict(width=2, dash='dot'),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Taylor Approximation of Loss\",\n",
    "                  xaxis_title='w1', yaxis_title='w2',\n",
    "                  height=600, width=1200)\n",
    "# fig.write_image(\"images/taylor_approx_loss_contours.pdf\", width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a1542",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Applied to Sine Regression</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc50da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_star = np.array([-2., -2.])\n",
    "sine['w_star'] = sine['grid_best']\n",
    "sine['L_star'] = sine_MSE(sine['w_star'])\n",
    "sine['g_star'] = grad_sine_MSE(sine['w_star'])\n",
    "sine['H_star'] = hessian_sine_MSE(sine['w_star'])\n",
    "sine['lam'], sine['U'] = np.linalg.eigh(sine['H_star'])\n",
    "print(\"w_star:\", sine['w_star'])\n",
    "print(\"L_star:\", sine['L_star'])\n",
    "print(\"g_star:\", sine['g_star'])\n",
    "print(\"H_star:\\n\", sine['H_star'])\n",
    "print(\"Eigenvalues of H_star:\", sine['lam'])\n",
    "print(\"Eigenvectors of H_star:\\n\", sine['U'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc3e87",
   "metadata": {},
   "source": [
    "Visualizing the Hessian at the optimal point we see that it is positive definite (both eigenvalues are positive).  This is consistent with the fact that the loss function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gradient(sine['w0'], sine['w1'], \n",
    "                    sine['error'], sine['dw0'], sine['dw1'], scale=0.1)\n",
    "\n",
    "sine['taylor_loss'] = np.array([\n",
    "    taylor_loss(w, sine['w_star'], sine['L_star'], sine['g_star'], sine['H_star'])\n",
    "for w in sine['ws']]).reshape(sine['w1'].shape)\n",
    "\n",
    "fig.add_surface(z=sine['taylor_loss'], x=sine['w0'], y=sine['w1'], \n",
    "                 colorscale='plasma_r', opacity=0.5, showscale=False,\n",
    "                 contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                      start=sine['taylor_loss'].min(), end=sine['taylor_loss'].max(), \n",
    "                                      size=(sine['taylor_loss'].max()-sine['taylor_loss'].min())/50)), \n",
    "                                      row=1, col=1)\n",
    "fig.update_layout(scene=dict(zaxis=dict(range=[0, 3])))\n",
    "# fig.write_image(\"images/loss_surface_3d_with_gradients.pdf\", width=800, height=800)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a949fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_contour(z=sine['taylor_loss'].flatten(), x=sine['w0'].flatten(), \n",
    "                y=sine['w1'].flatten(),\n",
    "                colorscale='viridis_r', opacity=0.5,\n",
    "                contours=dict(start=sine['taylor_loss'].min(), end=sine['taylor_loss'].max(), \n",
    "                            size=(sine['taylor_loss'].max()-sine['taylor_loss'].min())/50),\n",
    "                colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "scaling = 0.5\n",
    "scale = scaling / np.sqrt(np.abs(sine['lam']))\n",
    "\n",
    "cx, cy = sine['grid_best']\n",
    "for i, (lami, ui, si) in enumerate(zip(sine['lam'], sine['U'].T, scale), start=1):\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx + si*ui[0]],\n",
    "        y=[cy, cy + si*ui[1]],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        name=f'u{i} (Î»={lami:.3g})'\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx - si*ui[0]],\n",
    "        y=[cy, cy - si*ui[1]],\n",
    "        mode='lines',\n",
    "        line=dict(width=2, dash='dot'),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Taylor Approximation of Loss\",\n",
    "                  xaxis_title='w1', yaxis_title='w2',\n",
    "                  height=600, width=1200)\n",
    "# fig.write_image(\"images/taylor_approx_loss_contours.pdf\", width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7000b5",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Oscillating Loss Functions</h2>\n",
    "\n",
    "Here we visualize what happens when we have a poorly conditioned quadratic loss function.  This can happen when the Hessian has very different eigenvalues.  In this case, gradient descent can oscillate and take a long time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63494aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_bad = np.array([[.1, 0], [0, 1]])\n",
    "def quad(w):\n",
    "    \"\"\"A poorly conditioned quadratic function.\"\"\"\n",
    "    return np.sum((w @ H_bad) * w,1)\n",
    "    \n",
    "def grad_quad(w):\n",
    "    \"\"\"Gradient of a poorly conditioned quadratic function.\"\"\"\n",
    "    return np.array(w @ H_bad * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2 = np.meshgrid(np.linspace(-5, 5, 30), np.linspace(-5, 5, 30))\n",
    "ws = np.hstack([w1.reshape(-1, 1), w2.reshape(-1, 1)]) \n",
    "error =  quad(ws).reshape(w1.shape)\n",
    "contour = go.Contour(x=w1.flatten(), y=w2.flatten(), z=error.flatten(), colorscale='Viridis_r',\n",
    "                contours=dict(start=0, end=20, size=.5))\n",
    "go.Figure(data=contour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c733a",
   "metadata": {},
   "source": [
    "Suppose we start at the point (-4,0) and use a learning rate of 1.  This is an ideal point and we can visualize the path taken by gradient descent on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4., 0])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=1, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4220de8",
   "metadata": {},
   "source": [
    "What happens if we start a different point and a learning rate of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae431fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=1, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638c4e3",
   "metadata": {},
   "source": [
    "Or if we decrease the learning rate slightly to .9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=.9, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a27684",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Gradient Descent with Momentum</h2>\n",
    "\n",
    "\n",
    "Here we implement gradient descent with momentum.  This is a simple modification to the basic gradient descent algorithm that can help with oscillations and speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momentum(w_0, gradient, \n",
    "    learning_rate=1, nepochs=10, epsilon=1e-6, momentum=0.9):\n",
    "    \"\"\"Gradient descent with momentum.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        learning_rate: Step size for each iteration.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "        momentum: Momentum factor.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w_old = w_0\n",
    "    path = [w_old]\n",
    "    v = np.zeros_like(w_old)\n",
    "    for e in range(nepochs):\n",
    "        g = gradient(w_old)\n",
    "        v = momentum * v - learning_rate * g\n",
    "        w = w_old + v\n",
    "        path.append(w)\n",
    "        if np.linalg.norm(w - w_old) < epsilon: break\n",
    "        w_old = w\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf26dc1",
   "metadata": {},
   "source": [
    "Let's apply gradient descent with momentum to the poorly conditioned quadratic loss function from before.  We will start at the point (-4,-2) and use a learning rate of .9 and a momentum factor of .3.  We can visualize the path taken by gradient descent with momentum on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gd_momentum(w0, grad_quad, \n",
    "                   learning_rate=.9, \n",
    "                   momentum=0.3, \n",
    "                   nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad30b59",
   "metadata": {},
   "source": [
    "Try setting momentum to 0 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7629a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([2.2, 2.2])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "# w0 = np.array([1, 3])\n",
    "path = gd_momentum(w0, \n",
    "                   grad_sine_MSE,\n",
    "                   learning_rate=.1, \n",
    "                   momentum=0.9, \n",
    "                   nepochs=50)\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4022b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
