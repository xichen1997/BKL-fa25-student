{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f55c784",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 13: Gradient Descent and SGD – CS 189, Fall 2025</h1>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae09b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly import figure_factory as ff\n",
    "colors = px.colors.qualitative.Plotly\n",
    "px.defaults.width = 800\n",
    "from ipywidgets import HBox\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the images folder if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc0ccb",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Plotting Code you Can Safely Ignore (but need to run).</h2>\n",
    "\n",
    "This lecture has many complex visualizations and to keep the rest of the code short I have put the visualization code here.  Much of this code is out-of-scope for the course, but feel free to look through it if you are interested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd69551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_grid(figs, rows, cols):\n",
    "    \"\"\"Create a grid of figures with Plotly figures.\"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "    def get_trace_type(fig):\n",
    "        for trace in fig.data:\n",
    "            if trace.type == 'surface':\n",
    "                return 'surface'  # required for go.Surface\n",
    "            elif trace.type.startswith('scatter3d') or trace.type.startswith('mesh3d'):\n",
    "                return 'scene'  # 3D scene\n",
    "        return 'xy'  # default 2D\n",
    "    specs = [[{'type': get_trace_type(fig)} for fig in figs[i:i+cols]] for i in range(0, len(figs), cols)]\n",
    "    fig_grid = make_subplots(rows=rows, cols=cols, specs=specs,\n",
    "                             subplot_titles=[fig.layout.title.text for fig in figs])\n",
    "    for i, fig in enumerate(figs):\n",
    "        fig_grid.add_traces(fig.data, rows=(i//cols) + 1, cols=(i%cols) + 1 )\n",
    "    return fig_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_predictions(w, cancer):\n",
    "    \"\"\"Plot predictions of the logistic model.\"\"\"\n",
    "    cancer = cancer.copy()\n",
    "    cancer['logistic_pred'] = np.where(\n",
    "        logistic_model(w, cancer[['mean radius', 'mean texture']].values) > 0.5,\n",
    "        'Pred Malignant', 'Pred Benign'\n",
    "    )\n",
    "    # Create a scatter plot with logistic predictions\n",
    "    fig = px.scatter(cancer, x='mean radius', y='mean texture', \n",
    "                     symbol='logistic_pred', color='target',\n",
    "                     symbol_sequence=[ \"circle-open\", \"cross\"])\n",
    "    for (i,t) in enumerate(fig.data): t.legendgroup = str(i)\n",
    "    # decision boundary\n",
    "    xs = np.linspace(cancer['mean radius'].min(), cancer['mean radius'].max(), 100)\n",
    "    decision_boundary = -(w[0] * xs ) / w[1]\n",
    "    fig.add_scatter(x=xs, y=decision_boundary, mode='lines', \n",
    "                    name='Decision Boundary', legendgroup='Decision Boundary',\n",
    "                    line=dict(color='black', width=2, dash='dash', ))\n",
    "    # probability surface\n",
    "    ys = np.linspace(cancer['mean texture'].min(), cancer['mean texture'].max(), 100)\n",
    "    X, Y = np.meshgrid(xs, ys)\n",
    "    Z = logistic_model(w, np.c_[X.ravel(), Y.ravel()]).reshape(X.shape)\n",
    "    fig.add_contour(x=xs, y=ys, z=Z, \n",
    "                    colorscale='Matter_r', opacity=0.5,\n",
    "                    name='Probability Surface', \n",
    "                    colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "\n",
    "    fig.update_layout(title=f'w=({w[0]:.2f}, {w[1]:.2f})',\n",
    "                      xaxis_range=[xs.min(), xs.max()], yaxis_range=[ys.min(), ys.max()],\n",
    "                      xaxis_title='Mean Radius (scaled)', yaxis_title='Mean Texture (scaled)',\n",
    "                      width=800, height=600)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fdb013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(w1, w2, error, ncontours=50):\n",
    "    surf_fig = go.Figure()\n",
    "    surf_fig.add_surface(z=error, x=w1, y=w2, \n",
    "                         colorscale='Viridis_r', opacity=0.7, showscale=False,\n",
    "                         contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                          start=error.min(), end=error.max(), \n",
    "                                          size=(error.max()-error.min())/ncontours)))\n",
    "    surf_fig.update_layout(title=\"Loss Surface\")\n",
    "    contour_fig = go.Figure()\n",
    "    contour_fig.add_contour(x=w1.flatten(), y=w2.flatten(), z=error.flatten(),\n",
    "                            colorscale='Viridis_r', opacity=0.7,\n",
    "                            contours=dict(start=error.min(), end=error.max(), \n",
    "                                          size=(error.max()-error.min())/ncontours),\n",
    "                            colorbar=dict(x=1.05, y=0.35, len=0.75))\n",
    "    contour_fig.update_layout(title=\"Loss Contours\")\n",
    "    fig = make_plot_grid([surf_fig, contour_fig], 1, 2).update_layout(height=800)\n",
    "    fig.update_layout(scene=dict(xaxis_title='w1', yaxis_title='w2', zaxis_title='Loss', aspectmode='cube'))\n",
    "    fig.update_layout(xaxis_range=[w1.min(), w1.max()], yaxis_range=[w2.min(), w2.max()],\n",
    "                      xaxis_title='w1', yaxis_title='w2')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(w1, w2, error, dw1, dw2, scale=1.0):\n",
    "    fig = plot_loss(w1, w2, error)\n",
    "    fig.add_trace(\n",
    "        go.Cone(\n",
    "            x=w1.flatten(), y=w2.flatten(), z=np.zeros_like(error).flatten(),  # Ground plane\n",
    "            u=dw1.flatten(), v=dw2.flatten(), w=np.zeros_like(error).flatten(),  # No vertical component\n",
    "            sizeref=2, anchor=\"tail\", showscale=False\n",
    "        ), 1,1)\n",
    "    contour_fig = ff.create_quiver(\n",
    "        x=w1.flatten(), y=w2.flatten(), u=dw1.flatten(), v=dw2.flatten(), \n",
    "        line_width=2, line_color=\"white\",\n",
    "        scale = scale, arrow_scale=.2, showlegend=False)\n",
    "    fig.add_traces(contour_fig.data, rows=1, cols=2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_solution_path(fig, errors, ws):\n",
    "    s = np.linspace(0, 1, len(ws))\n",
    "    fig.add_scatter3d(x=ws[:, 0], y=ws[:, 1], z=errors, marker_color=s, marker_size=5,\n",
    "                    mode='lines+markers', line=dict(color='black', width=2), opacity=0.5,\n",
    "                    name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                    row=1, col=1)\n",
    "    fig.add_scatter(x=ws[:, 0], y=ws[:, 1], marker_color=s,\n",
    "                    mode='lines+markers', line=dict(color='black', width=2), opacity=0.5,\n",
    "                    name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                    showlegend=False,\n",
    "                    row=1, col=2)\n",
    "    fig.add_scatter3d(x=[ws[-1, 0]], y=[ws[-1, 1]], z=[errors[-1]],\n",
    "                       mode='markers', marker=dict(color='red', size=10),\n",
    "                       name='Final Solution', legendgroup='Final Solution',\n",
    "                       row=1, col=1)\n",
    "    fig.add_scatter(x=[ws[-1, 0]], y=[ws[-1, 1]],\n",
    "                       mode='markers', marker=dict(color='red', size=20),\n",
    "                       name='Final Solution', legendgroup='Final Solution',\n",
    "                       showlegend=False,\n",
    "                       row=1, col=2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618368d",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Lecture 12 Setup</h2>\n",
    "\n",
    "In the previous lecture we visualized the loss surfaces and derived gradient descent.  In this part of the notebook we setup the data, loss, and gradient descent code that we will use to visualize gradient descent and stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fbc5e1",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Logistic Regression Problem </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cancer_dict = datasets.load_breast_cancer(as_frame=True)\n",
    "cancer_df = pd.DataFrame(cancer_dict.data, columns=cancer_dict.feature_names)\n",
    "cancer_df['target'] = cancer_dict.target.astype(str)\n",
    "cancer_df = cancer_df[['mean radius', 'mean texture', 'target']].dropna()\n",
    "scaler = StandardScaler()\n",
    "cancer_df[['mean radius', 'mean texture']] = scaler.fit_transform(cancer_df[['mean radius', 'mean texture']])\n",
    "print(\"The dataset:\", cancer_df.shape)\n",
    "display(cancer_df.head())\n",
    "\n",
    "cancer = dict()\n",
    "cancer['npts'] = 30\n",
    "cancer['w1'], cancer['w2'] = np.meshgrid(\n",
    "    np.linspace(-10, 1, cancer['npts']), \n",
    "    np.linspace(-5, 1.3, cancer['npts'])\n",
    ")\n",
    "cancer['ws'] = np.stack([cancer['w1'].flatten(), cancer['w2'].flatten()]).T\n",
    "cancer['x'] = cancer_df[['mean radius', 'mean texture']].values\n",
    "cancer['t'] = cancer_df['target'].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f44912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_model(w, x):\n",
    "    \"\"\"Logistic model for binary classification.\"\"\"\n",
    "    z = w @ x.T\n",
    "    return sigmoid(z)\n",
    "\n",
    "def NLL(w):\n",
    "    \"\"\"Negative log-likelihood for logistic regression.\"\"\"\n",
    "    x = cancer['x']\n",
    "    t = cancer['t']\n",
    "    z = w @ x.T\n",
    "    # return -np.mean(t * np.log(logistic_model(w, x)) + (1 - t) * np.log(1 - logistic_model(w, x)))\n",
    "    # more numerically stable version\n",
    "    # np.mean(np.log(1+ np.exp(z)) - t * z) \n",
    "    # stable softplus: log(1+exp(z))\n",
    "    softplus_z = np.logaddexp(0, z)\n",
    "    return np.mean(softplus_z - t * z)\n",
    "\n",
    "\n",
    "def grad_NLL(w, x=cancer['x'], t=cancer['t']):\n",
    "    \"\"\"Compute the gradient of the negative log-likelihood.\"\"\"\n",
    "    p = logistic_model(w, x)\n",
    "    grad = np.mean((p - t).reshape(-1, 1) * x, 0)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer['error'] = np.array([NLL(w) \n",
    "                for w in cancer['ws']])\n",
    "cancer['error'] = cancer['error'].reshape(cancer['w1'].shape)\n",
    "# guesses = np.array([[-1., -1.],\n",
    "#                     [-2., -4.]])\n",
    "best_ind = np.argmin(cancer['error'])\n",
    "cancer['grid_best'] = np.array([cancer['w1'].flatten()[best_ind], \n",
    "                                cancer['w2'].flatten()[best_ind]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a9da3e",
   "metadata": {},
   "source": [
    "Computing the gradient fields and visualizing gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(cancer['dw1'], cancer['dw2']) = np.array(\n",
    "    [grad_NLL(w) for w in cancer['ws']]).T\n",
    "cancer['dw1'] = cancer['dw1'].reshape(cancer['w1'].shape)\n",
    "cancer['dw2'] = cancer['dw2'].reshape(cancer['w1'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52715bc",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Squared Error Loss Squared Loss on a Non-Linear Model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine = dict()\n",
    "np.random.seed(42)\n",
    "sine['n'] = 200\n",
    "# Generate some random data\n",
    "sine['x'] = np.random.rand(sine['n']) * 2.5 * np.pi\n",
    "sine['x'] = np.sort(sine['x']) # sort for easier plotting\n",
    "sine['y'] = np.sin(1.1 + 2.5 * sine['x']) + 0.5 * np.random.randn(sine['n'])\n",
    "sine_df = pd.DataFrame({'x': sine['x'], 'y': sine['y']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22381025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_model(w, x):\n",
    "    return np.sin(w[0] + x * w[1])\n",
    "\n",
    "def sine_MSE(w):\n",
    "    x = sine['x']\n",
    "    y = sine['y']\n",
    "    y_hat = sine_model(w, x)\n",
    "    return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "def grad_sine_MSE(w, x=sine['x'], y=sine['y']):\n",
    "    \"\"\"Compute the gradient of the negative log-likelihood.\"\"\"\n",
    "    y_hat = sine_model(w, x)\n",
    "    grad_w0 = -2 * np.mean((y - y_hat) * np.cos(w[0] + w[1] * x))\n",
    "    grad_w1 = -2 * np.mean((y - y_hat) * x * np.cos(w[0] + w[1] * x))\n",
    "    return np.array([grad_w0, grad_w1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0165f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine['guesses'] = np.array([[0, 2], [2, 3], [0, 3.5]])\n",
    "sine['xhat'] = np.linspace(sine_df['x'].min(), sine_df['x'].max(), 100)\n",
    "sine['pred_df'] = pd.DataFrame({'x': sine['xhat']})\n",
    "for w in sine['guesses']:\n",
    "    sine['pred_df'][f'yhat(w={w})'] = sine_model(w, sine['xhat'])\n",
    "sine['pred_df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cac444",
   "metadata": {},
   "outputs": [],
   "source": [
    "sine['npts'] = 30\n",
    "sine['w0'], sine['w1'] = np.meshgrid(\n",
    "    np.linspace(-1.5, 3, sine['npts']), np.linspace(1, 4, sine['npts']))\n",
    "# combine w1 and w2 into a single tensor\n",
    "sine['ws'] = np.stack([sine['w0'].flatten(), sine['w1'].flatten()]).T\n",
    "\n",
    "sine['error'] = np.array([sine_MSE(w) for w in sine['ws']]).reshape(sine['w0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmin(sine['error'])\n",
    "sine['grid_best'] = sine['ws'][ind,:]\n",
    "sine['grid_best_error'] = sine['error'].flatten()[ind]\n",
    "print(f\"Best weights: {sine['grid_best']}, with error: {sine['grid_best_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854871d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sine['dw0'], sine['dw1']) = np.array(\n",
    "    [grad_sine_MSE(w) for w in sine['ws']]).T\n",
    "sine['dw0'] = sine['dw0'].reshape(sine['w1'].shape)\n",
    "sine['dw1'] = sine['dw1'].reshape(sine['w1'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73316cae",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">The Gradient Descent Algorithm</h2>\n",
    "\n",
    "Here we implement the most basic version of gradient descent.  This version uses a fixed step size and does not use any fancy tricks like momentum or adaptive step sizes (which we will see soon).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_0, gradient, \n",
    "    learning_rate=1, nepochs=10, epsilon=1e-6):\n",
    "    \"\"\"Basic gradient descent algorithm.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        learning_rate: Step size for each iteration.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w_old = w_0\n",
    "    path = [w_old]\n",
    "    for e in range(nepochs):\n",
    "        w = w_old - learning_rate * gradient(w_old)\n",
    "        path.append(w)\n",
    "        if np.linalg.norm(w - w_old) < epsilon: break\n",
    "        w_old = w\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52134dd6",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Gradient Descent Applied to Logistic Regression</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706338ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0 = np.array([-10., -5.])\n",
    "# w0 = np.array([-1., 2.])\n",
    "w0 = np.array([0., 0])\n",
    "path = gradient_descent(w0, \n",
    "                     grad_NLL,\n",
    "                     learning_rate=1, \n",
    "                     nepochs=100)\n",
    "errors = [NLL(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    cancer['w1'], cancer['w2'], cancer['error'], \n",
    "    cancer['dw1'], cancer['dw2'], scale=2)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c983877",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Gradient Descent Applied to Sine Model</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w0 = np.array([1.2, 2.])\n",
    "# w0 = np.array([.5, 2.5])\n",
    "w0 = np.array([2, 2])\n",
    "path = gradient_descent(w0, \n",
    "                        grad_sine_MSE,\n",
    "                        learning_rate=.01, \n",
    "                        nepochs=20)\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d317d9",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Loss Curves</h3>\n",
    "\n",
    "In practice, we are typically unable to visualize the loss surface.  Instead, we can plot the loss value as a function of iteration number.  This is called a loss curve.  Here we plot the loss curve for gradient descent on our sine model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_curve(path, error_func):\n",
    "    \"\"\"Make a loss curve from a path of weights.\"\"\"\n",
    "    errors = [error_func(w) for w in path]\n",
    "    fig = px.line(x=np.arange(len(errors)), y=errors, \n",
    "                  labels={'x': 'Iteration (Gradient Steps)', 'y': 'Loss (Error)'})\n",
    "    fig.update_traces(line_width=4)\n",
    "    fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([2.5, 1.9])\n",
    "path = gradient_descent(w0, \n",
    "                        grad_sine_MSE,\n",
    "                        learning_rate=.05, \n",
    "                        nepochs=50)\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n",
    "\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-10., -5.])\n",
    "# w0 = np.array([-1., 2.])\n",
    "# w0 = np.array([0., 0])\n",
    "path = gradient_descent(w0, \n",
    "                     grad_NLL,\n",
    "                     learning_rate=10, \n",
    "                     nepochs=100)\n",
    "\n",
    "fig_loss = make_loss_curve(path, NLL)\n",
    "fig_loss.show()\n",
    "\n",
    "errors = [NLL(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    cancer['w1'], cancer['w2'], cancer['error'], \n",
    "    cancer['dw1'], cancer['dw2'], scale=2)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca55003",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Convergence on the Parabola</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff11210",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = x**2 + 1\n",
    "fig = px.line(x=x, y=y, labels={'x': 'x', 'y': 'f(x)'})\n",
    "fig.update_traces(line_width=5)\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2 * x\n",
    "w0 = 2\n",
    "path = gradient_descent(w0, grad_f, learning_rate=1.1, nepochs=10)\n",
    "fig.add_scatter(x=path, y=path**2 + 1, mode='markers+lines',\n",
    "                marker=dict(size=10, color='red'),\n",
    "                line=dict(color='black', width=2, dash=\"dash\"),\n",
    "                name='Gradient Descent Path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241b82e",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">The Second Order Structure</h2>\n",
    "\n",
    "Here we examine the second order structure of the loss function through a Taylor expansion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521d4e4",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Hessian of the Logistic Regression Model</h3>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{H} &= \\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} & \\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 E}{\\partial w_1^2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We can start with the gradient:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial w_j} \n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\left( p(t_i|x_i) - t_i  \\right) x_{ij} \n",
    "= \\frac{1}{N} \\sum_{i=1}^N \\left(  \\sigma(w^T x_i) - t_i \\right) x_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "Using the product rule and chain rule, we can compute the second derivatives of the loss function with respect to the parameters $w_k$:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 }{\\partial w_k} \\frac{\\partial L}{\\partial w_j}   \n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial^2 }{\\partial w_k}\\left(  \\sigma(w^T x_i) - t_i \\right) x_{ij}\\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N  x_{ij} \\frac{\\partial^2 }{\\partial w_k}\\sigma(w^T x_i) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N  x_{ij} \\sigma(w^T x_i)(1 - \\sigma(w^T x_i)) x_{ik}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd45192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_NLL(w):\n",
    "    \"\"\"Compute the Hessian of the negative log-likelihood.\"\"\"\n",
    "    x = cancer['x']\n",
    "    p = logistic_model(w, x)\n",
    "    hessian = (x.T @ np.diag(p * (1 - p)) @ x ) / len(p)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414d3c5",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">The Hessian of the Sine Regression Model</h3>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{H} &= \\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} & \\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1 \\partial w_0} & \\frac{\\partial^2 E}{\\partial w_1^2}\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We can start with the gradient:\n",
    "Calculating each term we get:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_0}\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_  1 x_n \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial w_1}\n",
    "&= -\\frac{2}{N} \\sum_{n=1}^N \\left(y_n - \\sin\\left(w_0 + w_1 x_n \\right)\\right) \\cos\\left(w_0 + w_1 x_n \\right) x_n\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Using the product rule and chain rule, we can compute the second derivatives of the loss function with respect to the parameters $w_0$ and $w_1$.\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2 E}{\\partial w_0^2} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) \\right]\\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_1^2} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) x_n^2 + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) x_n^2 \\right]\\\\\n",
    "\\frac{\\partial^2 E}{\\partial w_0 \\partial w_1} &=\n",
    "\\frac{2}{N} \\sum_{n=1}^N \\left[ \\cos^2(w_0 + w_1 x_n) x_n + (y_n - \\sin(w_0 + w_1 x_n)) \\sin(w_0 + w_1 x_n) x_n \\right] \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e098b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_sine_MSE(w):\n",
    "    \"\"\"Compute the Hessian of the negative log-likelihood.\"\"\"\n",
    "    x = sine['x']\n",
    "    y = sine['y']\n",
    "    dw0dw0 = 2 * np.mean(np.cos(w[0] + w[1] * x)**2 + \n",
    "                (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    dw0dw1 = 2 * np.mean(x * np.cos(w[0] + w[1] * x)**2 +\n",
    "                x * (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    dw1dw1 = 2 * np.mean(x**2 * np.cos(w[0] + w[1] * x)**2 + \n",
    "                (x**2) * (y - np.sin(w[0] + w[1] * x)) * np.sin(w[0] + w[1] * x))\n",
    "    hessian = np.array([[dw0dw0, dw0dw1], [dw0dw1, dw1dw1]])\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b98f4",
   "metadata": {},
   "source": [
    "We can also derive the gradient using a symbolic algebra library.  This is a powerful technique that allows us to compute the gradient of a function without having to derive it by hand.  We will use the `sympy` library to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately on colab we need to upgrade sympy to get the needed functionality\n",
    "# !pip install --upgrade sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ca7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sympy as sp\n",
    "# # define our symbols\n",
    "# w0, w1, x, y = sp.symbols('w0 w1 x y')\n",
    "# # Define a symbolic expression for the error\n",
    "# E = (y - sp.sin(w0 + w1 * x))**2\n",
    "# # Compute the gradient of E with respect to w0 and w1\n",
    "# gE = [sp.diff(E, var) for var in (w0, w1)]\n",
    "# gE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hessian_E = sp.hessian(E, (w0, w1))\n",
    "# hessian_E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d965af6",
   "metadata": {},
   "source": [
    "Using sympy to do everything algebraically we get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b62aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = sp.symbols('n', integer=True)    # number of data points (symbolic length)\n",
    "# i = sp.Idx('i', n)                   # index variable for summation\n",
    "# w0, w1 = sp.symbols('w0 w1')         # weights\n",
    "# x = sp.IndexedBase('x', shape=(n,))  # indexed variable for x data\n",
    "# y = sp.IndexedBase('y', shape=(n,))  # indexed variable for y data\n",
    "\n",
    "# E_i = (y[i] - sp.sin(w0 + w1 * x[i]))**2\n",
    "# E = sp.summation(E_i, (i, 0, n - 1)) / n\n",
    "# H = sp.hessian(E, (w0, w1))\n",
    "\n",
    "# Hfun = sp.lambdify((w0, w1, x, y, n), H)\n",
    "# def hessian_sine_MSE2(w):\n",
    "#     return np.array(Hfun(w[0], w[1], sine['x'], sine['y'], len(sine['x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hessian_sine_MSE(np.array([1.1, 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e533aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hessian_sine_MSE2(np.array([1.1, 2.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84553c",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Taylor Expansion of the Loss</h3>\n",
    "\n",
    "The Taylor expansion of a function $f(x)$ around a point $x=a$ is given by:\n",
    "\n",
    "$$\n",
    "f(x) = f(a) + \\nabla f(a)^T (x - a) + \\frac{1}{2} (x - a)^T H(a) (x - a) + \\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_loss(w, w_star, L_star, g_star, H_star):\n",
    "    delta = w - w_star\n",
    "    return (L_star + delta @ g_star + 1/2 * delta.T @ H_star @ delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7e293",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Applied to Logistic Regression</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cdf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_star = np.array([-2., -2.])\n",
    "w_star = cancer['grid_best']\n",
    "L_star = NLL(w_star)\n",
    "g_star = grad_NLL(w_star)\n",
    "H_star = hessian_NLL(w_star)\n",
    "s,u = np.linalg.eigh(H_star)\n",
    "print(\"w_star:\", w_star)\n",
    "print(\"L_star:\", L_star)\n",
    "print(\"g_star:\", g_star)\n",
    "print(\"H_star:\\n\", H_star)\n",
    "print(\"Eigenvalues of H_star:\", s)\n",
    "print(\"Eigegenvectors of H_star:\\n\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c2f06",
   "metadata": {},
   "source": [
    "Visualizing the Hessian at the optimal point we see that it is positive definite (both eigenvalues are positive).  This is consistent with the fact that the loss function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0789160",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gradient(cancer['w1'], cancer['w2'], \n",
    "                    cancer['error'], cancer['dw1'], cancer['dw2'])\n",
    "\n",
    "cancer['taylor_loss'] = np.array([\n",
    "    taylor_loss(w, w_star, L_star, g_star, H_star)\n",
    "for w in cancer['ws']]).reshape(cancer['w1'].shape)\n",
    "\n",
    "fig.add_surface(z=cancer['taylor_loss'], x=cancer['w1'], y=cancer['w2'], \n",
    "                 colorscale='plasma_r', opacity=0.5, showscale=False,\n",
    "                 contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                      start=cancer['taylor_loss'].min(), end=cancer['taylor_loss'].max(), \n",
    "                                      size=(cancer['taylor_loss'].max()-cancer['taylor_loss'].min())/50)), \n",
    "                                      row=1, col=1)\n",
    "fig.update_layout(scene=dict(zaxis=dict(range=[0, 3])))\n",
    "# fig.write_image(\"images/loss_surface_3d_with_gradients.pdf\", width=800, height=800)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc514da",
   "metadata": {},
   "source": [
    "We can visualize the eigenvectors of the Hessian at the optimal point on the quadratic approximation.  The eigenvectors give us the directions of curvature of the loss function.  The eigenvalues give us the amount of curvature in each direction.\n",
    "\n",
    "The contours of the ellipse of the quadratic approximation are given by the equation:\n",
    "$$\n",
    "\\sum_i \\lambda_i \\alpha_i^2 = \\text{const}.\n",
    "$$\n",
    "If we solve for the direction of the $i^{th}$ eigenvector we get:\n",
    "$$\n",
    "\\alpha_i = \\pm \\sqrt{\\frac{\\text{const}}{\\lambda_i}}\n",
    "$$\n",
    "We can visualize the eigenvectors of the Hessian at the optimal point on the quadratic approximation. The eigenvectors give us the directions of curvature of the loss function. The eigenvalues give us the amount of curvature in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86763384",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_contour(z=cancer['taylor_loss'].flatten(), x=cancer['w1'].flatten(), \n",
    "                y=cancer['w2'].flatten(),\n",
    "                colorscale='viridis_r', opacity=0.5,\n",
    "                contours=dict(start=cancer['taylor_loss'].min(), end=cancer['taylor_loss'].max(), \n",
    "                            size=(cancer['taylor_loss'].max()-cancer['taylor_loss'].min())/50),\n",
    "                colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "scaling = 0.5\n",
    "lam, U = np.linalg.eigh(H_star)\n",
    "scale = scaling / np.sqrt(np.abs(lam))\n",
    "\n",
    "cx, cy = cancer['grid_best']\n",
    "for i, (lami, ui, si) in enumerate(zip(lam, U.T, scale), start=1):\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx + si*ui[0]],\n",
    "        y=[cy, cy + si*ui[1]],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        name=f'u{i} (λ={lami:.3g})'\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx - si*ui[0]],\n",
    "        y=[cy, cy - si*ui[1]],\n",
    "        mode='lines',\n",
    "        line=dict(width=2, dash='dot'),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Taylor Approximation of Loss\",\n",
    "                  xaxis_title='w1', yaxis_title='w2',\n",
    "                  height=600, width=1200)\n",
    "# fig.write_image(\"images/taylor_approx_loss_contours.pdf\", width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a1542",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h4 class=\"cal cal-h4\">Applied to Sine Regression</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc50da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_star = np.array([-2., -2.])\n",
    "sine['w_star'] = sine['grid_best']\n",
    "sine['L_star'] = sine_MSE(sine['w_star'])\n",
    "sine['g_star'] = grad_sine_MSE(sine['w_star'])\n",
    "sine['H_star'] = hessian_sine_MSE(sine['w_star'])\n",
    "sine['lam'], sine['U'] = np.linalg.eigh(sine['H_star'])\n",
    "print(\"w_star:\", sine['w_star'])\n",
    "print(\"L_star:\", sine['L_star'])\n",
    "print(\"g_star:\", sine['g_star'])\n",
    "print(\"H_star:\\n\", sine['H_star'])\n",
    "print(\"Eigenvalues of H_star:\", sine['lam'])\n",
    "print(\"Eigenvectors of H_star:\\n\", sine['U'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc3e87",
   "metadata": {},
   "source": [
    "Visualizing the Hessian at the optimal point we see that it is positive definite (both eigenvalues are positive).  This is consistent with the fact that the loss function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_gradient(sine['w0'], sine['w1'], \n",
    "                    sine['error'], sine['dw0'], sine['dw1'], scale=0.1)\n",
    "\n",
    "sine['taylor_loss'] = np.array([\n",
    "    taylor_loss(w, sine['w_star'], sine['L_star'], sine['g_star'], sine['H_star'])\n",
    "for w in sine['ws']]).reshape(sine['w1'].shape)\n",
    "\n",
    "fig.add_surface(z=sine['taylor_loss'], x=sine['w0'], y=sine['w1'], \n",
    "                 colorscale='plasma_r', opacity=0.5, showscale=False,\n",
    "                 contours=dict(z=dict(show=True, highlightcolor=\"white\", \n",
    "                                      start=sine['taylor_loss'].min(), end=sine['taylor_loss'].max(), \n",
    "                                      size=(sine['taylor_loss'].max()-sine['taylor_loss'].min())/50)), \n",
    "                                      row=1, col=1)\n",
    "fig.update_layout(scene=dict(zaxis=dict(range=[0, 3])))\n",
    "# fig.write_image(\"images/loss_surface_3d_with_gradients.pdf\", width=800, height=800)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a949fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_contour(z=sine['taylor_loss'].flatten(), x=sine['w0'].flatten(), \n",
    "                y=sine['w1'].flatten(),\n",
    "                colorscale='viridis_r', opacity=0.5,\n",
    "                contours=dict(start=sine['taylor_loss'].min(), end=sine['taylor_loss'].max(), \n",
    "                            size=(sine['taylor_loss'].max()-sine['taylor_loss'].min())/50),\n",
    "                colorbar=dict(x=1.05, y=0.3, len=0.75))\n",
    "scaling = 0.5\n",
    "scale = scaling / np.sqrt(np.abs(sine['lam']))\n",
    "\n",
    "cx, cy = sine['grid_best']\n",
    "for i, (lami, ui, si) in enumerate(zip(sine['lam'], sine['U'].T, scale), start=1):\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx + si*ui[0]],\n",
    "        y=[cy, cy + si*ui[1]],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        name=f'u{i} (λ={lami:.3g})'\n",
    "    )\n",
    "    fig.add_scatter(\n",
    "        x=[cx, cx - si*ui[0]],\n",
    "        y=[cy, cy - si*ui[1]],\n",
    "        mode='lines',\n",
    "        line=dict(width=2, dash='dot'),\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "fig.update_layout(title=\"Taylor Approximation of Loss\",\n",
    "                  xaxis_title='w1', yaxis_title='w2',\n",
    "                  height=600, width=1200)\n",
    "# fig.write_image(\"images/taylor_approx_loss_contours.pdf\", width=1200, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7000b5",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Oscillating Loss Functions</h2>\n",
    "\n",
    "Here we visualize what happens when we have a poorly conditioned quadratic loss function.  This can happen when the Hessian has very different eigenvalues.  In this case, gradient descent can oscillate and take a long time to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63494aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_bad = np.array([[.1, 0], [0, 1]])\n",
    "def quad(w):\n",
    "    \"\"\"A poorly conditioned quadratic function.\"\"\"\n",
    "    return np.sum((w @ H_bad) * w,1)\n",
    "    \n",
    "def grad_quad(w):\n",
    "    \"\"\"Gradient of a poorly conditioned quadratic function.\"\"\"\n",
    "    return np.array(w @ H_bad * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,w2 = np.meshgrid(np.linspace(-5, 5, 30), np.linspace(-5, 5, 30))\n",
    "ws = np.hstack([w1.reshape(-1, 1), w2.reshape(-1, 1)]) \n",
    "error =  quad(ws).reshape(w1.shape)\n",
    "contour = go.Contour(x=w1.flatten(), y=w2.flatten(), z=error.flatten(), colorscale='Viridis_r',\n",
    "                contours=dict(start=0, end=20, size=.5))\n",
    "go.Figure(data=contour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c733a",
   "metadata": {},
   "source": [
    "Suppose we start at the point (-4,0) and use a learning rate of 1.  This is an ideal point and we can visualize the path taken by gradient descent on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4., 0])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=1, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4220de8",
   "metadata": {},
   "source": [
    "What happens if we start a different point and a learning rate of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae431fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=1, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638c4e3",
   "metadata": {},
   "source": [
    "Or if we decrease the learning rate slightly to .9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gradient_descent(w0, grad_quad, learning_rate=.9, nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a27684",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Gradient Descent with Momentum</h2>\n",
    "\n",
    "\n",
    "Here we implement gradient descent with momentum.  This is a simple modification to the basic gradient descent algorithm that can help with oscillations and speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb3d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_momentum(w_0, gradient, \n",
    "    learning_rate=1, nepochs=10, epsilon=1e-6, momentum=0.9):\n",
    "    \"\"\"Gradient descent with momentum.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        learning_rate: Step size for each iteration.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "        momentum: Momentum factor.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w_old = w_0\n",
    "    path = [w_old]\n",
    "    v = np.zeros_like(w_old)\n",
    "    for e in range(nepochs):\n",
    "        g = gradient(w_old)\n",
    "        v = momentum * v - learning_rate * g\n",
    "        w = w_old + v\n",
    "        path.append(w)\n",
    "        if np.linalg.norm(w - w_old) < epsilon: break\n",
    "        w_old = w\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf26dc1",
   "metadata": {},
   "source": [
    "Let's apply gradient descent with momentum to the poorly conditioned quadratic loss function from before.  We will start at the point (-4,-2) and use a learning rate of .9 and a momentum factor of .3.  We can visualize the path taken by gradient descent with momentum on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90be606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-4.,-2.])\n",
    "path = gd_momentum(w0, grad_quad, \n",
    "                   learning_rate=.9, \n",
    "                   momentum=0.3, \n",
    "                   nepochs=50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(contour)\n",
    "#add arrows to lines\n",
    "fig.add_scatter(x=path[:, 0], y=path[:, 1],\n",
    "                mode='lines+markers', line=dict(color='black', width=2), \n",
    "                marker=dict(size=10, color='black', symbol= \"arrow-bar-up\", angleref=\"previous\"),\n",
    "                name='Gradient Descent Path', legendgroup='Gradient Descent',\n",
    "                showlegend=False)\n",
    "fig.update_layout(margin=dict(l=5, r=5, t=5, b=5))\n",
    "# fig.write_image(\"images/flat_quadratic.pdf\", width=1000, height=500)\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad30b59",
   "metadata": {},
   "source": [
    "We can also try momentum on our non-convex messy least squares regression problem (using a non-linear model).  Try setting momentum to 0 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7629a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([2.2, 2.2])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "w0 = np.array([1.5, 2])\n",
    "path = gd_momentum(w0, \n",
    "                   grad_sine_MSE,\n",
    "                   learning_rate=.1, \n",
    "                   momentum=0.9, \n",
    "                   nepochs=50)\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n",
    "\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde89e6",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Learning Rate Schedules</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b4e44",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Linear Schedule</h3>\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\eta_0 \\left(1 - \\frac{t}{T}\\right) + \\eta_{T} \\frac{t}{T}\n",
    "$$\n",
    "\n",
    "The constant $T$ is the total number of epochs.\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\eta_0$: initial learning rate\n",
    "- $\\eta_T$: final learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_learning_rate(initial_lr, T, end_lr=0):\n",
    "    \"\"\"Linearly decaying learning rate.\n",
    "    Args:\n",
    "        initial_lr: Initial learning rate.\n",
    "        T: Total number of iterations.\n",
    "        end_lr: Final learning rate.\n",
    "    Returns:\n",
    "        A function that takes the current iteration and returns the learning rate.\"\"\"\n",
    "    return lambda t: initial_lr * (1 - min(1, t / T)) + end_lr * min(1, t / T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e76022",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_learning_rate(initial_lr=0.1, T=100, end_lr=0.01)\n",
    "fig = px.line(x=np.arange(100), \n",
    "              markers=True,\n",
    "              y=[lr(epoch) for epoch in range(100)],\n",
    "              labels={'x': 'Epoch', 'y': 'Learning Rate'})\n",
    "fig.update_traces(line_width=4)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c24af",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Power-law Schedule</h3>\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\eta_0 \\left(1 + \\frac{t}{s}\\right)^{-c}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $s$: step size (in epochs) of the decay\n",
    "- $c$: power of the decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae39ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law_learning_rate(initial_lr, T, s=1, c=1):\n",
    "    \"\"\"Power-law decaying learning rate.\n",
    "    Args:\n",
    "        initial_lr: Initial learning rate.\n",
    "        T: Total number of epochs.\n",
    "        s: Scaling factor.\n",
    "        c: Exponent.\n",
    "    Returns:\n",
    "        A function that takes the current epoch and returns the learning rate.\"\"\"\n",
    "    return lambda t: initial_lr * (1 + t/s) ** (-c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = power_law_learning_rate(initial_lr=0.1, T=100, s=10, c=1)\n",
    "fig = px.line(x=np.arange(100), \n",
    "              markers=True,\n",
    "              y=[lr(epoch) for epoch in range(100)],\n",
    "              labels={'x': 'Epoch', 'y': 'Learning Rate'})\n",
    "fig.update_traces(line_width=4)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9172c3",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Exponential Schedule</h3>\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\eta_0 c^{\\lfloor t/s \\rfloor}\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $s$: step size (in epochs) of the decay\n",
    "- $c$: base of the exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_learning_rate(initial_lr, decay_rate=0.95, decay_steps=10):\n",
    "    \"\"\"Exponential decaying learning rate.\n",
    "    Args:\n",
    "        initial_lr: Initial learning rate.\n",
    "        decay_rate: Decay rate (0 < decay_rate < 1).\n",
    "        decay_steps: Number of steps before applying decay.\n",
    "    Returns:\n",
    "        A function that takes the current step and returns the learning rate.\"\"\"\n",
    "    return lambda t: initial_lr * decay_rate**(np.floor(t / decay_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578f6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = exponential_learning_rate(initial_lr=0.1, \n",
    "                               decay_rate=0.90, \n",
    "                               decay_steps=10)\n",
    "fig = px.line(x=np.arange(100), \n",
    "              markers=True,\n",
    "              y=[lr(epoch) for epoch in range(100)],\n",
    "              labels={'x': 'Epoch', 'y': 'Learning Rate'})\n",
    "fig.update_traces(line_width=4)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3521e",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Cosine Schedule</h3>\n",
    "\n",
    "$$\n",
    "\\eta^{(t)} = \\eta_{\\text{min}} + \\frac{1}{2}(\\eta_{\\text{max}} - \\eta_{\\text{min}}) \\left(1 + \\cos\\left(\\frac{t}{T} \\pi\\right)\\right)\n",
    "$$\n",
    "\n",
    "Hyperparameters:\n",
    "- $\\eta_{\\text{min}}$: minimum learning rate\n",
    "- $\\eta_{\\text{max}}$: maximum learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b7fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_learning_rate(T, max_lr, min_lr=0):\n",
    "    \"\"\"Cosine annealing learning rate.\n",
    "    Args:\n",
    "        T: Total number of epochs.\n",
    "        max_lr: Maximum learning rate.\n",
    "        min_lr: Minimum learning rate.\n",
    "    Returns:\n",
    "        A function that takes the current epoch and returns the learning rate.\"\"\"\n",
    "    return lambda t: min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * t / T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b20a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = cosine_learning_rate(T=100, max_lr=1.0)\n",
    "fig = px.line(x=np.arange(100), \n",
    "              markers=True,\n",
    "              y=[lr(epoch) for epoch in range(100)],\n",
    "              labels={'x': 'Epoch', 'y': 'Learning Rate'})\n",
    "fig.update_traces(line_width=4)\n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec187b",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Comparing Schedules</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_lr = linear_learning_rate(initial_lr=0.1, T=100, end_lr=0.01)\n",
    "power_lr = power_law_learning_rate(initial_lr=0.1, T=100, s=10, c=1)\n",
    "exp_lr = exponential_learning_rate(initial_lr=0.1, decay_rate=0.90, decay_steps=5)\n",
    "cosine_lr = cosine_learning_rate(T=100, max_lr=0.1, min_lr=0.01)\n",
    "t = np.arange(100)\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=[linear_lr(epoch) for epoch in t], \n",
    "                         mode='lines+markers', name='Linear', line=dict(width=4)))\n",
    "fig.add_trace(go.Scatter(x=t, y=[power_lr(epoch) for epoch in t], \n",
    "                         mode='lines+markers', name='Power Law', line=dict(width=4)))\n",
    "fig.add_trace(go.Scatter(x=t, y=[exp_lr(epoch) for epoch in t], \n",
    "                         mode='lines+markers', name='Exponential', line=dict(width=4))) \n",
    "fig.add_trace(go.Scatter(x=t, y=[cosine_lr(epoch) for epoch in t], \n",
    "                         mode='lines+markers', name='Cosine', line=dict(width=4))) \n",
    "fig.update_layout(margin=dict(l=20, r=20, t=20, b=20),\n",
    "                  xaxis_title='Epoch', yaxis_title='Learning Rate', \n",
    "                  height=800)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a7345",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Adam Optimizer</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa64857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_adam(w_0, gradient, \n",
    "    learning_rate=lambda t: 0.1, \n",
    "    nepochs=100, \n",
    "    epsilon=1e-6, \n",
    "    beta1=0.9, \n",
    "    beta2=0.999):\n",
    "    \"\"\"Adam optimizer that combines momentum and RMSProp.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        learning_rate: Step size for each iteration.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "        beta1: Exponential decay rate for the first moment estimate.\n",
    "        beta2: Exponential decay rate for the second moment estimate.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w_old = w_0\n",
    "    path = [w_old]\n",
    "    s = np.zeros_like(w_old) # (Momentum)\n",
    "    r = np.zeros_like(w_old) # (RMSProp)\n",
    "    for t in range(nepochs):\n",
    "        g = gradient(w_old)\n",
    "        # Update biased first moment estimate and second moment estimate\n",
    "        s = beta1 * s + (1 - beta1) * g\n",
    "        r = beta2 * r + (1 - beta2) * g**2\n",
    "        # Bias correction\n",
    "        s_hat = s / (1 - beta1 ** (t + 1)) \n",
    "        r_hat = r / (1 - beta2 ** (t + 1))\n",
    "        # Weight Update\n",
    "        w_old = w_old - learning_rate(t) * s_hat / (np.sqrt(r_hat) + epsilon)\n",
    "        path.append(w_old)\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-1, 2.5])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "#w0 = np.array([1.5, 2])\n",
    "path = gd_adam(\n",
    "    w0, \n",
    "    grad_sine_MSE,\n",
    "    learning_rate=lambda t: 0.1, \n",
    "    nepochs=100, \n",
    "    epsilon=1e-6, \n",
    "    beta1=0.9, \n",
    "    beta2=0.999)\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n",
    "\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "add_solution_path(fig, errors, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676e708",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h3 class=\"cal cal-h3\">Factorized Adam Optimizer</h3>\n",
    "\n",
    "Here we factor out the Adam optimizer from the gradient descent code.  This allows us to easily switch between different optimization algorithms and reuse the Adam optimizer code for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, \n",
    "                 learning_rate=lambda t: 0.1, \n",
    "                 epsilon=1e-6, \n",
    "                 beta1=0.9, \n",
    "                 beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, w, g):\n",
    "        # initialize state variables on first call\n",
    "        if not hasattr(self, 's'):\n",
    "            self.s = np.zeros_like(w) # (Momentum)\n",
    "            self.r = np.zeros_like(w) # (RMSProp)\n",
    "        # Update biased first moment estimate and second moment estimate\n",
    "        self.s = self.beta1 * self.s + (1 - self.beta1) * g\n",
    "        self.r = self.beta2 * self.r + (1 - self.beta2) * g ** 2\n",
    "        # Bias correction\n",
    "        s_hat = self.s / (1 - self.beta1 ** (self.t + 1))\n",
    "        r_hat = self.r / (1 - self.beta2 ** (self.t + 1))\n",
    "        # Weight Update\n",
    "        w = w - self.learning_rate(self.t) * s_hat / (np.sqrt(r_hat) + self.epsilon)\n",
    "        self.t += 1\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753876f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaOptimizer:\n",
    "    def __init__(self, learning_rate=lambda t: 0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, w, g):\n",
    "        w = w - self.learning_rate(self.t) * g\n",
    "        self.t += 1\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a07a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumUpdate:\n",
    "    def __init__(self, learning_rate=lambda t: 0.1, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, w, g):\n",
    "        if not hasattr(self, 'v'):\n",
    "            self.v = np.zeros_like(w)\n",
    "        self.v = self.momentum * self.v - self.learning_rate(self.t) * g\n",
    "        self.t += 1\n",
    "        return w + self.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(w_0, gradient, optimizer, nepochs = 100, epsilon=1e-8):\n",
    "    \"\"\"Generic gradient descent optimization.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        optimizer: An optimizer object with a step method.\n",
    "        nepochs: Maximum number of iterations.\n",
    "        epsilon: Convergence threshold.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w = w_0\n",
    "    path = [w]\n",
    "    for t in range(nepochs):\n",
    "        g = gradient(w)\n",
    "        w = optimizer.step(w, g)\n",
    "        path.append(w)\n",
    "        if np.linalg.norm(w - path[-2]) < epsilon: break\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-1, 2.5])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "#w0 = np.array([1.5, 2])\n",
    "adam = AdamOptimizer(\n",
    "    learning_rate=lambda t: 0.1,  \n",
    "    beta1=0.9, \n",
    "    beta2=0.999)\n",
    "momentum = MomentumUpdate(\n",
    "    learning_rate=lambda t: 0.1, \n",
    "    momentum=0.9)\n",
    "basic = VanillaOptimizer(learning_rate=lambda t: 0.1)\n",
    "\n",
    "path = gd(w0, grad_sine_MSE, adam, nepochs=100)\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "fig = add_solution_path(fig, errors, path)\n",
    "fig.show()\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e6d24",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Stochastic Gradient Descent</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a791746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w_0, x, t, gradient, optimizer, nepochs = 2, epsilon=1e-8):\n",
    "    \"\"\"Generic stochastic gradient descent optimization.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        optimizer: An optimizer object with a step method.\n",
    "        epsilon: Convergence threshold.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w = w_0\n",
    "    path = [w]\n",
    "    for e in range(nepochs):\n",
    "        random_indices = np.random.permutation(len(t))\n",
    "        for i in random_indices:\n",
    "            g = gradient(w, x[i], t[i])\n",
    "            w = optimizer.step(w, g)\n",
    "            path.append(w)\n",
    "        if np.linalg.norm(w - path[-2]) < epsilon: break\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-1, 2.5])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "#w0 = np.array([1.5, 2])\n",
    "nepochs = 2\n",
    "adam = AdamOptimizer(\n",
    "    learning_rate=lambda t: 0.1, \n",
    "    beta1=0.9, \n",
    "    beta2=0.999)\n",
    "momentum = MomentumUpdate(\n",
    "    learning_rate=lambda t: 0.01, \n",
    "    momentum=0.5)\n",
    "powerlaw = power_law_learning_rate(initial_lr=0.1, T=nepochs*len(sine['y']), s=10, c=1)\n",
    "vanilla = VanillaOptimizer(learning_rate=powerlaw)\n",
    "\n",
    "path = sgd(w0, sine['x'], sine['y'], grad_sine_MSE, \n",
    "           adam, \n",
    "           nepochs=nepochs)\n",
    "\n",
    "\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "fig = add_solution_path(fig, errors, path)\n",
    "fig.show()\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a32fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w0 = np.array([-10., -5.])\n",
    "w0 = np.array([-1., 2.])\n",
    "#w0 = np.array([0., -4])\n",
    "nepochs = 1\n",
    "coslr = cosine_learning_rate(T=nepochs*len(cancer['t']), max_lr=0.4, min_lr=0.01)\n",
    "adam = AdamOptimizer(\n",
    "    learning_rate=coslr,\n",
    "    beta1=0.9, \n",
    "    beta2=0.999)\n",
    "momentum = MomentumUpdate(\n",
    "    learning_rate=coslr, \n",
    "    momentum=0.5)\n",
    "vanilla = VanillaOptimizer(learning_rate=coslr)\n",
    "\n",
    "path = sgd(w0, cancer['x'], cancer['t'], grad_NLL, \n",
    "           adam, \n",
    "           nepochs=nepochs)\n",
    "\n",
    "\n",
    "errors = [NLL(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    cancer['w1'], cancer['w2'], cancer['error'], \n",
    "    cancer['dw1'], cancer['dw2'], scale=1)\n",
    "fig = add_solution_path(fig, errors, path)\n",
    "fig.show()\n",
    "\n",
    "fig_loss = make_loss_curve(path, NLL)\n",
    "fig_loss.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f67f1",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h2 class=\"cal cal-h2\">Minibatch Stochastic Gradient Descent</h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb_sgd(w_0, x, t, gradient, optimizer, \n",
    "           batch_size=32, nepochs = 2, epsilon=1e-8):\n",
    "    \"\"\"Generic stochastic gradient descent optimization.\n",
    "    Args:\n",
    "        w_0: Initial weights (numpy array).\n",
    "        gradient: Function to compute the gradient.\n",
    "        optimizer: An optimizer object with a step method.\n",
    "        batch_size: Size of each mini-batch.\n",
    "        epsilon: Convergence threshold.\n",
    "    Returns:\n",
    "        path: Array of weights at each iteration.\"\"\"\n",
    "    w = w_0\n",
    "    path = [w]\n",
    "    for e in range(nepochs):\n",
    "        random_indices = np.random.permutation(len(t))\n",
    "        for start in range(0, len(t), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_indices = random_indices[start:end]\n",
    "            g = gradient(w, x[batch_indices], t[batch_indices])\n",
    "            w = optimizer.step(w, g)\n",
    "            path.append(w)\n",
    "        if np.linalg.norm(w - path[-2]) < epsilon: break\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ec3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([-1, 2.5])\n",
    "#w0 = np.array([.5, 2.5])\n",
    "#w0 = np.array([1.5, 2])\n",
    "nepochs = 10\n",
    "adam = AdamOptimizer(\n",
    "    learning_rate=lambda t: 0.1, \n",
    "    beta1=0.9, \n",
    "    beta2=0.999)\n",
    "momentum = MomentumUpdate(\n",
    "    learning_rate=lambda t: 0.01, \n",
    "    momentum=0.5)\n",
    "powerlaw = power_law_learning_rate(initial_lr=0.1, T=nepochs*len(sine['y']), s=10, c=1)\n",
    "vanilla = VanillaOptimizer(learning_rate=powerlaw)\n",
    "\n",
    "path = mb_sgd(\n",
    "    w0, sine['x'], sine['y'], grad_sine_MSE, \n",
    "    adam, \n",
    "    batch_size=32,\n",
    "    nepochs=nepochs)\n",
    "\n",
    "errors = [sine_MSE(w) for w in path]\n",
    "fig = plot_gradient(\n",
    "    sine['w0'], sine['w1'], sine['error'], \n",
    "    sine['dw0'], sine['dw1'], scale=.1)\n",
    "fig = add_solution_path(fig, errors, path)\n",
    "fig.show()\n",
    "\n",
    "fig_loss = make_loss_curve(path, sine_MSE)\n",
    "fig_loss.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
