{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd110d0",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Lecture 18 – CS 189, Fall 2025</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, itertools, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from types import SimpleNamespace\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "try:\n",
    "    import torchvision as tv\n",
    "    from torchvision import transforms\n",
    "except Exception as e:\n",
    "    torchvision = None\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "if tv is None:\n",
    "    print(\"torchvision not available. Please install torchvision to run MNIST demos.\")\n",
    "else:\n",
    "    transform = transforms.Compose([transforms.ToTensor()])  # Keep MNIST in [0,1], single-channel\n",
    "    train_ds = tv.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_ds  = tv.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "    print('Train size:', len(train_ds), ' Test size:', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c70789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if tv is not None:\n",
    "    imgs = [train_ds[i][0] for i in range(16)]\n",
    "    labels = [train_ds[i][1] for i in range(16)]\n",
    "    fig, axes = plt.subplots(4,4, figsize=(4,4))\n",
    "    for ax, img, lab in zip(axes.flatten(), imgs, labels):\n",
    "        ax.imshow(img[0].numpy(), cmap='gray')\n",
    "        ax.set_title(str(lab))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the first 10 images of the digit 9\n",
    "imgs_9 = [train_ds[i][0] for i in range(len(train_ds)) if train_ds[i][1] == 9][:10]\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "for i, img in enumerate(imgs_9):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img[0].numpy(), cmap='gray')  \n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"9\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8741e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = imgs_9[4].unsqueeze(0)  \n",
    "\n",
    "# Define convolution kernels\n",
    "kernels = {\n",
    "    'horizontal line': torch.tensor([[[-1, -1, -1], [2, 2, 2], [-1, -1, -1]]], dtype=torch.float32).unsqueeze(0),\n",
    "    'vertical line': torch.tensor([[[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]]], dtype=torch.float32).unsqueeze(0),\n",
    "    'diagonal line': torch.tensor([[[2, -1, -1], [-1, 2, -1], [-1, -1, 2]]], dtype=torch.float32).unsqueeze(0)\n",
    "}\n",
    "\n",
    "# Apply convolutions and visualize\n",
    "fig, axes = plt.subplots(len(kernels), 3, figsize=(9, len(kernels) * 3))\n",
    "\n",
    "for i, (name, kernel) in enumerate(kernels.items()):\n",
    "    # Display the kernel\n",
    "    axes[i, 0].imshow(kernel.squeeze().detach().numpy(), cmap='gray')\n",
    "    axes[i, 0].set_title(f'Kernel: {name}')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Display the input image\n",
    "    axes[i, 1].imshow(x.squeeze().detach().numpy(), cmap='gray')\n",
    "    axes[i, 1].set_title('Input Image')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    # Display the result of the convolution\n",
    "    y = F.conv2d(x, kernel, padding=1)\n",
    "    axes[i, 2].imshow(y.squeeze().detach().numpy(), cmap='gray')\n",
    "    axes[i, 2].set_title('Convolved Output')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tv is not None:\n",
    "    x, y = [train_ds[i] for i in range(len(train_ds)) if train_ds[i][1] == 9][4]   # x: [1,28,28]\n",
    "    x = x.unsqueeze(0)  # [1,1,28,28]\n",
    "    \n",
    "    # Define classic kernels (normalized where reasonable)\n",
    "    kernels = {\n",
    "        'identity': torch.tensor([[0,0,0],[0,1,0],[0,0,0]], dtype=torch.float32),\n",
    "        'edge_h':  torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32),\n",
    "        'edge_v':  torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32),\n",
    "        'sharpen': torch.tensor([[0,-1,0],[-1,5,-1],[0,-1,0]], dtype=torch.float32),\n",
    "        'box_blur': (1/9.0)*torch.ones((3,3), dtype=torch.float32)\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(6,4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    axes[0].imshow(x[0,0].numpy(), cmap='gray'); axes[0].set_title('Input'); axes[0].axis('off')\n",
    "\n",
    "    i = 1\n",
    "    for name, K in kernels.items():\n",
    "        W = K.view(1,1,3,3)\n",
    "        yk = F.conv2d(x, W, padding=1)  # keep size with padding=1\n",
    "        axes[i].imshow(yk[0,0].detach().numpy(), cmap='gray')\n",
    "        axes[i].set_title(name); axes[i].axis('off')\n",
    "        i += 1\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"torchvision not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceac2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tv is not None:\n",
    "    x, _ = train_ds[1]\n",
    "    x = x.unsqueeze(0)\n",
    "    k = 3\n",
    "    W = torch.ones(1,1,k,k) / (k*k)\n",
    "\n",
    "    configs = [(1,0), (1,1), (2,0), (2,1)]  # (stride, padding)\n",
    "    fig, axes = plt.subplots(1, len(configs)+1, figsize=(3*(len(configs)+1), 3))\n",
    "    axes[0].imshow(x[0,0].numpy(), cmap='gray'); axes[0].set_title('Input'); axes[0].axis('off')\n",
    "\n",
    "    for i, (s,p) in enumerate(configs, start=1):\n",
    "        y = F.conv2d(x, W, stride=s, padding=p)\n",
    "        axes[i].imshow(y[0,0].detach().numpy(), cmap='gray')\n",
    "        axes[i].set_title(f's={s}, p={p}\\n{tuple(y.shape)}'); axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1011056",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tv is not None:\n",
    "    x, _ = train_ds[2]\n",
    "    x = x.unsqueeze(0)\n",
    "    maxpool = nn.MaxPool2d(2,2)\n",
    "    avgpool = nn.AvgPool2d(4,4)\n",
    "\n",
    "    y_max = maxpool(x)\n",
    "    y_avg = avgpool(x)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(9,3))\n",
    "    axes[0].imshow(x[0,0].numpy(), cmap='gray'); axes[0].set_title(f'd={len(x[0,0])}x{len(x[0,0])}\\n Input'); axes[0].axis('off')\n",
    "    axes[1].imshow(y_max[0,0].detach().numpy(), cmap='gray'); axes[1].set_title(f'd={len(y_max[0,0])}x {len(y_max[0,0])}\\n MaxPool 2x2'); axes[1].axis('off')\n",
    "    axes[2].imshow(y_avg[0,0].detach().numpy(), cmap='gray'); axes[2].set_title(f'd={len(y_avg[0,0])}x {len(y_avg[0,0])}\\n AvgPool 4x4'); axes[2].axis('off')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 \n",
    "            nn.Conv2d(8, 64, 3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),               \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),  # Adjusted based on the output size of the features block\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = SmallCNN().to(device)\n",
    "summary(model, input_size=(1, 1, 28, 28),  # (batch, C, H, W)\n",
    "        col_names=(\"input_size\",\"output_size\",\"num_params\",\"kernel_size\"),\n",
    "        depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, opt, loss_fn):\n",
    "    model.train()\n",
    "    total, correct, running_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()*xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "    total, correct, running_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "        running_loss += loss.item()*xb.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds==yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "if tv is not None:\n",
    "    model = SmallCNN().to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = 4 \n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tl, ta = train_one_epoch(model, train_loader, opt, loss_fn)\n",
    "        vl, va = evaluate(model, test_loader, loss_fn)\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(tl); history['val_loss'].append(vl)\n",
    "        history['train_acc'].append(ta);  history['val_acc'].append(va)\n",
    "        print(f'E{epoch}: train_loss={tl:.4f} val_loss={vl:.4f} train_acc={ta:.3f} val_acc={va:.3f}')\n",
    "else:\n",
    "    print(\"torchvision not available. Skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecda17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if history and 'train_loss' in history and 'val_loss' in history:\n",
    "    plt.figure(); plt.plot(history['epoch'], history['train_loss']); plt.plot(history['epoch'], history['val_loss']); plt.legend(['train','val']); plt.title('Loss'); plt.xlabel('epoch'); plt.show()\n",
    "if history and 'train_acc' in history and 'val_acc' in history:\n",
    "    plt.figure(); plt.plot(history['epoch'], history['train_acc']); plt.plot(history['epoch'], history['val_acc']); plt.legend(['train','val']); plt.title('Accuracy'); plt.xlabel('epoch'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b60da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "if tv is not None:\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_pred.extend(preds.cpu().numpy())\n",
    "            all_true.extend(yb.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    all_true = np.array(all_true)\n",
    "    all_pred = np.array(all_pred)\n",
    "    accuracy = (all_true == all_pred).sum() / len(all_true)\n",
    "    print(f\"Accuracy on test data: {accuracy:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_true, all_pred, labels=list(range(10)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(10)))\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"torchvision not available. Cannot evaluate predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc239dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "alex_w   = tv.models.AlexNet_Weights.IMAGENET1K_V1\n",
    "\n",
    "MODELS = {\n",
    "    \"alexnet\": SimpleNamespace(\n",
    "        ctor=lambda: tv.models.alexnet(weights=alex_w).to(device).eval(),\n",
    "        weights=alex_w,\n",
    "        act_layers=[\"features.0\", \"features.3\"],  # conv blocks\n",
    "        maxact_layer=\"features.10\",\n",
    "        arch=\"alexnet\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0eba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, weights):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return weights.transforms()(img).unsqueeze(0).to(device)\n",
    "\n",
    "def show(tensor, title=None):\n",
    "    if tensor.ndim == 4:\n",
    "        grid = make_grid(tensor, nrow=int(np.ceil(np.sqrt(tensor.size(0)))))\n",
    "        arr = grid.permute(1,2,0).detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = tensor.permute(1,2,0).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(np.clip(arr, 0, 1))\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Robust per-item normalization (avoids tuple-dim min/max issues)\n",
    "def _norm_per_item(t):\n",
    "    # t shape: [N, ...]\n",
    "    if hasattr(torch, \"amin\"):\n",
    "        tmin = torch.amin(t, dim=tuple(range(1, t.ndim)), keepdim=True)\n",
    "        tmax = torch.amax(t, dim=tuple(range(1, t.ndim)), keepdim=True)\n",
    "    else:\n",
    "        flat = t.view(t.size(0), -1)\n",
    "        tmin = flat.min(dim=1, keepdim=True)[0].view(-1, *([1]*(t.ndim-1)))\n",
    "        tmax = flat.max(dim=1, keepdim=True)[0].view(-1, *([1]*(t.ndim-1)))\n",
    "    return (t - tmin) / (tmax - tmin + 1e-8)\n",
    "\n",
    "# Helper: resolve \"features.23\" dotted path to a module\n",
    "def resolve_module(root, name):\n",
    "    mod = root\n",
    "    for part in name.split('.'):\n",
    "        if part.isdigit():\n",
    "            mod = mod[int(part)]\n",
    "        else:\n",
    "            mod = getattr(mod, part)\n",
    "    return mod\n",
    "\n",
    "def first_conv_module(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            return m\n",
    "    raise RuntimeError(\"No Conv2d found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fb8bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_first_layer_filters(model, max_filters=64, label=\"model\"):\n",
    "    conv1 = None\n",
    "    # Try common entry points first\n",
    "    for attr in [\"conv1\", \"features\"]:\n",
    "        if hasattr(model, attr):\n",
    "            m = getattr(model, attr)\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                conv1 = m\n",
    "                break\n",
    "            # If Sequential, first layer likely Conv2d\n",
    "            if isinstance(m, nn.Sequential):\n",
    "                for x in m:\n",
    "                    if isinstance(x, nn.Conv2d):\n",
    "                        conv1 = x; break\n",
    "        if conv1 is not None: break\n",
    "    if conv1 is None:\n",
    "        conv1 = first_conv_module(model)\n",
    "\n",
    "    w = conv1.weight.data.clone().cpu()  # [out, in, k, k]\n",
    "    w = _norm_per_item(w)\n",
    "    show(w[:max_filters], title=f\"{label}: first-layer conv filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da4c8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(model, img_tensor, layer_names, label=\"model\"):\n",
    "    feats, hooks = {}, []\n",
    "    def hook(name): \n",
    "        return lambda m, i, o: feats.setdefault(name, o.detach().cpu())\n",
    "    # Register hooks\n",
    "    for name in layer_names:\n",
    "        try:\n",
    "            module = resolve_module(model, name)\n",
    "            hooks.append(module.register_forward_hook(hook(name)))\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] could not hook '{name}': {e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(img_tensor)\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    for name, feat in feats.items():\n",
    "        fmap = feat[0]                    \n",
    "        if fmap.ndim != 3:\n",
    "            print(f\"[info] {label}:{name} is non-spatial (shape {feat.shape}), skipping grid\")\n",
    "            continue\n",
    "        C = min(64, fmap.size(0))\n",
    "        fm = fmap[:C]\n",
    "        fm = _norm_per_item(fm.unsqueeze(1)).squeeze(1)\n",
    "        show(fm.unsqueeze(1), title=f\"{label}: activations @ {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0b0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_probs(model, x):\n",
    "    logits = model(x)\n",
    "    return F.softmax(logits, dim=1)\n",
    "\n",
    "def _resize_heat_with_torch(heat, H, W):\n",
    "    t = torch.from_numpy(heat)[None, None]\n",
    "    t = F.interpolate(t.float(), size=(H, W), mode=\"bilinear\", align_corners=False)\n",
    "    return t[0,0].numpy()\n",
    "\n",
    "def occlusion_heatmap(model, img_tensor, idx_to_label=None, target_class=None, patch=32, stride=16, baseline=0.0, label=\"model\"):\n",
    "    model.eval()\n",
    "    x = img_tensor.clone()\n",
    "    probs = predict_probs(model, x)[0]\n",
    "    if target_class is None:\n",
    "        target_class = probs.argmax().item()\n",
    "    base_p = probs[target_class].item()\n",
    "\n",
    "    _, _, H, W = x.shape\n",
    "    heat = np.zeros(((H - patch)//stride + 1, (W - patch)//stride + 1), dtype=np.float32)\n",
    "\n",
    "    for i, y in enumerate(range(0, H - patch + 1, stride)):\n",
    "        for j, z in enumerate(range(0, W - patch + 1, stride)):\n",
    "            x_ = x.clone()\n",
    "            x_[:,:, y:y+patch, z:z+patch] = baseline\n",
    "            p = predict_probs(model, x_)[0, target_class].item()\n",
    "            heat[i, j] = base_p - p\n",
    "\n",
    "    heat_resized = _resize_heat_with_torch(heat, H, W)\n",
    "    # quick unnormalize for show (using Imagenet stats)\n",
    "    im = x[0].detach().cpu()\n",
    "    im = (im * torch.tensor([0.229,0.224,0.225])[:,None,None] + torch.tensor([0.485,0.456,0.406])[:,None,None]).permute(1,2,0).numpy()\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(np.clip(im,0,1)); plt.imshow(heat_resized, alpha=0.5); plt.axis('off')\n",
    "    if idx_to_label:\n",
    "        tname = idx_to_label[target_class]\n",
    "    else:\n",
    "        tname = str(target_class)\n",
    "    plt.title(f\"{label}: occlusion (target='{tname}', base p={base_p:.3f})\")\n",
    "    plt.show()\n",
    "    return heat_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b428867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(model, img_tensor, target_class=None, label=\"model\"):\n",
    "    model.eval()\n",
    "    x = img_tensor.clone().requires_grad_(True)\n",
    "    logits = model(x)\n",
    "    if target_class is None:\n",
    "        target_class = logits.argmax(dim=1).item()\n",
    "    loss = logits[0, target_class]\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    g = x.grad.detach()[0]               # [3,H,W]\n",
    "    sal = g.abs().max(dim=0)[0]          # [H,W]\n",
    "    sal = (sal - sal.min())/(sal.max()-sal.min()+1e-8)\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(sal.cpu(), cmap='gray'); plt.axis('off'); plt.title(f\"{label}: saliency\")\n",
    "    plt.show()\n",
    "    return sal\n",
    "\n",
    "class GuidedBackpropReLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self.saved = x\n",
    "        return F.relu(x)\n",
    "    def backward_hook(self, module, grad_in, grad_out):\n",
    "        positive_grad = torch.clamp(grad_out[0], min=0.0)\n",
    "        positive_mask = (self.saved > 0).float()\n",
    "        return (positive_grad * positive_mask,)\n",
    "\n",
    "def guided_backprop(model_ctor, weights, img_tensor, target_class=None, label=\"model\"):\n",
    "    # Create a fresh copy to freely patch ReLUs\n",
    "    gb_model = model_ctor().to(device).eval()\n",
    "    # Swap all ReLUs\n",
    "    relus = []\n",
    "    for name, module in gb_model.named_modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            relu = GuidedBackpropReLU()\n",
    "            relus.append(relu)\n",
    "            parent = gb_model\n",
    "            *parents, leaf = name.split('.')\n",
    "            for p in parents:\n",
    "                parent = getattr(parent, p)\n",
    "            setattr(parent, leaf, relu)\n",
    "    x = img_tensor.clone().requires_grad_(True)\n",
    "    logits = gb_model(x)\n",
    "    if target_class is None:\n",
    "        target_class = logits.argmax(dim=1).item()\n",
    "    loss = logits[0, target_class]\n",
    "    gb_model.zero_grad()\n",
    "    hooks = [relu.register_full_backward_hook(relu.backward_hook) for relu in relus]\n",
    "    loss.backward()\n",
    "    for h in hooks: h.remove()\n",
    "\n",
    "    g = x.grad.detach()[0]\n",
    "    g = (g - g.min())/(g.max()-g.min()+1e-8)\n",
    "    g = g.permute(1,2,0).cpu().numpy()\n",
    "    plt.figure(figsize=(6,6)); plt.imshow(g); plt.axis('off'); plt.title(f\"{label}: guided backprop\")\n",
    "    plt.show()\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14ae26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatExtractor(nn.Module):\n",
    "    \"\"\"Return a fixed-dim feature vector (penultimate-ish) for each arch.\"\"\"\n",
    "    def __init__(self, model, arch):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        self.model = model\n",
    "        if arch == \"resnet\":\n",
    "            # body up to layer4 GAP\n",
    "            self.body = nn.Sequential(\n",
    "                model.conv1, model.bn1, model.relu, model.maxpool,\n",
    "                model.layer1, model.layer2, model.layer3, model.layer4,\n",
    "                nn.AdaptiveAvgPool2d((1,1))\n",
    "            )\n",
    "            self.out_dim = model.fc.in_features\n",
    "        elif arch == \"vgg\" or arch == \"alexnet\":\n",
    "            self.features = model.features\n",
    "            self.pool = nn.AdaptiveAvgPool2d((7,7))  # match VGG/Alex input to classifier\n",
    "            # classifier: take everything except final Linear\n",
    "            self.prefix = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "            # out_dim is the in_features of final Linear\n",
    "            last_linear = list(model.classifier.children())[-1]\n",
    "            self.out_dim = last_linear.in_features\n",
    "        else:\n",
    "            raise ValueError(\"Unknown arch\")\n",
    "    def forward(self, x):\n",
    "        if self.arch == \"resnet\":\n",
    "            x = self.body(x).flatten(1)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.features(x)\n",
    "            x = self.pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.prefix(x)\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0103e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_activating_images(model, dataset, layer_name, topk=16, label=\"model\"):\n",
    "    target = resolve_module(model, layer_name)\n",
    "    acts = []\n",
    "    imgs_cache = []\n",
    "    def fhook(m, i, o):\n",
    "        if o.ndim == 4:\n",
    "            a = o.detach().cpu().mean(dim=(2,3))  # GAP over H,W → [B, C]\n",
    "        else:\n",
    "            a = o.detach().cpu()\n",
    "        acts.append(a)\n",
    "    h = target.register_forward_hook(fhook)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            imgs_cache.append(xb)\n",
    "            _ = model(xb.to(device))\n",
    "    h.remove()\n",
    "    A = torch.cat(acts, 0).numpy()      # [N, C]\n",
    "    imgs_cache = torch.cat(imgs_cache, 0)\n",
    "    # Choose an arbitrary channel to inspect (customize this)\n",
    "    channel = min(5, A.shape[1]-1)\n",
    "    idxs = np.argsort(-A[:, channel])[:topk]\n",
    "    grid = imgs_cache[idxs]\n",
    "    # unnormalize for viewing (ImageNet stats)\n",
    "    grid = grid*torch.tensor([0.229,0.224,0.225])[None,:,None,None] + torch.tensor([0.485,0.456,0.406])[None,:,None,None]\n",
    "    grid = grid.clamp(0,1)\n",
    "    show(grid, title=f\"{label}: top-{topk} images for channel {channel} @ {layer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5092d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"pishi.png\" \n",
    "from PIL import Image\n",
    "\n",
    "models = {}\n",
    "for name, cfg in MODELS.items():\n",
    "    m = cfg.ctor()\n",
    "    models[name] = SimpleNamespace(\n",
    "        model=m, weights=cfg.weights, act_layers=cfg.act_layers,\n",
    "        maxact_layer=cfg.maxact_layer, arch=cfg.arch,\n",
    "        idx_to_label=cfg.weights.meta.get(\"categories\", None)\n",
    "    )\n",
    "\n",
    "# Ensure the input image is resized to 224x224\n",
    "images = {name: load_image(img_path, cfg.weights) for name, cfg in models.items()}\n",
    "for name, img in images.items():\n",
    "    assert img.shape[-2:] == (224, 224), f\"Image for model {name} is not resized to 224x224\"\n",
    "\n",
    "# 1) First-layer filters comparison\n",
    "for name, cfg in models.items():\n",
    "    visualize_first_layer_filters(cfg.model, max_filters=64, label=name)\n",
    "\n",
    "# 2) Activation maps at key layers\n",
    "for name, cfg in models.items():\n",
    "    visualize_activations(cfg.model, images[name], cfg.act_layers, label=name)\n",
    "\n",
    "# 3) Occlusion sensitivity (same target class per model by default)\n",
    "for name, cfg in models.items():\n",
    "    _ = occlusion_heatmap(cfg.model, images[name], idx_to_label=cfg.idx_to_label, patch=32, stride=16, label=name)\n",
    "\n",
    "# 4) Saliency and Guided Backprop\n",
    "for name, cfg in models.items():\n",
    "    _ = saliency_map(cfg.model, images[name], label=name)\n",
    "    _ = guided_backprop(MODELS[name].ctor, cfg.weights, images[name], label=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
