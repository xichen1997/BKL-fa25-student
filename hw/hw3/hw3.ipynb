{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e052c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw3\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229bcf43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d576e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Homework 03 – Optimizers and Backpropagation</h1>\n",
    "\n",
    "CS 189, Fall 2025\n",
    "\n",
    "Welcome to Homework 3! In this assignment, you will use your newfound skills in backpropagation to implement an autodifferentiation library from scratch. You'll extend the single-variable autograd that we learned in lecture to function with general tensors, mimicking how `torch`'s autograd is implemented in research and industry.\n",
    "\n",
    "---\n",
    "\n",
    "## Due Date: November 7th, 2025\n",
    "\n",
    "This assignment is due on **November 7th, 2025**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "### Submission Tips\n",
    "- **Plan ahead**: We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early**: If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "### Assignment Overview\n",
    "\n",
    "This notebook contains a series of tasks designed to help you practice and apply theoretical concepts you learned in class.\n",
    "\n",
    "Question 1: Implementing Backpropagation\n",
    "\n",
    "Question 2 : Implementing Optimizers\n",
    "\n",
    "--- -->\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "1. Learn how backpropagation is implemented in libraries like PyTorch\n",
    "2. Understand how optimizers are implemented, and the advantages of Adam\n",
    "    \n",
    "---\n",
    "\n",
    "### Collaboration Policy\n",
    "You are encouraged to discuss high-level concepts with your peers. However:\n",
    "- All submitted work must be written in your own words and code.\n",
    "- Do not share or copy solutions directly.\n",
    "- List any collaborators (students you worked with) in the line below. Include their name and SID:\n",
    "\n",
    "**Your Collaborators**: **TODO**\n",
    "\n",
    "### AI Tools Usage Disclosure\n",
    "We allow the use of AI tools (e.g., ChatGPT, Copilot) **only as support**, not as a replacement for your own reasoning. To ensure transparency, you must acknowledge any use of AI tools.\n",
    "\n",
    "Please answer with one of the following options:\n",
    "- **A) I did not use any AI tools for this homework.**\n",
    "- **B) I used AI tools in the following way(s):**  \n",
    "  (describe briefly, e.g., “Used ChatGPT to get hints for debugging a NumPy indexing error”)\n",
    "\n",
    "\n",
    "**Your Answer**: **TODO**\n",
    "    \n",
    "---\n",
    "\n",
    "### Grading Breakdown\n",
    "\n",
    "<!-- <div align=\"center\"> -->\n",
    "\n",
    "\n",
    "| Question  | Manual Grading? | Points |\n",
    "| --------- | --------------- | ------ |\n",
    "| q1        | No              | 10     |\n",
    "| q2        | No              | 20     |\n",
    "| q3        | No              | 10     |\n",
    "| q4        | No              | 10     |\n",
    "| **Total** |                 | **50** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f22d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import typst\n",
    "import re\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364f8fa",
   "metadata": {},
   "source": [
    "**Introducing BearTensor** \n",
    "\n",
    "Before we can dig into our multivariable calculus, we first need a good abstraction for tensors,\n",
    "which are just $n$-dimensional arrays (similar to `numpy`'s `ndarray`). `numpy` doesn't\n",
    "provide autodifferentiation for its `ndarray`, so we can build a wrapper around it. For this homework assignment we are going to be using the `BearTensor` class, which you can think of as a custom implementation of `torch.Tensor` that acts as a wrapper around `numpy` arrays.\n",
    "\n",
    "Take a look at the `BearTensor`, `BearGrad`, and `BearParent` classes below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b912205",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 1**: Building the Computation Graph\n",
    "\n",
    "**Conceptual Overview**:\n",
    "\n",
    "Our model needs some way to build a computation graph for backpropagation so that it knows where to pass down gradients during the backwards pass. The way that PyTorch does this, is that every time you combine two values to compute a third one in the forward pass, it will keep track of this internally. We want to implement a similar functionality for `BearTensor`; everytime we add, subtract, divide, etc. two `BearTensor` to get a third one, we should keep track of the fact that the third tensor's **parents** are the two source tensors. \n",
    "\n",
    "Here is an example of what a computation graph might look like from lecture:\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://imgur.com/PO3S6TI.png\" alt=\"Backpropagation\" style=\"display: block; margin-left: auto; margin-right: auto; width: 60%;\">\n",
    "</div>\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "Currently, our `BearTensor` has no operations. Implement the following functions in the class:\n",
    "- Addition (`__add__`)\n",
    "- Subtraction (`__sub__`)\n",
    "- Multiplication (`__mul__`)\n",
    "- Power (`__pow__`)\n",
    "- Matrix multiplication (`__matmul__`)\n",
    "- Dot product (`dot`)\n",
    "- Sum (`sum`)\n",
    "- Mean (`mean`)\n",
    "- ReLU (`relu`)\n",
    "- Sigmoid (`sigmoid`)\n",
    "\n",
    "A few things to remember:\n",
    "1. You should return a new `BearTensor` from these functions without modifying the existing one\n",
    "2. Remember to set the parents of this new Tensor to be the two source tensors; this is how we will build a computation graph\n",
    "3. Remember to set the correct gradient function as the first argument of `BearParent.BearGrad`; this will tell us how to compute the downstream gradient during the backward pass\n",
    "\n",
    "Additionally, we are going to be doing a lot NumPy operations in this Homework. It is important that you think about how the shapes of different arrays will change due to NumPy broadcasting rules, and reshape when needed. Some examples of this that are relevant to this HW are:\n",
    "- NumPy will automatically broadcast 1-element arrays to a scalar (ex. when computing the dot product of two vectors)\n",
    "- NumPy will broadcast array of shape (N, 1) to just (N,) (ex. when a matrix times a matrix gives a vector)\n",
    "\n",
    "**Note**: The public tests that we've provided in this notebook are NOT fully-comprehensive and often very simple. You should build more complex graphs and manually verify that your computed gradients and values are correct. Our hidden tests are more comprehensive and will check for full correctness \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8223f46",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BearGrad:\n",
    "    '''Stores how to compute the downstream gradient from the upstream gradient \n",
    "    - `op_str` is just a string describing the operation; you don't need to use this for this HW, but it may help in debugging if you print out what operations create your computation graph.\n",
    "    - `fn` is a function that takes in the upstream loss gradient and outputs the loss gradient that should be passed downstream. This essentially applies the chain rule at the current node in the computation graph.\n",
    "    '''\n",
    "    fn: callable\n",
    "    op_str: str | None = None\n",
    "\n",
    "@dataclass\n",
    "class BearParent:\n",
    "    '''This class represents a parent of a node; a node must track its parents so that it knows who to propagate its gradients to.\n",
    "    - `grad` is the `BearGrad` object for this parent; we can apply the `fn` from this to compute the gradient that should be passed downstream.\n",
    "    - `parent` is the `BearTensor` object for the parent\n",
    "    '''\n",
    "    parent: BearTensor\n",
    "    grad: BearGrad\n",
    "\n",
    "    def __hash__(self):\n",
    "        return id(self)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self is other\n",
    "\n",
    "class BearTensor:\n",
    "    '''`BearTensor`: This represents a node in our computation graph\n",
    "    - `value` is the underlying data in the Tensor; this is computed during the forward pass\n",
    "    - `parents` keeps track of the parents in the computation graph to whom we pass our gradient to\n",
    "    - `adjoint` is the gradient (the transpose of it technically) that we compute in the backward pass\n",
    "    '''\n",
    "    def __init__(self, name: str, value: np.ndarray, parents: list[BearParent] | None = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        self.parents = parents if parents is not None else []\n",
    "        self.adjoint: float | np.ndarray = 0.0\n",
    "\n",
    "    def __add__(self, other: BearTensor) -> BearTensor:\n",
    "        if self.value.shape != other.value.shape:\n",
    "            raise ValueError(\"Shapes must match\")\n",
    "        ...\n",
    "\n",
    "    def __sub__(self, other: BearTensor) -> BearTensor:\n",
    "        if self.value.shape != other.value.shape:\n",
    "            raise ValueError(\"Shapes must match\")\n",
    "        ...\n",
    "\n",
    "    def __mul__(self, other: BearTensor) -> BearTensor:\n",
    "        if self.value.shape != other.value.shape:\n",
    "            raise ValueError(\"Shapes must match\")\n",
    "        ...\n",
    "\n",
    "    def __pow__(self, power: float) -> BearTensor:\n",
    "        ...\n",
    "\n",
    "    def __matmul__(self, other: BearTensor) -> BearTensor:\n",
    "        # This may be helpful to avoid shape mismatch errors\n",
    "        def ensure_2d(x):\n",
    "            '''If x is a scalar or 1-D, convert to 2-D'''\n",
    "            x = np.asarray(x)\n",
    "            if x.ndim == 0:          # scalar\n",
    "                return x.reshape(1, 1)\n",
    "            elif x.ndim == 1:        # vector\n",
    "                return x.reshape(-1, 1)  # column vector convention\n",
    "            else:                     # already 2D\n",
    "                return x\n",
    "        ...\n",
    "\n",
    "    def dot(self, other: BearTensor) -> BearTensor:\n",
    "        if self.value.ndim != 1 or other.value.ndim != 1:\n",
    "            raise ValueError(\"dot() only supports 1-D BearTensors (like torch.dot).\")\n",
    "        ...\n",
    "\n",
    "    def sum(self) -> BearTensor:\n",
    "        ...\n",
    "\n",
    "    def mean(self) -> BearTensor:\n",
    "        ...\n",
    "\n",
    "    \n",
    "    def relu(self) -> BearTensor:\n",
    "        ...\n",
    "\n",
    "    def sigmoid(self) -> BearTensor:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d3d1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Disclaimer**: The Q1 test cases will NOT test that your adjoint calculations are correct until you implement Q2; if you are struggling with the next question, we recommend also looking back at your solution to Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86a99f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deffccf",
   "metadata": {},
   "source": [
    "Below, we have provided some code that will allow you to visualize your computation graphs and an example for how to use it. You are not required to use nor understand how this works, but it may help you debug while working on this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_typst_math(s: str) -> str:\n",
    "    processed_s = s\n",
    "    processed_s = processed_s.replace(\" @ \", \" \")\n",
    "    processed_s = re.sub(r\"([a-zA-Z])([0-9]+)\", r\"\\1_\\2\", processed_s)\n",
    "    processed_s = re.sub(r\"\\.([a-zA-Z]+)\", r'.\"\\1\"', processed_s)\n",
    "    processed_s = re.sub(r\"([a-zA-Z]+)_([a-zA-Z_]+)\", r'\\1_\"\\2\"', processed_s)\n",
    "    processed_s = re.sub(r\"\\b(relu|sum)\\b\", r'\"\\1\"', processed_s)\n",
    "    return processed_s\n",
    "\n",
    "\n",
    "def _traverse_graph(\n",
    "    root: BearTensor,\n",
    ") -> tuple[list[BearTensor], list[tuple[BearTensor, BearTensor, str]]]:\n",
    "    nodes, edges = set(), set()\n",
    "    visited = set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            nodes.add(v)\n",
    "            for p in v.parents:\n",
    "                edges.add((p.parent, v, p.grad.op_str))\n",
    "                build(p.parent)\n",
    "\n",
    "    build(root)\n",
    "    return list(nodes), list(edges)\n",
    "\n",
    "\n",
    "def _minimize_crossings(\n",
    "    nodes_by_level: list[list[BearTensor]],\n",
    "    edges: list[tuple[BearTensor, BearTensor, str]],\n",
    "    iterations: int = 4,\n",
    ") -> list[list[BearTensor]]:\n",
    "    node_to_parents = {n: [] for level in nodes_by_level for n in level}\n",
    "    node_to_children = {n: [] for level in nodes_by_level for n in level}\n",
    "    for p, c, _ in edges:\n",
    "        if p in node_to_children and c in node_to_parents:\n",
    "            node_to_children[p].append(c)\n",
    "            node_to_parents[c].append(p)\n",
    "\n",
    "    ordered_levels = [list(level) for level in nodes_by_level]\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        for level_idx in range(1, len(ordered_levels)):\n",
    "            child_level = ordered_levels[level_idx - 1]\n",
    "            child_ranks = {node: rank for rank, node in enumerate(child_level)}\n",
    "\n",
    "            def barycenter_down(node):\n",
    "                children = node_to_children.get(node, [])\n",
    "                if not children:\n",
    "                    return -1\n",
    "                avg_rank = sum(child_ranks.get(c, 0) for c in children) / len(children)\n",
    "                return avg_rank\n",
    "\n",
    "            ordered_levels[level_idx].sort(key=barycenter_down)\n",
    "\n",
    "        for level_idx in range(len(ordered_levels) - 2, -1, -1):\n",
    "            parent_level = ordered_levels[level_idx + 1]\n",
    "            parent_ranks = {node: rank for rank, node in enumerate(parent_level)}\n",
    "\n",
    "            def barycenter_up(node):\n",
    "                parents = node_to_parents.get(node, [])\n",
    "                if not parents:\n",
    "                    return -1\n",
    "                avg_rank = sum(parent_ranks.get(p, 0) for p in parents) / len(parents)\n",
    "                return avg_rank\n",
    "\n",
    "            ordered_levels[level_idx].sort(key=barycenter_up)\n",
    "\n",
    "    return ordered_levels\n",
    "\n",
    "\n",
    "def _assign_node_levels(\n",
    "    root: BearTensor,\n",
    "    nodes: list[BearTensor],\n",
    "    edges: list[tuple[BearTensor, BearTensor, str]],\n",
    ") -> dict[BearTensor, tuple[float, float]]:\n",
    "    node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "    id_to_node = {i: node for i, node in enumerate(nodes)}\n",
    "\n",
    "    adj = {node_to_id[n]: [] for n in nodes}\n",
    "    for p, c, _ in edges:\n",
    "        if c in node_to_id and p in node_to_id:\n",
    "            adj[node_to_id[c]].append(node_to_id[p])\n",
    "\n",
    "    levels = {n: -1 for n in nodes}\n",
    "    if root in levels:\n",
    "        levels[root] = 0\n",
    "\n",
    "    q = deque([root])\n",
    "    max_level = 0\n",
    "    visited_bfs = {root}\n",
    "\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        u_id = node_to_id[u]\n",
    "        for v_id in adj.get(u_id, []):\n",
    "            v = id_to_node.get(v_id)\n",
    "            if v and v not in visited_bfs:\n",
    "                levels[v] = levels[u] + 1\n",
    "                max_level = max(max_level, levels[v])\n",
    "                visited_bfs.add(v)\n",
    "                q.append(v)\n",
    "\n",
    "    nodes_by_level = [[] for _ in range(max_level + 1)]\n",
    "    for node, level in levels.items():\n",
    "        if level != -1:\n",
    "            nodes_by_level[level].append(node)\n",
    "\n",
    "    for level_nodes in nodes_by_level:\n",
    "        level_nodes.sort(key=lambda n: n.name)\n",
    "\n",
    "    nodes_by_level = _minimize_crossings(nodes_by_level, edges)\n",
    "\n",
    "    node_coords = {}\n",
    "    for level, nodes_in_level in enumerate(nodes_by_level):\n",
    "        num_in_level = len(nodes_in_level)\n",
    "        for rank, node in enumerate(nodes_in_level):\n",
    "            y_pos = rank - (num_in_level - 1) / 2.0\n",
    "            x_pos = -level\n",
    "            node_coords[node] = (x_pos, y_pos)\n",
    "\n",
    "    return node_coords\n",
    "\n",
    "\n",
    "def _generate_typst_source(\n",
    "    nodes: list[BearTensor],\n",
    "    edges: list[tuple[BearTensor, BearTensor, str]],\n",
    "    coords: dict[BearTensor, tuple[float, float]],\n",
    ") -> str:\n",
    "    node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    header = \"\"\"#import \"@preview/fletcher:0.5.8\": diagram, node, edge\n",
    "#set page(width: auto, height: auto, margin: 10mm, fill: white)\n",
    "#set text(font: \"Linux Libertine\", size: 10pt)\n",
    "\n",
    "#diagram(cell-size: (40mm, 40mm), {\n",
    "\"\"\"\n",
    "\n",
    "    node_definitions = \"\"\n",
    "    for n in nodes:\n",
    "        if n not in coords:\n",
    "            continue\n",
    "        x, y = coords[n]\n",
    "        node_id = node_to_id[n]\n",
    "        math_name = to_typst_math(n.name)\n",
    "        shape_str = str(n.value.shape)\n",
    "        content = f\"\"\"block(stroke: 0.5pt, inset: 8pt, radius: 4pt, [\\n#align(center, ${math_name}$)\\n#line(length: 100%)\\n#text(size: 9pt, `shape: {shape_str}`)\\n])\"\"\"\n",
    "        node_definitions += f'  node(({x}, {y}), {content}, name: \"{node_id}\")\\n'\n",
    "\n",
    "    edge_definitions = \"\"\n",
    "    LABEL_LENGTH_THRESHOLD = 20\n",
    "\n",
    "    for parent, child, op_str in edges:\n",
    "        from_id = node_to_id.get(parent)\n",
    "        to_id = node_to_id.get(child)\n",
    "        if from_id is None or to_id is None:\n",
    "            continue\n",
    "\n",
    "        op_str = op_str or \"\"\n",
    "        math_op_str = to_typst_math(op_str)\n",
    "        label = f\"${math_op_str}$\"\n",
    "\n",
    "        if len(op_str) > LABEL_LENGTH_THRESHOLD:\n",
    "            edge_definitions += f'  edge(label(\"{from_id}\"), label(\"{to_id}\"), \"->\", label: {label}, label-sep: 4em)\\n'\n",
    "        else:\n",
    "            edge_definitions += (\n",
    "                f'  edge(label(\"{from_id}\"), label(\"{to_id}\"), \"->\", label: {label})\\n'\n",
    "            )\n",
    "\n",
    "    footer = \"})\"\n",
    "\n",
    "    return header + node_definitions + edge_definitions + footer\n",
    "\n",
    "\n",
    "def draw_graph(root: BearTensor, output_typ_path: str = \"computational_graph.typ\"):\n",
    "    nodes, edges = _traverse_graph(root)\n",
    "    coords = _assign_node_levels(root, nodes, edges)\n",
    "    typst_source = _generate_typst_source(nodes, edges, coords)\n",
    "\n",
    "    with open(output_typ_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(typst_source)\n",
    "    typst.compile(output_typ_path, output=output_typ_path.replace(\".typ\", \".pdf\"))\n",
    "\n",
    "# Example code\n",
    "a = BearTensor(\"a\", np.array([2, 3]))\n",
    "b = BearTensor(\"b\", np.array([1, 1]))\n",
    "c = a + b\n",
    "draw_graph(c, \"demo_graph.typ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e1ced",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 2**:\n",
    "\n",
    "After solving the previous question, your `BearTensor` should be able to construct the computation graph from the forward pass. In this part, we will implement the backwards pass, using the `BearGrad` functionality from above to propagate the gradients downstream.\n",
    "\n",
    "**Conceptual Overview**\n",
    "\n",
    "When training networks, we want to backpropagate the gradients starting from the very final node, which produced our training loss. To do this efficiently, we adopt a recursive approach; the loss node propagates its gradient back to its parent nodes. Then, every parent node propagates its gradient one layer further back; we keep recursively doing this until gradients have been propagated across the whole computation graph.\n",
    "\n",
    "In lecture, we implemented single-variable `autograd` with two passes when `.backward()` is called: \n",
    "a \"reset\" pass that sets up counters for each node in our computational graph, \n",
    "and then a recursive backward pass to iterate through the entire graph and accumulate each node's adjoint.\n",
    "\n",
    "We can actually perform both of these steps at once using an algorithm called <i>topological sort</i>.\n",
    "This addresses the core problem we are trying to solve: a node can only propagate its gradient backward after it has received gradients from all its children.\n",
    "\n",
    "[Topological sort](https://en.wikipedia.org/wiki/Topological_sorting) starts from the root node (where we call\n",
    "`.backward()`), and traverses our computational graph to determine a correct order of operations. From Wikipedia: \n",
    "it provides a linear ordering that ensures that for any edge $u \\to v$ in our computational graph, $u$ comes before\n",
    "$v$ in the ordering. It turns out that *depth-first search (DFS)* actually gives us a topological sort!\n",
    "\n",
    "You should first implement topological sort, then backprop; this ensures that for deep networks we do not encounter a stack overflow error. One implementation of this is [Kahn's algorithm](https://www.geeksforgeeks.org/dsa/topological-sorting-indegree-based-solution/) which is iterative instead of recursive, avoiding stack overflow errors for deep networks. Alternatively, you can use the implementation discussed in lecture.\n",
    "\n",
    "**Your Task:**\n",
    "\n",
    "Fill out `topological_sort` first, then `reset_children`, and `backward` below to implement backpropagation. After completing this, you finally have your very own PyTorch implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4cc10",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def topological_sort(node):\n",
    "    '''Return a list of nodes ordered topologically'''\n",
    "    ...\n",
    "    return sorted_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e89b385",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def reset_children(self):\n",
    "    \"\"\"Resets the gradient in the current node to zero and all nodes before it in the computation graph.\"\"\"\n",
    "    ...\n",
    "    pass\n",
    "\n",
    "def backward(self):\n",
    "    \"\"\"\n",
    "    Take a node in the computation graph, reset all gradients, and perform backpropagation \n",
    "    to compute the adjoints (gradients) for all nodes in the graph.\n",
    "\n",
    "    Hint: After resetting the gradients, what should the gradient at the current node be?\n",
    "    \"\"\"\n",
    "    ...\n",
    "    pass\n",
    "\n",
    "BearTensor.reset_children = reset_children\n",
    "BearTensor.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdaf55",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45104c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Question 3: Optimizers**\n",
    "\n",
    "Now that we are able to compute the gradients for all the parameters in our model, we want to be able to optimize them. In lecture we talked about Gradient Descent; in this homework, we will implement two widely-used optimizers, Stochastic Gradient Descent and Adam. It is highly recommended that you complete the paper/written portion of the homework before attempting this question.\n",
    "\n",
    "**Your Task**:\n",
    "\n",
    "Implement the SGD, Momentum, and Adam optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f73d8",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params: list[BearTensor], lr: float):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.adjoint = np.zeros_like(p.value)\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params: list[BearTensor], lr: float):\n",
    "        super().__init__(params, lr)\n",
    "\n",
    "    def step(self):\n",
    "        ...\n",
    "        pass\n",
    "\n",
    "class Momentum(Optimizer):\n",
    "    def __init__(self, params: list[BearTensor], lr: float, beta: float = 0.9):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta = beta\n",
    "        self.velocities = [np.zeros_like(p.value) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        ...\n",
    "        pass\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: list[BearTensor],\n",
    "        lr: float,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.999,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.t = 0\n",
    "        self.ms = [np.zeros_like(p.value) for p in self.params]\n",
    "        self.vs = [np.zeros_like(p.value) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a7222",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f4369",
   "metadata": {},
   "source": [
    "The code below compares the convergence rates of all 3 optimizers. Play around with the learning rates and other hyperparameters to see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c5133c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "def compare_optimizers(optimizers):\n",
    "    np.random.seed(0)\n",
    "    X = BearTensor(\"X\", np.random.randn(1000, 24))\n",
    "    y = BearTensor(\"y\", np.random.randn(1000, 1))\n",
    "    \n",
    "    n_steps = 100\n",
    "    results = {}\n",
    "\n",
    "    W1_init = np.random.randn(24, 12)\n",
    "    W2_init = np.random.randn(12, 1)\n",
    "\n",
    "    for opt_class, title, lr in optimizers:\n",
    "        W1 = BearTensor(\"W1\", deepcopy(W1_init))\n",
    "        W2 = BearTensor(\"W2\", deepcopy(W2_init))\n",
    "        params = [W1, W2]\n",
    "\n",
    "        optimizer = opt_class(params, lr=lr)\n",
    "        losses = []\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden = (X @ W1).sigmoid()\n",
    "            output = hidden @ W2\n",
    "            loss = ((output - y) ** 2).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            losses.append(loss.value.item())\n",
    "            optimizer.step()\n",
    "\n",
    "        results[title] = losses\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for title, losses in results.items():\n",
    "        plt.plot(losses, label=title, linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Optimizer Convergence Comparison (Same Init)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "compare_optimizers([\n",
    "    (SGD, \"SGD\", 0.05),\n",
    "    (Momentum, \"Momentum\", 0.05),\n",
    "    (Adam, \"Adam\", 0.05)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce34683",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Question 4: Train a model!**\n",
    "\n",
    "In this section, we're going to use the backpropagation engine and optimizers you created to train a model to predict wine quality scores!\n",
    "\n",
    "**Conceptual Overview:**\n",
    "\n",
    "Unfortunately, training our model is going to be slightly more involved than just using PyTorch. Our simple implementation does not allow us to define layers. Instead, we are going to define `BearTensors`, combine them manually to do a forward pass (which will automatically construct a computation graph!) and then finally backpropagate on our loss.\n",
    "\n",
    "We are going to use the Wine Quality dataset, which contains 1599 samples of red wine. Each sample has 11 features representing various chemical properties, and the task is to predict the wine quality score (between 0 and 10). You can choose any loss function and optimizer (as long as you implemented it!) of your choice, feel free to play around with hyperparameters and different model sizes!\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "Train a model to predict wine quality scores. Your model should:\n",
    "- Use at least one hidden layer\n",
    "- Use an activation function for hidden layers\n",
    "- Achieve a Mean Squared Error (MSE) of ≤ 2.0 on the dataset\n",
    "\n",
    "**You need to fill in the `predict()` function that uses your architecture to return a prediction given an input datapoint. The autograder will use this to test your implementation**.\n",
    "\n",
    "**Note**: If your model takes more than 30 seconds to train it could crash the autograder; you should easily be able to **achieve a MSE of ≤2** on the entire dataset with a 1-layer neural net for full credit. The staff solution takes <1 second to train, so if your code is slow, you may be constructing your graph inefficiently.\n",
    "\n",
    "**You will receive 50% credit for this question if your MSE is ≤ 3.0 on the dataset.**\n",
    "\n",
    "Tips for training models:\n",
    "- Output your training and test loss every epoch; are they smoothly going down? If not, you may need to change the learning rate\n",
    "- Ensure your computation graph is being built properly (remember our visualization helper functions from earlier!)\n",
    "- Changing the number of layers or adding/removing bias terms can be a useful way to control how much your model is overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbe660",
   "metadata": {
    "otter": {
     "tests": [
      "q4"
     ]
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DO NOT CHANGE THE PREPROCESSING CODE\n",
    "data = fetch_openml(\"wine-quality-red\", as_frame=True)\n",
    "X = data.data.to_numpy()\n",
    "y = data.target.to_numpy().astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.4, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_tensor = BearTensor(\"X_train\", X_train)               # shape: (N_train, input_dim)\n",
    "y_tensor = BearTensor(\"y_train\", y_train.reshape(-1,1)) # shape: (N_train,1)\n",
    "\n",
    "# Add your training code here!\n",
    "...\n",
    "\n",
    "# --- Prediction function ---\n",
    "def predict(x):\n",
    "    \"\"\"Output the scalar prediction for a single training datapoint x\"\"\"\n",
    "    ...\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35609ec6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46456b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606981a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba6161",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1": {
     "name": "q1",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> a = BearTensor('a', np.array([1, 2, 3]))\n>>> b = BearTensor('b', np.array([4, 5, 6]))\n>>> c = a - b\n>>> expected_value = np.array([-3, -3, -3])\n>>> assert np.array_equal(c.value, expected_value), f'Forward subtraction failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([1, 2, 3]))\n>>> b = BearTensor('b', np.array([4, 5, 6]))\n>>> c = a + b\n>>> expected_value = np.array([5, 7, 9])\n>>> assert np.array_equal(c.value, expected_value), f'Forward addition failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([1.0, 2.0, 3.0]))\n>>> b = BearTensor('b', np.array([4.0, 5.0, 6.0]))\n>>> c = a * b\n>>> expected_value = np.array([4.0, 10.0, 18.0])\n>>> assert np.allclose(c.value, expected_value), f'Forward multiplication failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a ** 2\n>>> expected_value = np.array([1, 4, 9])\n>>> assert np.array_equal(c.value, expected_value), f'Forward exponentiation failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([[-1, 2], [2, 3]]))\n>>> b = BearTensor('a', np.array([[4, 2], [0, -3]]))\n>>> c = a @ b\n>>> expected_value = np.array([[-4, -8], [8, -5]])\n>>> assert np.array_equal(c.value, expected_value), f'Forward matmul failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> b = BearTensor('a', np.array([4, 2, 2]))\n>>> c = a.dot(b)\n>>> expected_value = np.array([6])\n>>> assert np.array_equal(c.value, expected_value), f'Forward dot product failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.sum()\n>>> expected_value = np.array([4])\n>>> assert np.array_equal(c.value, expected_value), f'Forward sum failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.mean()\n>>> expected_value = np.array([4 / 3])\n>>> assert np.allclose(c.value, expected_value, atol=1e-05), f'Forward sum failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.relu()\n>>> expected_value = np.array([0, 2, 3])\n>>> assert np.allclose(c.value, expected_value, atol=1e-05), f'Forward relu failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.sigmoid()\n>>> expected_value = np.array([0.26894142, 0.88079708, 0.95257413])\n>>> assert np.allclose(c.value, expected_value, atol=1e-05), f'Forward relu failed: {c.value} != {expected_value}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 20,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> a = BearTensor('a', np.array([1, 2, 3]))\n>>> b = BearTensor('b', np.array([4, 5, 6]))\n>>> c = BearTensor('c', np.array([2, 3, 3]))\n>>> d = a - b\n>>> e = d + c\n>>> nodes = topological_sort(e)\n>>> assert len(nodes) == 5\n>>> assert nodes[-1] == e\n>>> assert nodes.index(a) < nodes.index(d)\n>>> assert nodes.index(b) < nodes.index(d)\n>>> assert nodes.index(d) < nodes.index(e)\n>>> assert nodes.index(c) < nodes.index(e)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([1, 2, 3]))\n>>> b = BearTensor('b', np.array([4, 5, 6]))\n>>> c = a - b\n>>> s = c.sum()\n>>> s.backward()\n>>> expected_a_grad = np.ones_like(c.value)\n>>> expected_b_grad = -np.ones_like(c.value)\n>>> assert np.array_equal(a.adjoint, expected_a_grad), f'Gradient for a failed: {a.adjoint} != {expected_a_grad}'\n>>> assert np.array_equal(b.adjoint, expected_b_grad), f'Gradient for b failed: {b.adjoint} != {expected_b_grad}'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([1, 2, 3]))\n>>> b = BearTensor('b', np.array([4, 5, 6]))\n>>> c = a + b\n>>> s = c.sum()\n>>> s.backward()\n>>> assert np.array_equal(a.adjoint, np.ones_like(c.value))\n>>> assert np.array_equal(b.adjoint, np.ones_like(c.value))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([1.0, 2.0, 3.0]))\n>>> b = BearTensor('b', np.array([4.0, 5.0, 6.0]))\n>>> c = a * b\n>>> s = c.sum()\n>>> s.backward()\n>>> assert np.allclose(a.adjoint, b.value)\n>>> assert np.allclose(b.adjoint, a.value)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a ** 3\n>>> s = c.sum()\n>>> s.backward()\n>>> expected_value = np.array([3, 12, 27])\n>>> assert np.allclose(a.adjoint, expected_value)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([[-1, 2], [2, 3]]))\n>>> b = BearTensor('a', np.array([[4, 2], [0, -3]]))\n>>> c = a @ b\n>>> s = c.sum()\n>>> s.backward()\n>>> a_value = np.array([[6, -3], [6, -3]])\n>>> b_value = np.array([[1, 1], [5, 5]])\n>>> assert np.allclose(a.adjoint, a_value)\n>>> assert np.allclose(b.adjoint, b_value)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> b = BearTensor('a', np.array([4, 2, 2]))\n>>> c = a.dot(b)\n>>> s = c.sum()\n>>> s.backward()\n>>> assert np.allclose(a.adjoint, b.value)\n>>> assert np.allclose(b.adjoint, a.value)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.sum()\n>>> c.backward()\n>>> assert np.array_equal(a.adjoint, np.array([1, 1, 1]))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.mean()\n>>> c.backward()\n>>> assert np.array_equal(a.adjoint, np.array([1 / 3, 1 / 3, 1 / 3]))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.relu()\n>>> c.backward()\n>>> assert np.array_equal(a.adjoint, np.array([0, 1, 1]))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> a = BearTensor('a', np.array([-1, 2, 3]))\n>>> c = a.sigmoid()\n>>> c.backward()\n>>> assert np.array_equal(a.adjoint, c.value * (1 - c.value))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 10,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> p = BearTensor('p', np.array([10.0]))\n>>> p.adjoint = np.array([2.0])\n>>> optimizer = SGD([p], lr=0.1)\n>>> optimizer.step()\n>>> assert np.allclose(p.value, 9.8)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = BearTensor('p', np.array([2.0, 3.0]))\n>>> p.adjoint = np.array([3.0])\n>>> optimizer = SGD([p], lr=0.2)\n>>> optimizer.step()\n>>> assert np.allclose(p.value, np.array([1.4, 2.4]))\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = BearTensor('p', np.array([10.0]))\n>>> optimizer = Momentum([p], lr=0.1, beta=0.9)\n>>> p.adjoint = np.array([2.0])\n>>> optimizer.step()\n>>> assert np.allclose(optimizer.velocities[0], -0.2)\n>>> assert np.allclose(p.value, 9.8)\n>>> p.adjoint = np.array([4.0])\n>>> optimizer.step()\n>>> assert np.allclose(optimizer.velocities[0], -0.58)\n>>> assert np.allclose(p.value, 9.22)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> p = BearTensor('p', np.array([10.0]))\n>>> optimizer = Adam([p], lr=0.1, beta1=0.9, beta2=0.999, eps=1e-08)\n>>> p.adjoint = np.array([2.0])\n>>> optimizer.step()\n>>> assert np.allclose(p.value, 9.9)\n>>> p.adjoint = np.array([-1.0])\n>>> optimizer.step()\n>>> assert np.allclose(p.value, 9.8733, atol=0.0001)\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 10,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
