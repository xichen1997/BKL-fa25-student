{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75785e6f",
   "metadata": {
    "id": "07b625d4"
   },
   "source": [
    "# Homework 5: LLM Fine-tuning with Transformers\n",
    "\n",
    "In this homework you will be **finetuning a small instruction-tuned Language Model (LLM) to do well on previous 189 exam problems while maintaining its general knowledge capabilities.** We will be evaluating you on a hidden test set containing 189 exam problems and general knowledge questions, all in multiple choice format.\n",
    "\n",
    "We will walk you through how to fine-tune on custom datasets using the standard Hugging Face `transformers` and `trl` libraries.\n",
    "\n",
    "In this notebook, we use **Qwen/Qwen2.5-0.5B-Instruct**. This is a small but capable model (0.5 billion parameters) that fits easily on most GPUs and trains quickly, allowing us to perform **full fine-tuning** (updating all weights) rather than needing parameter-efficient methods like LoRA (although you are welcome to use LoRA instead).\n",
    "\n",
    "In this notebook, we provide a small subset of CS189 Exam Questions for you to test and a walkthrough of a simple finetuning pipeline. The actual test set you would be making predictions will be a mix of different questions (Full details are provided in the accompanying PDF)\n",
    "\n",
    "## Overview\n",
    "Your task is to:\n",
    "1. **Adapt the provided notebook**\n",
    "2. **Generate predictions** on the private test questions (test.csv)\n",
    "3. **Submit** your results to Kaggle  \n",
    "\n",
    "**Kaggle competition link:**  \n",
    "https://www.kaggle.com/t/11c8ffdc967fe3f27755cde6fb5810e8\n",
    "\n",
    "---\n",
    "### Rules\n",
    "\n",
    "You are encouraged to improve the model's performance!\n",
    "\n",
    "**What you CAN change:**\n",
    "- **Parsing Logic:** You can improve `parse_choice_from_boxed` to handle more edge cases or different output formats.\n",
    "- **Training and Testing Data:** You can mix in additional datasets to the training set and build your own eval sets to test if your model is overfitting.\n",
    "- **Test-Time Adaptations:** You can try different decoding strategies, majority voting, or other inference-time techniques.\n",
    "- **Prompt Engineering:** You can experiment with Chain-of-Thought (CoT) prompting or different system prompts during inference.\n",
    "\n",
    "**What you CANNOT change:**\n",
    "- **The Model:** You must train the `Qwen/Qwen2.5-0.5B-Instruct` model. Do not switch to a different model architecture or size.\n",
    "- **Colab Compatibility:** Your final notebook must be runnable in Google Colab. Do not add dependencies or steps that break this compatibility.\n",
    "\n",
    "#### Evaluation \n",
    "\n",
    "**Important:** Your model will be evaluated on a hidden test set containing both:\n",
    "1.  **CS189 Exam Problems:** Similar to the ones in your training set.\n",
    "2.  **General Knowledge Questions:** To check if the model has retained its general capabilities.\n",
    "\n",
    "**Catastrophic Forgetting:**\n",
    "As shown in the paper you read for this homework, fine-tuning on a narrow dataset (e.g. just CS189 MCQs) can sometimes cause the model to \"forget\" how to answer general questions or lose its reasoning abilities.\n",
    "\n",
    "**Recommendation:**\n",
    "We strongly encourage you to build your own **test set** that includes both domain-specific and general knowledge questions. Use this to monitor your model's performance and ensure it isn't suffering from catastrophic forgetting. You might want to mix in some general datasets during training or use early stopping to prevent this (the paper you read will be useful here).\n",
    "\n",
    "---\n",
    "## The Finetuning Pipeline\n",
    "\n",
    "Now let's walk through a simple finetuning pipeline using the Hugging Face `transformers` and `trl` libraries. Even though we are using a smaller model, the core structure of the finetuning pipeline follows the classic **ML Lifecycle** covered in lecture!\n",
    "\n",
    "<img src=\"https://i.imgur.com/ya2hBEk.png\" width=\"60%\">\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Problem (P)\n",
    "\n",
    "**Goal:** Decide what behavior we want the LLM to learn, and from what data.\n",
    "\n",
    "We want to fine-tune our base Qwen model to better perform on CS189-style multiple choice questions. Our objective is to minimize cross-entropy loss on a dataset of these questions.\n",
    "\n",
    "Concretely, we will:\n",
    "- **Load the CS189 MCQ dataset:** A CSV file containing questions, options (A-E), and the correct answer.\n",
    "- **Format the data:** Convert each row into a \"chat\" format that the model understands.\n",
    "  - User: The question + options.\n",
    "  - Assistant: The correct answer (e.g., `\\boxed{A}`).\n",
    "\n",
    "---\n",
    "## Model Design (L)\n",
    "\n",
    "**Goal:** Decide which model we use and how we adapt it.\n",
    "\n",
    "We use **Qwen/Qwen2.5-0.5B-Instruct**.\n",
    "- **Architecture:** A Transformer-based Causal Language Model.\n",
    "- **Adaptation:** We use **Full Fine-tuning**. Since the model is small, we can update all parameters. This differs from \"LoRA\" (Low-Rank Adaptation) which is often used for larger models (7B+) to save memory.\n",
    "\n",
    "---\n",
    "## Optimization (M)\n",
    "\n",
    "**Goal:** Train the model on the dataset by minimizing loss.\n",
    "\n",
    "We use TRLâ€™s `SFTTrainer` (Supervised Fine-Tuning Trainer) to perform gradient-based optimization:\n",
    "- **Loss function:** Standard token-level cross-entropy loss.\n",
    "- **Optimizer:** AdamW (8-bit version to save some memory, though standard AdamW may also fit depending on your GPU).\n",
    "- **Hyperparameters:** Learning rate, batch size, etc.\n",
    "\n",
    "---\n",
    "## Predict & Evaluate (O)\n",
    "\n",
    "**Goal:** Check whether the fine-tuned model behaves as desired.\n",
    "\n",
    "After training, we:\n",
    "- **Run inference:** Ask the model to answer the MCQs.\n",
    "- **Compute accuracy:** Check if the model's output (parsed from `\\boxed{X}`) matches the ground truth.\n",
    "- **Compare:** We measure accuracy *before* and *after* fine-tuning to quantify improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd23e4",
   "metadata": {
    "id": "5bea7d5f"
   },
   "source": [
    "---\n",
    "## Part 0: Environment Setup\n",
    "\n",
    "This cell installs and imports the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c2e92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAzi-Ieub6vs",
    "outputId": "39053c11-d69f-4c07-f894-7f647dcfedaf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw5\n",
    "    ! pip install -q transformers==4.57.2 accelerate datasets trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfdd3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ef2c9f2",
    "outputId": "3787ed61-9c43-430d-a3da-db22746e4453"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset, load_from_disk\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#MAKE SURE YOU ARE USING GPU\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb46830",
   "metadata": {
    "id": "9a2b62e3"
   },
   "source": [
    "---\n",
    "## Part 1: Configuration & Model Loading\n",
    "\n",
    "Here we define all our settings and load the base model.\n",
    "\n",
    "**Model Design (L):** We select `Qwen/Qwen2.5-0.5B-Instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13e0256",
   "metadata": {
    "id": "db7250a2"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# === CONFIGURATION - ALL SETTINGS IN ONE PLACE ===\n",
    "# ============================================================================\n",
    "\n",
    "# --- Model Configuration ---\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\" # YOU CANNOT CHANGE THIS\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "#TODO: REPLACE WITH YOUR OWN PATH\n",
    "MCQ_CSV_PATH = \"hw5_sample_eval.csv\"  # Path to CS189 MCQ sample eval dataset\n",
    "\n",
    "# --- Training Configuration (feel free to adjust!) ---\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WARMUP_STEPS = 5\n",
    "MAX_STEPS = 50  # or set num_train_epochs instead\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "OPTIM = \"adamw_8bit\"  # requires bitsandbytes\n",
    "SEED = 189\n",
    "\n",
    "# --- Evaluation Configuration ---\n",
    "EVAL_MAX_NEW_TOKENS = 64  # How many tokens to generate for inference\n",
    "OUTPUT_DIR = \"./mcq_finetuned_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703737d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "c223c7dfcad948df828e956cf3b2c7b8",
      "aff9a9bb05dc48cab28b153306297e2f",
      "423624d19c934b8d8a446e79071c2495",
      "9e47a70ccb914d30b36799ea7ee95836",
      "c9a43677ed454a1caa6e3129414a958e",
      "f690e1c0f4da4bb580f36f526a01562e",
      "531e6cc4033d4886a6beb0dc791cf6c5",
      "a81199979de346d2879a771b047dc993",
      "0a0183eb02914bde87812d1be5b7cc17",
      "55d49c3b1e194d26b009510ac30c0ce5",
      "071199027fa04bc8bdf9927547a2da4c",
      "3b22c19427a74679ad32c8515b6891ab",
      "9bff4f9cea914b7a8cc743f0e26b5dd7",
      "424b35b2898e458d808c7ba5d5514265",
      "e74d6ae843344e449b4d2ac5603c917d",
      "fa1ea5f637fa4c109b530c6d75043d28",
      "964b6a11ec884f5ab0d55b0c228c2945",
      "3bbeb6c4e08f4fde993dba9538a776b5",
      "f3b8d841b61141b6a2ebb5e4356d2625",
      "2c78813e81e54c4a85a0e134216450df",
      "b943648740724368804bbf2fd0d28efc",
      "9bd7079176e0450fbed0d79d7525058d",
      "c926721758a34bb4bcb3d70ab9666ff9",
      "cbd45e507a6c4c2d8b597a780e46b0ec",
      "bfc9a87e7870400b83387ea40a9c708e",
      "cf8b8c6e281542968be202a2a7d3980f",
      "4363160752d941c6a759d6e8ed766e31",
      "a613082b99f940acb4847dae9d0114e0",
      "de01d7f093c34114803e05eeb1ee9b89",
      "fcf8d6cf168541c398894a1ef8ec739d",
      "a03191aa021a4ab8b858125513c578ba",
      "150c8baaeee5440bbf85919f04c4cce7",
      "aef96a8b7c1e474597834886cfdc9504",
      "3a805a8b2cd54de2b131ef91f6483819",
      "ce62a11695ef475a83dbed5e7af0b35c",
      "1da18947287442a0a83bffb7e9c75ebd",
      "8cc0c0a5a2c242b4845cb3468ccc42e9",
      "f35b11c6c06d489fb0175d4e6a948073",
      "b1d85265b7f6416683579f7e32fffd64",
      "af4ae691f03a4ec18238e35cf1ee4cbc",
      "06e3c43329814bd3826f1d36422499a3",
      "21b4e296bc294eddbea50954331ebd9e",
      "c4cf02d1ef9d46c9bafb3539743a3511",
      "644393211d9b4add9ae2aebc3d2a8e87",
      "c138c806be284fbdb3d1a5df9792adc0",
      "b119204458024404bc6d48ea3bab310d",
      "d67373205705456fa5f300e2c3a1eb2e",
      "73459f95849d4e4fbfefe44319bdd1a6",
      "231a4d2a164149469e0825a630ed7463",
      "c4e97c0b394a4912b1cfd127f6fe03f3",
      "93eef8099e684caead8a71e47d8921a6",
      "5adc3d33de344393a3484311d42dd38b",
      "240073fe0d214833be6ee53ffa1a7b48",
      "d3f0f95876cf4bb69bc719999c81f295",
      "d0477ea6ec7d4959b8f6870068ee9dd0",
      "4228c14ba559493182c8894899670e6b",
      "db5f8a35b2944a449d00e418e1d99e5e",
      "2eb66de83bc84fa4abf917c5fb1a98f3",
      "311fd21426464610b8340c0fe5cf1011",
      "3e8317e29ead4779906f68ad4fd2f2b4",
      "2cbee7f1ec24401bafacc34f7dd9e269",
      "96c49269b06647de8437a7554d91799e",
      "1c58748032f84c768ab4da2e4d130da1",
      "43a1ef299e804a509f04c7d26ed17a9d",
      "9a0d2c45464d403fbf0e1704fffa8842",
      "f4b5925df8774b4f89c6892423f4b93e",
      "0afd5a9dd5054b7cbf9a1bf88a3d89d2",
      "6a51f244a5054ee9a3b283243d4083b7",
      "73c9b2faa9114d158cca60ff8c5a0992",
      "740870e1c72146ab852f61de2e2ee76f",
      "3552b67db9e44384958770097395e085",
      "df97918b10bc46cf8099c52c17de7ef7",
      "579d7d4dc2524717abc887fc33b07a61",
      "d2d9d91d1d70455fa8fa9df90305e4de",
      "028eb59e817b4630a690842805295de8",
      "cfb600d6aa264c5884803b36abe29340",
      "aee5f07ed0404d1badb674fc11bfbbcb"
     ]
    },
    "id": "23b70ad3",
    "outputId": "1dcaad24-c849-4f4f-e33a-4b37e395291f"
   },
   "outputs": [],
   "source": [
    "# === Load base model & tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure we have a pad token for training\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c015d38",
   "metadata": {
    "id": "a19de667"
   },
   "source": [
    "---\n",
    "## Part 2: Data Preparation\n",
    "\n",
    "**Learning Problem (P):** We need to format our raw CSV data into training examples.\n",
    "\n",
    "We define helper functions to:\n",
    "1.  Load the CSV.\n",
    "2.  Build a \"prompt\" (Question + Options).\n",
    "3.  Build the full \"SFT text\" (Prompt + Answer) using the model's chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f916ce",
   "metadata": {
    "id": "b74f50c6",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# === MCQ helpers ===\n",
    "LETTER_SET = set(list(\"ABCDE\"))\n",
    "\n",
    "def load_mcq_dataset(csv_path: str = MCQ_CSV_PATH):\n",
    "    \"\"\"Load the CS189 MCQ dataset.\n",
    "\n",
    "    Expected columns:\n",
    "        - question\n",
    "        - A, B, C, D, E\n",
    "        - answer (single letter A-E)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = [\"question\", \"A\", \"B\", \"C\", \"D\", \"E\", \"answer\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns in MCQ CSV: {missing}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"answer\"] = (\n",
    "        df[\"answer\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.upper()\n",
    "    )\n",
    "    df = df[df[\"answer\"].isin(LETTER_SET)].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def build_mcq_prompt(row):\n",
    "    \"\"\"Prompt for inference: instruction + question + options.\n",
    "\n",
    "    The model is expected to answer with the correct letter in \\\\boxed{} format.\n",
    "    \"\"\"\n",
    "    q = str(row[\"question\"]).strip()\n",
    "    options = \"\\n\".join([\n",
    "        f\"A. {row['A']}\",\n",
    "        f\"B. {row['B']}\",\n",
    "        f\"C. {row['C']}\",\n",
    "        f\"D. {row['D']}\",\n",
    "        f\"E. {row['E']}\",\n",
    "    ])\n",
    "    prompt = (\n",
    "        \"Choose exactly one correct option from A, B, C, D, and E.\\n\"\n",
    "        \"Return your answer inside a LaTeX box.\\n\\n\"\n",
    "        f\"{q}\\n\\n{options}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621c9c3",
   "metadata": {
    "id": "b4ee0d58"
   },
   "source": [
    "### Understanding the Chat Format (OpenAI Style)\n",
    "\n",
    "To fine-tune a chat model, we need to structure our data as a conversation. This is often called the **OpenAI Chat Format** or **Messages Format**.\n",
    "\n",
    "Instead of a single string of text, each example is a list of dictionaries, where each dictionary represents a message in the conversation:\n",
    "-   `{\"role\": \"user\", \"content\": \"...\"}`: The input prompt or question.\n",
    "-   `{\"role\": \"assistant\", \"content\": \"...\"}`: The model's desired response.\n",
    "\n",
    "For our MCQ task, we structure it as:\n",
    "1.  **User**: \"Choose exactly one correct option... [Question] ... [Options]\"\n",
    "2.  **Assistant**: \"\\\\boxed{A}\"\n",
    "\n",
    "We then use `tokenizer.apply_chat_template()` to convert this structured list into the specific string format that the model expects (e.g., adding special tokens like `<|im_start|>user...<|im_end|>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa84e3b",
   "metadata": {
    "id": "f8d40fdd",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_choice_from_boxed(text: str):\n",
    "    \"\"\"Parse an MCQ choice Aâ€“E from the model output.\n",
    "\n",
    "    We first look for a literal '\\\\boxed{X}' pattern. If not found, we\n",
    "    fallback to the last standalone A-E in the decoded text.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    # Direct \\\\boxed{A} ... \\\\boxed{E}\n",
    "    m = re.search(r\"\\\\boxed\\{\\s*([A-E])\\s*\\}\", text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    # Fallback: last standalone Aâ€“E\n",
    "    letters = re.findall(r\"\\b([A-E])\\b\", text.upper())\n",
    "    if letters:\n",
    "        return letters[-1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a4cc2",
   "metadata": {
    "id": "423b68c0"
   },
   "source": [
    "### **Load and Format the (Eval) Dataset**\n",
    "\n",
    "We load the MCQ dataset and apply the formatting function.\n",
    "These are sample eval sets we provided. The actual test set you would be making predictions will be a mix of different questions (Full details are provided in the accompanying PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70742437",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "f15e4bf4",
    "outputId": "7076569b-4515-4b78-a633-905941b5a48d"
   },
   "outputs": [],
   "source": [
    "# === Load MCQ CSV (Evaluation Data) ===\n",
    "try:\n",
    "    mcq_df = load_mcq_dataset(MCQ_CSV_PATH)\n",
    "    print(f\"Loaded MCQ dataset with {len(mcq_df)} rows from {MCQ_CSV_PATH}.\")\n",
    "except Exception as e:\n",
    "    mcq_df = None\n",
    "    print(\"Error loading MCQ CSV â€” check MCQ_CSV_PATH.\")\n",
    "    raise e\n",
    "mcq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361d2a1",
   "metadata": {
    "id": "937671d5"
   },
   "source": [
    "### **Load Training Dataset (MMLU)**\n",
    "\n",
    "We will use the **MMLU (Massive Multitask Language Understanding)** dataset, specifically the `machine_learning` subset, as our training data. This helps the model learn general machine learning concepts which should transfer to the CS189 exam problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3029e",
   "metadata": {
    "id": "52412671"
   },
   "outputs": [],
   "source": [
    "# === MMLU Helper Functions ===\n",
    "def load_mmlu_dataset(subset: str = \"machine_learning\", split: str = \"test\"):\n",
    "    \"\"\"Load a subset of the MMLU dataset from Hugging Face.\"\"\"\n",
    "    print(f\"Loading MMLU dataset (subset={subset}, split={split})...\")\n",
    "    ds = load_dataset(\"cais/mmlu\", subset, split=split)\n",
    "    return ds\n",
    "\n",
    "def build_mmlu_prompt(row):\n",
    "    \"\"\"Prompt for inference: instruction + question + options.\"\"\"\n",
    "    q = str(row[\"question\"]).strip()\n",
    "    choices = row[\"choices\"]\n",
    "\n",
    "    options_list = []\n",
    "    for i, choice in enumerate(choices):\n",
    "        letter = chr(ord(\"A\") + i)\n",
    "        options_list.append(f\"{letter}. {choice}\")\n",
    "    options_str = \"\\n\".join(options_list)\n",
    "\n",
    "    prompt = (\n",
    "        \"Choose exactly one correct option from the choices provided.\\n\"\n",
    "        \"Return your answer inside a LaTeX box.\\n\\n\"\n",
    "        f\"{q}\\n\\n{options_str}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def build_mmlu_sft_text(row, tokenizer):\n",
    "    \"\"\"Build properly formatted chat template text for training.\"\"\"\n",
    "    user_content = build_mmlu_prompt(row)\n",
    "\n",
    "    answer_int = row[\"answer\"]\n",
    "    answer_letter = chr(ord(\"A\") + answer_int)\n",
    "    assistant_content = f\"\\\\boxed{{{answer_letter}}}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "# === Load MMLU Machine Learning Dataset ===\n",
    "mmlu_ds = load_mmlu_dataset(\"machine_learning\", split=\"test\")\n",
    "mmlu_text_ds = mmlu_ds.map(lambda x: {\"text\": build_mmlu_sft_text(x, tokenizer)})\n",
    "print(\"Loaded MMLU ML dataset with\", len(mmlu_text_ds), \"rows\")\n",
    "\n",
    "# Set the training dataset - you can mix and match datasets here\n",
    "train_dataset = mmlu_text_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ecf67",
   "metadata": {
    "id": "7ceca684"
   },
   "source": [
    "Let's look at an example of the training data to see what the model is actually seeing as input. Note that there are now start and stop tokens `<|im_start|>` and `<|im_end|>` as well as the role `user` and `assistant` indicating who is speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a68768",
   "metadata": {
    "id": "3b25d1d9"
   },
   "outputs": [],
   "source": [
    "# print out what the first row looks like\n",
    "print(train_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944e9a7",
   "metadata": {
    "id": "4e6edca6"
   },
   "source": [
    "---\n",
    "## Part 3: Baseline Evaluation\n",
    "\n",
    "**Predict & Evaluate (O):** Before we train, let's see how the model performs \"zero-shot\" or \"few-shot\" (depending on the prompt) on our CS189 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0a605",
   "metadata": {
    "id": "832894d4"
   },
   "outputs": [],
   "source": [
    "def eval_mcq_accuracy(\n",
    "    curr_model,\n",
    "    curr_tokenizer,\n",
    "    df,\n",
    "    max_new_tokens: int = 64,\n",
    "    return_details: bool = False,\n",
    "):\n",
    "    \"\"\"Evaluate a model on the MCQ dataset using greedy decoding.\n",
    "\n",
    "    If return_details=True, also return a pandas DataFrame with\n",
    "    [idx, question, A, B, C, D, E, gold, decoded, parsed, correct].\n",
    "    \"\"\"\n",
    "    curr_model.eval()\n",
    "    n = len(df)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    records = []\n",
    "\n",
    "    for idx in range(n):\n",
    "        row = df.iloc[idx]\n",
    "        user_content = build_mcq_prompt(row)\n",
    "\n",
    "        # Apply chat template for inference\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        prompt = curr_tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = curr_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = curr_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "            )\n",
    "\n",
    "        gen_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "        decoded = curr_tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "\n",
    "        pred = parse_choice_from_boxed(decoded)\n",
    "        is_correct = (pred is not None and pred == row[\"answer\"])\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        records.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"A\": row[\"A\"],\n",
    "            \"B\": row[\"B\"],\n",
    "            \"C\": row[\"C\"],\n",
    "            \"D\": row[\"D\"],\n",
    "            \"E\": row[\"E\"],\n",
    "            \"gold\": row[\"answer\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"decoded\": decoded,\n",
    "            \"parsed\": pred,\n",
    "            \"correct\": is_correct,\n",
    "        })\n",
    "\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            print(f\"Processed {idx + 1}/{n} questions...\")\n",
    "\n",
    "    acc = correct / max(total, 1)\n",
    "    print(f\"MCQ accuracy: {acc * 100:.2f}% ({correct}/{total})\")\n",
    "\n",
    "    details_df = pd.DataFrame(records)\n",
    "    if return_details:\n",
    "        return acc, details_df\n",
    "    return acc\n",
    "\n",
    "# === Baseline MCQ accuracy before fine-tuning ===\n",
    "print(\"Evaluating baseline model on MCQ dataset...\")\n",
    "baseline_acc, baseline_details = eval_mcq_accuracy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mcq_df,\n",
    "    max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
    "    return_details=True,\n",
    ")\n",
    "baseline_details.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd23a52",
   "metadata": {
    "id": "39e93705"
   },
   "source": [
    "---\n",
    "## Part 4: Training (Optimization)\n",
    "\n",
    "**Optimization (M):** We now configure the `SFTTrainer`.\n",
    "\n",
    "We set:\n",
    "- `dataset_text_field=\"text\"`: Tells the trainer which column contains the formatted chat.\n",
    "- `learning_rate`, `batch_size`: Standard hyperparameters.\n",
    "- `optim=\"adamw_8bit\"`: Efficient optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a66c60",
   "metadata": {
    "id": "88223843"
   },
   "outputs": [],
   "source": [
    "# === Set up SFTTrainer ===\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=1,\n",
    "    optim=OPTIM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=None,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24773043",
   "metadata": {
    "id": "eef700ae"
   },
   "source": [
    "### Run Training\n",
    "\n",
    "This will iterate through the dataset and update the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecfa21",
   "metadata": {
    "id": "1a9b6c41",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# === Fine-tune the model ===\n",
    "model.train()\n",
    "trainer.train()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a1c00",
   "metadata": {
    "id": "7f587f1c"
   },
   "source": [
    "---\n",
    "## Part 5: Post-Training Evaluation\n",
    "\n",
    "**Predict & Evaluate (O):** Now that the model is trained, we evaluate it again on the same MCQ dataset to see if accuracy improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44b8c0",
   "metadata": {
    "id": "0f693fda"
   },
   "outputs": [],
   "source": [
    "# === Evaluate MCQ accuracy after fine-tuning ===\n",
    "print(\"Evaluating fine-tuned model on MCQ dataset...\")\n",
    "ft_acc, ft_details = eval_mcq_accuracy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    mcq_df,\n",
    "    max_new_tokens=EVAL_MAX_NEW_TOKENS,\n",
    "    return_details=True,\n",
    ")\n",
    "ft_details.head()\n",
    "print(f\"Baseline acc: {baseline_acc:.4f}, Fine-tuned acc: {ft_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac6b9c",
   "metadata": {
    "id": "c786f8b9"
   },
   "source": [
    "### Save the Model\n",
    "\n",
    "We save the fine-tuned model and tokenizer so we can use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ca44e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e6105f8",
    "outputId": "e3c59846-2086-44ad-c4bd-adbd472b664b"
   },
   "outputs": [],
   "source": [
    "# === Save fine-tuned model (optional) ===\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Saved fine-tuned model to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bac87",
   "metadata": {
    "id": "kZc5S7Boc7vz"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eb10d0",
   "metadata": {
    "id": "rK5oepUMc1c9"
   },
   "source": [
    "## ðŸ§© **YOUR TURN â€” Final Kaggle Submission**\n",
    "\n",
    "Now itâ€™s your turn to run the *full* ML lifecycle.\n",
    "\n",
    "You should now be able to load and inspect the **private test CSV**, which contains **169 questions** with mixed types.  \n",
    "Use your **fine-tuned LLM** to make predictions on these questions.  \n",
    "**Beware of formatting**: Kaggle will reject incorrectly formatted submissions!\n",
    "\n",
    "### Objective\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. **Use the provided notebook** to fine-tune the base model on your own choice of train data (feel free to adapt the one we provided)\n",
    "2. **Adapt the same pipeline** to run inference on `test.csv`.\n",
    "3. For each row in `test.csv`, output **exactly one letter** from the set  \n",
    "   **{A, B, C, D, E}**.\n",
    "4. Save these predictions in the **strict submission format** described below and\n",
    "   upload your CSV to Kaggle:\n",
    "   - https://www.kaggle.com/t/11c8ffdc967fe3f27755cde6fb5810e8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374de122",
   "metadata": {
    "id": "JRTRFoO8gwzd"
   },
   "source": [
    "\n",
    "### **Submission Format (Strict)**\n",
    "\n",
    "Your submission must be a **CSV** with exactly **two columns** â€” `id` and `prediction` â€” and a **single header row**.\n",
    "\n",
    "A valid submission looks like:\n",
    "\n",
    "| id          | prediction |\n",
    "|-------------|------------|\n",
    "| test_00001  | A          |\n",
    "| test_00002  | A          |\n",
    "| test_00003  | C          |\n",
    "| test_00004  | E          |\n",
    "| ...         | ...        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b7d3d",
   "metadata": {
    "id": "Fy45k2Rqgzbf"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Evaluation Metric\n",
    "\n",
    "Submissions are evaluated using **accuracy**: the fraction of test examples for which your\n",
    "predicted answer matches the hidden correct answer.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "| **id**        | **True Answer** | **Your Prediction** | **Correct?** |\n",
    "|---------------|-----------------|---------------------|--------------|\n",
    "| `test_00001`  | A               | A                   | Yes          |\n",
    "| `test_00002`  | A               | B                   | No           |\n",
    "| `test_00003`  | A               | A                   | Yes          |\n",
    "\n",
    "Here, the accuracy would be 2/3, or approximately 66.7%\n",
    "\n",
    "The leaderboard is split into:\n",
    "\n",
    "- **Public leaderboard**: 50% of the test data  \n",
    "- **Private leaderboard**: remaining 50% (used for final ranking and grading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81747e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "dR3swYFvdMMh",
    "outputId": "4672b980-e7f1-41ed-c50d-0432453fb758"
   },
   "outputs": [],
   "source": [
    "YOUR_PATH_TO_TEST_CSV = 'kaggle_test.csv' #TODO: REPLACE WITH YOUR OWN PATH\n",
    "test_questions = pd.read_csv(YOUR_PATH_TO_TEST_CSV)\n",
    "test_questions\n",
    "#TODO:\n",
    "# 1. Make predictions on your finetuned model\n",
    "# 2. submit to kaggle following the expected format (id, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7e11e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "fivmpZSIhRWl",
    "outputId": "45924cae-ec05-4d1b-94b2-dc0d03240a51"
   },
   "outputs": [],
   "source": [
    "# Dummy Place Holder\n",
    "\n",
    "# Create a dummy submission dataframe with all \"A\"\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_questions[\"id\"],\n",
    "    \"prediction\": [\"A\"] * len(test_questions)\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"dummy_submission.csv\", index=False)\n",
    "\n",
    "print(\"Saved dummy_submission.csv\")\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "cs189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
