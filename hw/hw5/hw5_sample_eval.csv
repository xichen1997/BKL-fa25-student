id,question,A,B,C,D,E,answer
mcq_1,"Peanut wants to train a model to accurately classify different types of animals from images. After training and testing his model, he observes that the model has high training error and high test error. What can we most confidently say about the bias/variance characteristics of Peanut’s model?",High bias.,Low bias.,High variance.,Low variance.,none of the above,A
mcq_2,"Consider a binary classification data set with 9000 positively labelled examples and 1000 negatively labelled examples. What is the area under the ROC curve (AUC-ROC) of a random classifier that classifies any example as positive with probability π and as negative with probability 1 − π? Here, the probability π is a hyperparameter.",Close to zero.,Close to 0.1.,Close to 0.5.,Close to 0.9.,Close to one.,C
mcq_3,"Again, consider a binary classification data set with 9000 positively labelled examples and 1000 negatively labelled examples. What is the precision and the recall of a classifier that always classifies any example as positive?","The precision is 0.1, and the recall is 0.9.","The precision is 0.9, and the recall is 0.1.","The precision is 1.0, and the recall is 0.9.","The precision is 0.9, and the recall is 1.0.","The precision is 0.1, and the recall is 1.0.",D
mcq_4,"Assume we are given X ∈ Rn×d and y ∈ Rn for n > d. The Ridge regression estimator with regularization coefficient λ estimates the weight vector to be  2 2 wˆ=argmin y−Xw2+λ∥w∥2 . (1) w The Ridge regression estimator is equivalent to the ordinary least squares estimator on which of the following modified version of X and y? Id denotes the d × d identity matrix. 0d denotes the all-zero d-dimensional vector, and 1d denotes the all-one d-dimensional vector.","y′ = [ y; 0d ], X′ = [ X; √λ Id ]","y′ = [ y; 1d ], X′ = [ X; √λ Id ]","y′ = [ y; 0d ], X′ = [ X; λ Id ]","y′ = [ y; 1d ], X′ = [ X; λ Id ]",none of the above,A
mcq_5,Which of the following statements are TRUE regarding positive semi-definite and positive-definite matrices?,“Every entry of a matrix is non-negative” is a necessary but insufficient condition for a matrix to be positive definite.,The singular values of a positive semi-definite matrix are the same as its eigenvalues.,"If a matrix A is positive semi-definite, then there exists a matrix B such that BT B = A. (heuristic)",The covariance matrix of any distribution is positive semi-definite and invertible.,"If the Jacobian of a function is positive semi-definite, then the function is convex.",B
mcq_6,"Which of the following statements are TRUE regarding Lasso and Ridge regression? Let X ∈ Rn×d be the data matrix and y ∈ Rn be the observed labels for the n examples. Let w ∈ Rd be the weight parameters learned from Lasso or Ridge regression, and let λ be the regularization coefficient.","In the Bayesian MAP interpretation, Lasso regression can be interpreted as linear regression for which the coefficients have Poisson prior distributions.","In Lasso regression, as the regularization coefficient becomes very large (λ → ∞), the learned weights from Lasso regression will be close to zero (w → 0).",Lasso regression performs both feature expansion and regularization.,There is no unique solution to Ridge regression whenever X is not full rank.,none of the above,B
mcq_7,"If the model resulting from Ridge regression is currently overfitting, what are possible things to reduce overfitting? Select all that apply.",Collect new data to increase the training data size.,Repeat the current data twice to increase the training data size.,Increase the ℓ1 regularization penalty in the loss function.,Add new features to the model.,Add synthetic features from the model.,A
mcq_8,Which of the following statements are TRUE about gradient descent?,"After a gradient descent update step, the objective function value at the weight vector is always lower after the update than before.",There is always a unique steepest descent direction in gradient descent.,Gradient descent converges to a globally optimal solution for logistic regression under appropriate assumptions.,"Since ReLU is a convex function, a neural network that uses ReLU activations is also a convex function, and therefore gradient descent will converge to a globally optimal solution on neural networks with ReLU activations.",none of the above,C
mcq_9,Which of the following statements are TRUE about cross entropy?,The value of cross-entropy loss is always non-negative.,Cross-entropy loss is only suitable for binary classification but not for multiclass classification.,"For two discrete probability distributions P and Q, the KL divergence is symmetric, i.e. DKL(P||Q) = DKL(Q||P).",Minimizing the cross-entropy is equivalent to maximizing the likelihood with re- spect to the model parameters. (heuristic),none of the above,A
mcq_10,"Which of the following statements are TRUE about cross validation? In this problem, the final test set is not involved in the cross validation process.","During the k-fold cross validation process, precisely k models are trained on different subsets of the data.","During the k-fold cross validation process, precisely k − 1 models are trained on different subsets of the data.","During the k-fold cross validation process, we need to draw k random seeds to shuffle the data precisely k times.","At the end of the k-fold cross validation process, we choose hyperparameters that minimize the highest validation loss among the different splits.",none of the above,A
mcq_11,"Which of the following statements are TRUE about principle component analysis (PCA)? Given n data points of dimension d, X ∈ Rn×d, assume that we are performing PCA to reduce the dimension of the data to k where k < d. Which of the following would result in a DIFFERENT PCA basis? Recall that the PCA basis is a set of unit vectors.",Multiplying all columns in X by a factor of two before performing PCA.,Multiplying the first column in X by a factor of two before performing PCA.,Replacing the first column of X by the sum of all columns (including the first column). (heuristic),Not subtracting the median from X before performing PCA.,none of the above,B
mcq_12,Which of the following statements are TRUE about weight updates in neural networks?,"Typically, the weights in all hidden layers are initially all set to zero.","If using the mean squared error loss, the weight changes in the last layer are proportional to the difference between the model output and the true labels.",The weight changes in a particular hidden layer are proportional to the input to that weight layer. (heuristic),Weight updates are computed in the forward pass of the network.,none of the above,B
mcq_13,"Assume a linear model Y = Xw∗ + z, where z ∼ N(0, In) and w∗ is the true parameter we are trying to estimate. Consider the following objective:
wˆ = argminw∈Rd ∥Xw − Y∥^2_2 + λ∥w∥^2_2, λ > 0
How will increasing λ in Equation 1 affect the bias of the resulting estimator ˆw?",Bias will increase.,Bias will decrease.,Bias will remain unchanged.,Decaf coldbrew latte is delicious,none of the above,A
mcq_14,How will increasing λ in Equation 1 affect the variance of the resulting estimator ˆw?,Variance will increase.,Variance will decrease.,Variance will remain unchanged.,Decaf coldbrew latte is delicious,none of the above,B
mcq_15,"For this question, X ∈ R^{n×d} denotes an input data matrix of rank d, y ∈ R^n an outcome vector, wridge the ridge regression solution, and wOLS the solution to unregularized linear regression. Select the false statement.","For all X and true linear predictors w∗ ∈ R^d, under the statistical assumption y = Xw∗ + z, z ∼ N(0, σ^2 I_n), we have bias(wridge) ≥ bias(wOLS), where the bias of an estimate wˆ of w∗ is defined as bias(wˆ ) = ∥E[wˆ ] − w∗∥_2.","For all X and y, ∥wridge∥_2 ≤ ∥wOLS∥_2.","For all X and y, ∥wridge∥_1 ≤ ∥wOLS∥_1.",Decaf coldbrew latte is delicious,none of the above,C
mcq_16,"Suppose you have a dataset where the label is binary and generated by a fair coin toss and the input features are generated by sampling i.i.d. from a standard Gaussian, independently of the label. Let n denote the number of examples in your dataset and d the number of input features. You perform logistic regression using a random 70/30 train-validation split. Let Acctrain denote the training accuracy and Accval the validation accuracy. Select the false statement.","As n → ∞, Acctrain approaches 50%.","As n → ∞, Accval approaches 50%.","As d → ∞, Acctrain approaches 100%.","As d → ∞, Accval approaches 100%.",none of the above,D
mcq_17,"You are walking down Shattuck Ave. when you find a quarter on the ground. You see nothing unusual about this quarter, so you figure it is almost certainly a fair coin, though you realize that manufacturing irregularities in the coin minting process mean that coins are rarely exactly fair. You toss the coin 10 times and observe the following outcomes: H H H H H H H H H T (H=heads, T=tails). Assume coin tosses are independent. What is the maximum likelihood estimate of the next toss being heads?",5/10,between 5/10 and 9/10,9/10,more than 9/10,none of the above,C
mcq_18,Consider the setup of the previous problem (Problem 6). What is the maximum a posteriori (MAP) estimate of the next toss being heads?,5/10,between 5/10 and 9/10,9/10,more than 9/10,none of the above,B
mcq_19,Consider a binary classification problem with two outcomes: positive and negative. The F1 score of a classifier is the harmonic mean between precision and recall: F1 = 2 p·r / (p+r). Select the true statement relating F1 on the test set to the bias and variance of its estimated parameters. Assume the classical (non-interpolating) regime of bias and variance.,Low bias implies a high F1 score.,Low variance implies a high F1 score.,A low F1 score implies a high variance.,A high F1 score implies low bias.,none of the above,D
mcq_20,Julia is using SNE to visualize her high-dimensional dataset in two dimensions. She runs her code twice to find that it outputs different visualizations each time. Why is this expected? Assume she did not fix a random seed.,"This is expected due to the nonconvexity of the optimization objective, and also occurs with t-SNE.","This is expected due to the Gaussian distributions in SNE, and could be addressed by using t-SNE instead.","This is expected due to the iterative nature of SNE, and could be addressed by using PCA to find the solution to the SNE objective without gradient descent.",Decaf coldbrew latte is delicious,none of the above,A
mcq_21,"Which of the following is a problem with the sigmoid activation function, in the context of deep neural networks?",Sigmoid is prone to vanishing gradients at extreme values.,Sigmoid can take on negative values.,"Sigmoid is non-linear, which provides less representation power.",Sigmoid is numerically unstable when the input is large.,none of the above,A
fa2025_Q3A,"In the bias–variance decomposition, what does the term E[(t – h(x))^2] represent?",Bias,Bias^2,Variance,Variance^2,Noise,E
fa2025_Q3B,"In the bias–variance decomposition, the expression E[(f_w(x) – E[f_w(x)])^2] corresponds to which concept?",Bias,Bias^2,Variance,Variance^2,Cold brew latte is delicious,C
fa2025_Q3C,"If we reduce the L1 regularization parameter λ in a linear model, what most likely happens to (h(x) – E[f_w(x)])^2?",Increases,Stays the same,Decreases,Cappuccino is my favorite,The moon is made of cheese,C
fa2025_Q3D,Suppose we change the feature basis to a constant feature ϕ(x) = 1. What happens to the model variance E[(f_w(x) – E[f_w(x)])^2]?,Remains the same,Increases,Decreases,Avocado toast,Matcha cream puff,C
