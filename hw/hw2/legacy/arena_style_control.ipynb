{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw2\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6fdfb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"arena_style_control.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0294b00",
   "metadata": {
    "id": "S5acmOY9WR4j"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Homework 02 ‚Äì Welcome to the Arena (Style Control)</h1>\n",
    "\n",
    "CS 189, Fall 2025\n",
    "\n",
    "In this homework you will get more experience with logistic regression in two very different settings: creating leaderboards and predicting model responses.\n",
    "\n",
    "We will be taking real data from [LMArena](https://lmarena.ai/), a popular platform for crowsourcing evaluations of large language models and recreating their leaderboards, with a few fun extra steps along the way.\n",
    "\n",
    "The chats can be viewed interactively by accessing [ChatBot-Arena-Viewer](https://huggingface.co/spaces/BerkeleyML/Chatbot-Arena-Viewer) through hugging face. Much of the first half of this homework was first written by Prof Gonzalez back when his students first started the project, and now LMArena is a standard evaluation for large language models and turned into a company! Don't let anyone tell you logistic regression isn't valuable, it's worth at least $600 Million.\n",
    "    \n",
    "---\n",
    "\n",
    "\n",
    "## Due Date: Friday, October 17, 11:59 PM\n",
    "\n",
    "This assignment is due on **Friday, October 17, 11:59 PM**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "### Submission Tips\n",
    "- **Plan ahead**: We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early**: If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "      \n",
    "<!-- --- -->\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "In this homework you will build on the previous warmup section, implementing the Bradley-Terry ranking used in the actual Arena and taking account of style controls for model rank. In particular, you will:\n",
    "1. Apply the Bradley‚ÄìTerry model to build leaderboards\n",
    "2. Practice analyzing conversational data and extracting stylistic features  \n",
    "3. Build custom features (length, punctuation, phrase presence, etc.) and integrate them into ranking models  \n",
    "4. Explore confounding stylistic variables in LLM evaluation (style vs. content)  \n",
    "5. Apply pairwise evaluation methods to understand how style affects outcomes  \n",
    "  \n",
    "---\n",
    "\n",
    "### Collaboration Policy\n",
    "You are encouraged to discuss high-level concepts with your peers. However:\n",
    "- All submitted work must be written in your own words and code.\n",
    "- Do not share or copy solutions directly.\n",
    "- List any collaborators (students you worked with) in the line below:\n",
    "\n",
    "**Your Collaborators**: **TODO**\n",
    "\n",
    "### AI Tools Usage Disclosure\n",
    "We allow the use of AI tools (e.g., ChatGPT, Copilot) **only as support**, not as a replacement for your own reasoning. To ensure transparency, you must acknowledge any use of AI tools.\n",
    "\n",
    "Please complete one of the following:\n",
    "- **A) I did not use any AI tools for this homework.**\n",
    "- **B) I used AI tools in the following way(s):**  \n",
    "  (describe briefly, e.g., ‚ÄúUsed ChatGPT to get hints for debugging a NumPy indexing error‚Äù)\n",
    "\n",
    "\n",
    "**Your Answer**: **TODO**\n",
    "    \n",
    "---\n",
    "\n",
    "### Grading Breakdown\n",
    "\n",
    "| Question | Manual Grading? | Points |\n",
    "|----------|-----------------|--------|\n",
    "| q4a      | No              | 2      |\n",
    "| q4b      | No              | 2      |\n",
    "| q4c      | Yes             | 2      |\n",
    "| q5a      | No              | 2      |\n",
    "| q5b      | No              | 2      |\n",
    "| q6a      | No              | 2      |\n",
    "| q6b      | No              | 2      |\n",
    "| q6c      | No              | 2      |\n",
    "| q6d      | No              | 2      |\n",
    "| q7a      | No              | 2      |\n",
    "| q7b      | No              | 2      |\n",
    "| q8a      | No              | 2      |\n",
    "| q8afrq   | Yes             | 2      |\n",
    "| q8b      | No              | 2      |\n",
    "| q8c      | No              | 2      |\n",
    "| q8d      | No              | 2      |\n",
    "| q9a      | No              | 2      |\n",
    "| q9b      | Yes             | 4      |\n",
    "| q9c      | Yes             | 6      |\n",
    "| **Total**|                 | **44** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503bc49a",
   "metadata": {
    "executionInfo": {
     "elapsed": 1661,
     "status": "ok",
     "timestamp": 1754949252732,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "Fjrjh-brWR4n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotting_utils import plot_rank_heatmap, plot_style_features\n",
    "#set fixed seed of 189\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29ad47",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1754949269115,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2BHL7ichWR4o"
   },
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500168a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "ee9bd84d37b74df599bb04a0674d96ae",
      "f999e16b1f03445f819a2d780482f8fd",
      "924710c922e04900bbf39d5c0148184a",
      "3b29af24732d4ce09978703f5819620e",
      "b2090f9ca74d446b80879d3e7bb8fa0e",
      "7fe95e7a459c4b188f911ec6a2a869d1",
      "6b77a7fe1f434937bab915ab291d84c0",
      "1f03c4dc981047348492cd1ee93844b4",
      "a34a2b4628534adeb1275f54d9f856fd",
      "7269d9a850684affadbb845fac333fd6",
      "40c695f88e114b8a804305936f15e34a",
      "f44ab2de0a66487b99385b06ca3e0927",
      "6da5f82604ae425f9a15d0d4a036001e",
      "e5727bca7f8c4e64abe2271add37c3c4",
      "515d41fc4015482c9f3e1de36ec964a5",
      "799bb3928ea24eff837684dc6f84becf",
      "7c0d795d30ba4782b0ebff647f6376f0",
      "5c7e225b4ebd4e65be7d7315a9dd8168",
      "663a003ffbc84a4787ddf7ba13ce1d7e",
      "21f023f65970482e84f7daa78dc16b34",
      "86d5261e767645d088bafd4f616603f2",
      "e0bb2688693a4ab1815d897cb67ada37",
      "49400e054cde42759dab2d5a519c71c7",
      "5396890a6a2e47c785a9c6b410e7bac7",
      "96273c7dae504597875d36bdcb81e7c2",
      "a5a749f2d39048089df69d673dece032",
      "d116ee9efee24cad9b7ae54c4ccc95ea",
      "c19dac25e6044be194c44e7ecb87ef44",
      "361a0ef284604dbea5fb4c499a4b2d74",
      "8e92ffcb211b4a43847b14d240f6f006",
      "99105de4cb444e9ca2291b9a9fed5c14",
      "cb90820ae343424494f84181b496aca3",
      "096e39f01bfd4eb889d988253db708ff"
     ]
    },
    "executionInfo": {
     "elapsed": 30891,
     "status": "ok",
     "timestamp": 1754949300115,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "VpHcbudcWR4o",
    "outputId": "e64834e3-ba19-41c6-fc8f-8e61c63adb4c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset (this will take a few minutes to download)\n",
    "ds = load_dataset(\"lmarena-ai/arena-human-preference-100k\")\n",
    "battles = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21a677",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1754949300351,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "hUb4ANfAWR4p",
    "outputId": "4b3c3625-107c-489e-958d-4fb51cb35337"
   },
   "outputs": [],
   "source": [
    "print(\"Before dedup: \", len(battles))\n",
    "battles = battles[battles[\"dedup_tag\"].apply(lambda x: x.get(\"sampled\", False))]\n",
    "print(\"After dedup: \", len(battles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e6c74",
   "metadata": {},
   "source": [
    "## Initialize with HW2 Warmup\n",
    "\n",
    "Fill in the cells in this section with your implementations from part 1 of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d51f61",
   "metadata": {},
   "source": [
    "#### FILL IN: HW2 Part 1 Question 1B Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a77032",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754949300357,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "OJdE_ePCWR4p"
   },
   "outputs": [],
   "source": [
    "# #TODO: FILL IN from your solution for Question 1B\n",
    "# models = ...\n",
    "# selected_models = ...\n",
    "# selected_battles = ...\n",
    "# selected_battles_no_ties = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb012b",
   "metadata": {},
   "source": [
    "##  **Question 4: Model Strengths**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ee982",
   "metadata": {},
   "source": [
    "In the earlier part of the homework, we calculated the Average Model Win-Rate.\n",
    "\n",
    "However, this method is not ideal for our use case where battle counts per model are not equal. For instance, if ChatGPT-4o-latest battled more often with weaker models, it would have a high win rate without being an actually stronger model. Now let's explore how we can instead *learn* these model strengths.\n",
    "\n",
    "**To recap,** we want to construct a leaderboard by assigning a strength score $S_m$ to each model $m \\in \\{1,...,M\\}$, such that:\n",
    "- The ranking reflects the probability of one model winning against another.\n",
    "- For any pair of models A and B, the probability that A beats B, should depend on the *difference* in their strengths: $S_A - S_B$. Why the difference? Since we are measuring pairwise preference, there is no absolute measure of strength but rather a model's strength *relative* to other models.\n",
    "\n",
    "**Formally, we want a function $f$ such that**\n",
    "- For models A and B with scores $S_A$ and $S_B$, we want:\n",
    "  $$ P(\\text{A beats B}) = f(S_A - S_B) $$\n",
    "- The function $f$ should be increasing (bigger skill gap, higher win chance), and always output a probability between 0 and 1.\n",
    "\n",
    "At this point, a natural question is: what should we choose for the function $f$? A standard and effective choice is the logistic (sigmoid) function:\n",
    "\n",
    "$$ P(\\text{A beats B}) = \\sigma(S_A - S_B) = \\frac{1}{1 + e^{-(S_A - S_B)}} $$\n",
    "\n",
    "Notice that this is exactly the same form as logistic regression, where the model scores are the parameters to be learned. In other words, learning model strengths from pairwise outcomes is equivalent to fitting a logistic regression model to the data.\n",
    "\n",
    "So, we can use logistic regression to learn the model strengths that best explain the observed battle outcomes. The higher a model's score, the more likely it is to win against others. The methodology is called the [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) model and is the underlying theory to other common scoring systems like ELO ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234dff26",
   "metadata": {},
   "source": [
    "#### Step 1: Understanding the formulation of the problem as features\n",
    "\n",
    "To learn these model strengths, we need to prepare our data in a form suitable for logistic regression. Recall that each battle involves two models: A and B. One of them wins (for simplicity we still start by removing any battles that end in ties).\n",
    "\n",
    "We want to convert this into:\n",
    "\n",
    "1. A feature vector indicating which two models were involved.\n",
    "2. A label representing the winner.\n",
    "\n",
    "Each row produces two training examples:\n",
    "\n",
    "1. One with model A as +1 and model B as ‚Äì1, and a label denoting if model A wins\n",
    "2. Another with model B as +1 and model A as ‚Äì1, and a label denoting if model B wins\n",
    "\n",
    "Let's take a look an example:\n",
    "\n",
    "`row = {'model_a': 'gpt-4o-2024-05-13', 'model_b': 'claude-3-opus-20240229', 'winner': 'model_a'}`\n",
    "\n",
    "This generates two features, which are...\n",
    "\n",
    "**Feature 1:** [1, -1]\n",
    "* +1 at index 0 (gpt-4o)\n",
    "* ‚Äì1 at index 1 (claude-3-opus)\n",
    "* Label: 1 because model A (gpt-4o) won\n",
    "\n",
    "**Feature 2:** [-1, 1]\n",
    "* -1 at index 0 (gpt-4o)\n",
    "* +1 at index 1 (claude-3-opus)\n",
    "* Label: 0 because model B (claude-3-opus) lost\n",
    "\n",
    "Why do we have to do this?\n",
    "This lets the model take account for both ways:\n",
    "\n",
    "\n",
    "$$ P(\\text{GPT-4o beats Claude-3-opus}) = \\sigma(S_{\\text{GPT-4o}}  - S_{\\text{Claude}})$$\n",
    "\n",
    "\n",
    "\n",
    "$$ P(\\text{Claude-3-opus beats GPT-4o}) = \\sigma(S_{\\text{Claude}} - S_{\\text{GPT-4o}}  )$$\n",
    "\n",
    "\n",
    "**Stop and Think:** When we have more than two models, how should we handle the models that were not invovled in the battle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23ce2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 2: Constructing generalized features and labels\n",
    "\n",
    "Cool, now we want to generalize this formulation to the pair of not only GPT-4o and Claude-3-opus, but all the models.\n",
    "\n",
    "Once we have turned all battle outcomes into feature vectors, we can organize them into a **feature matrix** $\\mathbf{X}$ and a **label vector** $\\mathbf{y}$.\n",
    "\n",
    "We have the model strengths we want to learn:\n",
    "\\begin{bmatrix}\n",
    "S_A \\\\\n",
    "S_B \\\\\n",
    "S_C\n",
    "\\end{bmatrix}\n",
    "\n",
    "And we want our model to predict:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\sigma(S_A - S_C) \\\\\n",
    "\\sigma(S_B - S_A) \\\\\n",
    "\\sigma(S_B - S_C)\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "As a recap...\n",
    "\n",
    "- $\\mathbf{X}$ encodes **who played whom** and in what direction.\n",
    "- $\\mathbf{S}$ are the model strengths we are trying to learn.\n",
    "- $\\mathbf{y} = \\sigma(\\mathbf{X} \\cdot \\mathbf{S})$ gives us the predicted win probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387f5b6",
   "metadata": {},
   "source": [
    "Phew, that was a long. Now let's try to actually featurize these battles and labels!\n",
    "\n",
    "However, before analysis, let's deduplicate common prompts like \"hi\" and \"hello\" to ensure they don't overly influence the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfd893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before dedup: \", len(battles))\n",
    "battles = battles[battles[\"dedup_tag\"].apply(lambda x: x.get(\"sampled\", False))]\n",
    "print(\"After dedup: \", len(battles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909d1a3f",
   "metadata": {},
   "source": [
    "Also, let's focus on the top 20 models like we have done for 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a96f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = battles.model_a.value_counts().index.tolist()\n",
    "selected_models = models[:20]\n",
    "selected_battles = battles[battles['model_a'].isin(selected_models) & battles['model_b'].isin(selected_models)]\n",
    "selected_battles_no_ties = selected_battles[~selected_battles[\"winner\"].str.contains(\"tie\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd4e35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 4a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3234dc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to train our model, we should first featurize our battles as discussed before.\n",
    "\n",
    "**Task:** Implement the function below to transform `selected_battles_no_ties` and `selected_models` into feature vectors and labels. This will allow us to represent each battle as input-output pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbe88a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def turn_into_features(df, models):\n",
    "    '''\n",
    "    Convert pairwise battle results into feature matrix X and label vector y\n",
    "    suitable for logistic regression based on the Bradley-Terry model\n",
    "    '''\n",
    "    # TODO:\n",
    "    # 1. Iterate through each row in the DataFrame.\n",
    "    # 2. For each battle, create a feature vector:\n",
    "    #    - Assign +1 to the column corresponding to 'model_a'.\n",
    "    #    - Assign -1 to the column corresponding to 'model_b'.\n",
    "    #    - All other columns should be 0.\n",
    "    # 3. Append the label:\n",
    "    #    - 1 if 'model_a' is the winner.\n",
    "    #    - 0 if 'model_b' is the winner.\n",
    "    # 4. Return the feature matrix X and label vector y as numpy arrays.\n",
    "    ...\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = turn_into_features(selected_battles_no_ties, selected_models)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c1c1b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169a1a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 4b**\n",
    "Now that we have extracted out the features from the previous question, let's now dive into actually building the model. \n",
    "\n",
    "**Task:** \n",
    "Train the model with the features and labels created in Question 4a, and store the strengths, sorted in the order of scores in `results_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f378e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = ...\n",
    "scores = ...\n",
    "\n",
    "results = {\"Model\": selected_models, \"Score\": scores}\n",
    "results_df = pd.DataFrame(results).sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706168ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50bfeb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 4c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7414c13",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's think about an important aspect of our formulation.\n",
    "\n",
    "**Task:** Answer the following question: Why we don't need an intercept for the logistic regression formulation above?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "359b1b97",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b97c02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# **Question 5: Confidence Intervals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddedfc",
   "metadata": {},
   "source": [
    "From the previous question, we were able to train the model and obtain the scores of the models. \n",
    "\n",
    "However, when comparing model scores, it's important to understand not just the average performance, but also how much uncertainty there is in our estimates. Our rankings are based on a finite sample of battles, and if we had collected a different set of match-ups, the resulting scores could be different. This sampling variability means that our estimated model strengths are subject to noise.\n",
    "\n",
    "Bootstrapping is a powerful, intuitive way to assess this uncertainty without making strong assumptions about the underlying data. By repeatedly resampling our observed battles (with replacement) and retraining the model on each resampled dataset, we simulate what might have happened if we had observed a slightly different set of battles. For each resample, we get a new set of model scores. By looking at the distribution of these bootstrapped scores, we can estimate confidence intervals for each model's strength.\n",
    "\n",
    "In short, bootstrapping helps us answer: \"If we repeated this evaluation process many times, how much could each model's score vary just due to random chance in which battles we happened to observe?\" This gives us a more honest sense of which differences in model scores are robust, and which might just be due to luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b515f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 5a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d3f1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's implement a function that returns these scores and confidence intervals after bootstrapping.\n",
    "\n",
    "**Task:** \n",
    "* Bootstrap the samples to train a new logistic regression model.\n",
    "* Store each set of coefficients (or learned model strengths).\n",
    "* Compute the mean and percentiles (2.5th and 97.5th) to obtain the 95% confidence intervals.\n",
    "* Return i) results_df, ii) mean_scores, iii) confidence_intervals.\n",
    "\n",
    "An example outout of results_df is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8a53b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://imgur.com/ZmC1PhW.png\" alt=\"WarmupPairwisePlot\" style=\"display: block; margin-left: auto; margin-right: auto; width: 80%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08164228",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_bootstrapped_score(X, y, models, category_name=\"Overall\", n_bootstrap=25):\n",
    "    \"\"\"\n",
    "    Bootstraps logistic regression model scores to estimate confidence intervals.\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        models: List of model names (order matches columns of X)\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "    Returns:\n",
    "        results_df: DataFrame with Model, Average Score, Lower Bound, Upper Bound\n",
    "        mean_scores: Mean of bootstrapped scores (np.array)\n",
    "        confidence_intervals: 2.5 and 97.5 percentiles (np.array shape [2, n_models])\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    np.random.seed(189)  # for reproducibility\n",
    "    bootstrap_scores = []\n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        ...\n",
    "        model = ...\n",
    "        model.fit(...)\n",
    "        ...\n",
    "    ...\n",
    "    mean_scores = ...\n",
    "    ...\n",
    "    results = {\n",
    "        \"Model\": ...\n",
    "        \"Average Score\": ...\n",
    "        \"Lower Bound\": ...\n",
    "        \"Upper Bound\": ...\n",
    "    }\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, mean_scores, confidence_intervals\n",
    "results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=25)\n",
    "\n",
    "# Test that confidence intervals make sense\n",
    "assert (confidence_intervals[0] <= confidence_intervals[1]).all(), \"Every lower bound must be <= upper bound.\"\n",
    "assert ((confidence_intervals[0] <= mean_scores) & (mean_scores <= confidence_intervals[1])).all(), \"Each mean score should lie within its CI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978d22b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd231e0d",
   "metadata": {},
   "source": [
    "### Now let's visualize the intervals! *üßô*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=25)\n",
    "fig = go.Figure()\n",
    "\n",
    "# Use the sorted values from results_df for plotting\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df[\"Model\"],\n",
    "    y=results_df[\"Average Score\"],\n",
    "    mode='markers',\n",
    "    name='Model Scores',\n",
    "    marker=dict(size=5, color='blue'),\n",
    "    error_y=dict(\n",
    "        type='data',\n",
    "        array=results_df[\"Upper Bound\"] - results_df[\"Average Score\"],   # Upper error\n",
    "        arrayminus=results_df[\"Average Score\"] - results_df[\"Lower Bound\"],  # Lower error\n",
    "        visible=True\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Scores with 95% Confidence Intervals (Sorted by Mean Score)',\n",
    "    xaxis_title='Models',\n",
    "    yaxis_title='Score',\n",
    "    xaxis=dict(tickangle=45),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa3d12",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 5b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f877d3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have confidence intervals, we can assign a rank to each model. We want the rank of model $i$ to represent the number of models that are **confidently better** than model $i$.\n",
    "\n",
    "When we say model A is **confidently better** than model B, it will mean that model A's lower bound is still greater than model B's upper bound. Remember that greater rank means that there are more models that perform better than the current model.\n",
    "\n",
    "**Task:**\n",
    "Implement the `assign_rank` function below that assigns rank to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d2e70",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def assign_rank(row, df=results_df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        row : pd.Series\n",
    "            A row of the DataFrame (representing a model‚Äôs metrics).\n",
    "        df : pd.DataFrame (default = results_df)\n",
    "            DataFrame containing model performance with 'Lower Bound' and 'Upper Bound'.\n",
    "\n",
    "    Output:\n",
    "        int : The rank of the model, defined as (# of models confidently better) + 1.\n",
    "    \"\"\"\n",
    "\n",
    "    count = ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "results_df['Rank'] = results_df.apply(lambda r: assign_rank(r, results_df), axis=1)\n",
    "results_df = results_df.sort_values(by=\"Rank\", ascending=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b471aa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213b971",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plot_rank_heatmap(results_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900663e8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "> **NOTICE BEFORE YOUR PROGRESS\n",
    "(Q6 Data)**\n",
    "> - If you accidentally modify `selected_battles_no_ties` in a way that breaks later parts, double check and **reset it** using the initial block of code you have placed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a2364",
   "metadata": {},
   "source": [
    "# **Question 6: Category Leaderboards**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d1e6f",
   "metadata": {
    "id": "YgAjZ7oqWR4s"
   },
   "source": [
    "So far, we have computed overall model rankings using all available battles.\n",
    "However, models may perform differently in specific categories, such as creativity, technical_accuracy, instruction_following, or math.\n",
    "Now that we know how to get rankings, let's see what the leaderboards look like for certain categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c40340",
   "metadata": {
    "id": "1RVe-BP9K4v2"
   },
   "source": [
    "Breaking this down, we want to do the following:\n",
    "\n",
    "1. For each category we are interested in, filter the battles to only those belonging to a given category.\n",
    "2. Compute bootstrapped confidence intervals for model strengths in that category.\n",
    "3. Rank the models within that category.\n",
    "4. Combine category-specific ranks with the overall leaderboard into a single DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f57b3",
   "metadata": {},
   "source": [
    "### Function Reference For Q6\n",
    "\n",
    "Below is a summary of the functions you have already implemented that might be helpful. Remember that the **overall leaderboard** (used in Q6d) should be a DataFrame named `results_df` with a unique **`Model`** column and an overall **`Rank`**.\n",
    "\n",
    "| Function | Inputs (types) | Output | One-liner purpose | Where you‚Äôll use it |\n",
    "|---|---|---|---|---|\n",
    "| `turn_into_features` | `df_filtered: pd.DataFrame`, `selected_models: list[str]` | `X, y` | Build model‚Äìvs‚Äìmodel feature matrix `X` and labels `y` from filtered battles. | Q6a, Q6d |\n",
    "| `get_bootstrapped_score` | `X`, `y`, `selected_models: list[str]`, `n_bootstrap: int` | `results_df, ci_low, ci_high` | Bootstrap model strengths; returns per-model scores + confidence intervals. | Q6a, Q6d |\n",
    "| `assign_rank` | `row: pd.Series` (row of `results_df`) | `int` | Compute a row's rank from its score(s). | Q6a, Q6d |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50340fb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LpplCsBEKQrK"
   },
   "source": [
    "## **Question 6a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4b3c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Dx9Yk9hfKkLR"
   },
   "source": [
    "Let's start with the first step described above! \n",
    "\n",
    "Specifically, we will implement a general function that returns a DataFrame filtered to the battles belonging to a given mask, computes model scores using bootstrapping, assigns ranks, and finally returns the results sorted by rank. This function will allow us to conveniently obtain the score for any specific category.\n",
    "\n",
    "---\n",
    "\n",
    "**Task:**\n",
    "Implement a function `get_category_results` that does the following:\n",
    "\n",
    "- Takes in a DataFrame of battles, a boolean filter mask for a category, and a list of models.  \n",
    "- Filters the battles to those in the category.\n",
    "- Turns the filtered battles into features using `turn_into_features`\n",
    "- Computes bootstrapped scores for the selected models using `get_bootstrapped_score`\n",
    "- Assigns ranks based on model performance.  \n",
    "- Returns a DataFrame sorted by ascending rank (best model first).  \n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "- **`df` (pd.DataFrame)**  \n",
    "  The full battles DataFrame. Each row corresponds to a single head-to-head battle between two models, along with metadata such as the category of the prompt (e.g., `\"math\"`, `\"coding\"`, `\"writing\"`).  \n",
    "\n",
    "- **`filter_mask` (pd.Series[bool])**  \n",
    "  A boolean array (same length as `df`) that indicates which rows to keep.  \n",
    "  - Example: `filter_mask = (df[\"category\"] == \"math\")` produces a Series of `True`/`False` values.  \n",
    "  - When applied as `df.loc[filter_mask]`, only rows where the mask is `True` are kept.  \n",
    "  - This lets us focus only on battles from a specific category.  \n",
    "\n",
    "- **`selected_models` (list[str])**  \n",
    "  A list of model names (strings) to evaluate and compare. The function will restrict bootstrapped scoring and ranking to this set.  \n",
    "  - Example: `[\"gpt-4\", \"llama-2\", \"claude-3\"]`.  \n",
    "\n",
    "- **`n_bootstrap` (int, optional, default = 25)**  \n",
    "  The number of bootstrap resamples to use when estimating model scores. Larger values give more stable estimates but take longer to compute.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc678898",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754949319791,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "uDSdc5wOWR4s",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_category_results(df, filter_mask, selected_models, category_name=\"Overall\", n_bootstrap=25):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - df (pd.DataFrame): The full DataFrame of battles.\n",
    "      - filter_mask (pd.Series[bool]): A boolean mask selecting rows belonging \n",
    "        to a specific category (e.g., df[\"category\"] == \"math\").\n",
    "      - selected_models (list[str]): A list of model names to include.\n",
    "      - n_bootstrap (int, optional): Number of bootstrap resamples (default = 25).\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: A ranked DataFrame of models with bootstrapped scores,\n",
    "      sorted by ascending rank (best model first).\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_df = ...\n",
    "    ...\n",
    "    ...\n",
    "    results_df['Rank'] = ...\n",
    "    return ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55117017",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c7672",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4twd9LMqLbMq"
   },
   "source": [
    "## **Question 6b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cb8a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1oBbmmFrLvEL"
   },
   "source": [
    "We computed the score for each of the categories in the previous question. Ultimately, we want to compute the intervals for model strengths in these new categories like we originally did before. To achieve this, we first need to extract out the relevant characteristic of each of the battles (whether it is creative, has certain technical accuracy, etc.).\n",
    "\n",
    "**Task:**\n",
    "Using the `selected_battles_no_ties` DataFrame, create four new boolean columns that indicate whether each battle belongs to a given category:\n",
    "* creative\n",
    "* technical_accuracy\n",
    "* instruction_following\n",
    "* math\n",
    "\n",
    "These columns should be derived from the nested dictionary in the `category_tag `column.\n",
    "We are going to make a copy first to avoid pandas SettingWithCopyWarning.\n",
    "\n",
    "**Hint:** Each of these categories is stored inside a specific subkey (e.g., \"criteria_v0.1\" or \"math_v0.1\") within `category_tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384b89a",
   "metadata": {
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1754949346112,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "OK9aF2KSKio3",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# extract category columns\n",
    "\n",
    "#TODO\n",
    "# We are going to make a copy first to avoid pandas SettingWithCopyWarning.\n",
    "selected_battles_no_ties = selected_battles_no_ties.copy()\n",
    "selected_battles_no_ties.loc[:, 'creative'] = ...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87697061",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Memory cleanup: delete large dataframes no longer needed\n",
    "del battles, selected_battles\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41dc70",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858497e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VCfmBe8dMVxB"
   },
   "source": [
    "## **Question 6c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618756f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "o-1sUE-GMXxt"
   },
   "source": [
    "Now that obtained the relevant characteristic of each battle, let's try to define a filter that extracts out the battles we want for each category. \n",
    "\n",
    "**Task:**\n",
    "Define the category filters for each of the categories.\n",
    "Specifically, using the `selected_battles_no_ties` DataFrame, create a dictionary called `category_filters` that maps each category name to a boolean mask selecting only the battles in that category.\n",
    "\n",
    "Your dictionary should include filters for:\n",
    "*  'english' (battles where language is \"English\")\n",
    "*  'coding' (battles where is_code is True)\n",
    "*  'creative' (battles where creative is True)\n",
    "*  'instruction_following' (battles where instruction_following is True)\n",
    "*  'math' (battles where math is True)\n",
    "*  'technical_accuracy' (battles where technical_accuracy is True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c6805",
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1754949407853,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "9ozz8w7HMU66",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "category_filters = {\n",
    "    'english': selected_battles_no_ties['language'] == 'English',\n",
    "    ...\n",
    "    ...\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af74913",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f084a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1DncSLxqQ_q5"
   },
   "source": [
    "## **Question 6d**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef251f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hU6sNOd6SUdf"
   },
   "source": [
    "\n",
    "Now that we have all the filters defined, let's compute the **per-category leaderboards** and combine them with the **overall leaderboard** in tidy data format.\n",
    "\n",
    "**Task:**\n",
    "Compute per-category leaderboards using `get_category_results` and combine them into a single tidy DataFrame.\n",
    "\n",
    "1. **Overall leaderboard:** Use `get_category_results` with a mask that includes all battles (all `True` values) to generate the overall leaderboard with category name \"Overall\".\n",
    "\n",
    "2. **Per-category leaderboards:** For each category in `category_filters`, use `get_category_results` to build a DataFrame containing `Model`, `Category`, `Average Score`, `Lower Bound`, `Upper Bound`, and `Rank`.\n",
    "\n",
    "3. **Tidy format combination:** Concatenate all category results into a single DataFrame with columns `['Model', 'Rank', 'Category']` where:\n",
    "   - Each row represents one model's performance in one category\n",
    "   - The `Category` column identifies which category the rank belongs to (e.g., \"Overall\", \"english\", \"coding\", etc.)\n",
    "\n",
    "4. **Sort final table:** Sort each category dataframe by Rank then sort the entire dataframe by Category (both ascending)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8475b88",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PtzHPwIWVFG4"
   },
   "source": [
    "**Example Output:**\n",
    "\n",
    "| Model               | Rank | Category              |\n",
    "|---------------------|-----:|----------------------:|\n",
    "| chatgpt-4o-latest   | 1    | Overall               |\n",
    "| gemini-1.5-pro-exp-0801 | 2    | Overall               |\n",
    "| gpt-4o-2024-05-13   | 3    | Overall               |\n",
    "| chatgpt-4o-latest   | 1    | english               |\n",
    "| gemini-1.5-pro-exp-0801 | 2    | english               |\n",
    "| gpt-4o-2024-05-13   | 4    | english               |\n",
    "| chatgpt-4o-latest   | 1    | coding                |\n",
    "| gpt-4o-2024-05-13   | 2    | coding                |\n",
    "\n",
    "This is in the same tidy format as the previous part of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f338c",
   "metadata": {
    "executionInfo": {
     "elapsed": 39889,
     "status": "ok",
     "timestamp": 1754949456089,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "v1uj3YE7Rhfd",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Create tidy format by concatenating all category results (including overall)\n",
    "# We have provided the overall mask for you\n",
    "\n",
    "overall_mask = pd.Series([True] * len(selected_battles_no_ties), index=selected_battles_no_ties.index)\n",
    "\n",
    "category_results_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9faf360",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed7710",
   "metadata": {
    "id": "PjSJlzQ7VRbE"
   },
   "source": [
    "Amazing! Now let's plot the heatmap showing rank across the categories üßô. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c4c73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1754949473093,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "bvoRsGVtWR4t",
    "outputId": "95b851bf-f009-43a0-9894-bfbe6a7dd855"
   },
   "outputs": [],
   "source": [
    "fig = plot_rank_heatmap(category_results_df, title=\"Model Rankings by Category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82c5b7",
   "metadata": {
    "id": "YSy82VV0WR4t"
   },
   "source": [
    "**Something to ponder upon:** We see model rankings can change a lot depending on the type of question being asked. Sometimes these make sense, like how deepseek coder gets much higher rankings on coding problems, but sometimes it isn't clear why one model does better than another. Especially for things like creative tasks, why do people like Gemini 1.5 so much more than Claude 3.5?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebc5ab",
   "metadata": {},
   "source": [
    "# **Question 7: Ranking Influences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77695f",
   "metadata": {
    "id": "BqCtCfqjWR4u"
   },
   "source": [
    "One thing that has been known to affect user preference is response length: people (and LLM's) tend to prefer longer answers. A recurring observation in human grading and UX is that **longer responses are often preferred**. For example, analyses from the SAT essay reported that **essay length strongly correlated with higher scores‚Äîeven when errors were present** ([New York Times, 2005](https://www.nytimes.com/2005/05/04/education/sat-essay-test-rewards-length-and-ignores-errors.html)). \n",
    "\n",
    "In the context of LLM evaluations, this motivates a core question: **does response length systematically tilt battle outcomes and model rankings?**\n",
    "\n",
    "\n",
    "Let's investigate whether length plays a role in model rankings. First let's do some quick analysis on the response length per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e5fc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9ubVgwtsWcd0"
   },
   "source": [
    "## **Question 7a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a36ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CTjSPxR-YZew"
   },
   "source": [
    "We want to analyze whether **response length (in tokens)** is related to model rankings.\n",
    "\n",
    "In `per_model_battles` (which is what you would implement in Q7b), the **`conversation`** column contains, for each row, a *single exchange* between a user and a model (one battle). It is represented as a **list of message dictionaries**. These dictionaries are representing a full exchange between a user and a model in a single battle. The number of turns is `len(row['conversation']/2`.\n",
    "\n",
    "**Each message dictionary contains (as provided):**\n",
    "1. `\"content\"` ‚Äì the text of the message  \n",
    "3. `\"role\"` ‚Äì either `\"user\"` or `\"assistant\"`. In our question we will be focusing on `\"assistant\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### What exactly is `conv`?\n",
    "\n",
    "For this question, assume your function will receive **`conv`**, which is a **dictionary** with a single key `\"conversation\"` mapping to a **list of message dictionaries**:\n",
    "\n",
    "```python\n",
    "conv_example = {\n",
    "    \"conversation\": [\n",
    "        {\"role\": \"user\", \"content\": \"How do I sum a list in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Use the built-in function: sum(your_list).\"}\n",
    "    ]\n",
    "}\n",
    "# conv_example[\"conversation\"]  -> list of message dicts, ordered by turns\n",
    "# Each dict has:\n",
    "#   - \"role\": \"user\" or \"assistant\"\n",
    "#   - \"content\": str (message text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bdd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-pzu_xZcWhl_"
   },
   "source": [
    "**Task:**\n",
    "Implement a function `calculate_response_length` that, given a conversation `conv`, returns the total number of GPT-2 tokens in the concatenation of all **assistant messages** in that conversation.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use the tiktoken library with the \"gpt2\" encoding. Import tiktoken.\n",
    "\n",
    "2. Concatenate only the assistant messages and count tokens.\n",
    "\n",
    "3. Call `enc.encode(..., disallowed_special=())` to allow all special tokens (avoids ValueError, e.g., for <|endoftext|>).\n",
    "\n",
    "4. Concatenate all assistant role messages separated by two newlines (\"\\n\\n\") before counting tokens (join each them by this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f09d6",
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1754949482422,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "tBHBaRLDWR4u",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def calculate_response_length(conv):\n",
    "    ...\n",
    "    ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bb23c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab331db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c4pSBvMCZW4N"
   },
   "source": [
    "## **Question 7b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2445ef2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J6UGwlkkc_rc"
   },
   "source": [
    "In the previous question (Q7a), you wrote a function to compute the **token length** of a model's reply from a conversation. We‚Äôll now **reshape** the battle-level data so that each row corresponds to a **single model‚Äôs response in a single battle** (instead of one row per battle).\n",
    "\n",
    "In the `selected_battles_no_ties` DataFrame, each row represents a battle between two models, with:\n",
    "\n",
    "*   conversation_a = the conversation for model_a in that battle\n",
    "*   conversation_b = the conversation for model_b in that battle\n",
    "\n",
    "\n",
    "To analyze response length per model, we would want a table where each row corresponds to a single model's response in a single battle (rather than one row per battle).\n",
    "\n",
    "Specifically, our goal is to turn each battle row into **two rows** (tidy format):\n",
    "1. one for `model_a` using `conversation_a`\n",
    "2. one for `model_b` using `conversation_b`\n",
    "\n",
    "**Task:**\n",
    "Using the function defined in Question 7a, create a DataFrame named `per_model_battles ` with columns:\n",
    "\n",
    "1. **`conversation`** ‚Äî the list of message dicts for that model‚Äôs side of the battle  \n",
    "2. **`model`** ‚Äî the model name  \n",
    "3. **`response_length`** ‚Äî integer token count of all assistant messages concatenated (computed via **`calculate_response_length`** from Q7a)\n",
    "> **Hint:** `pd.concat` might be handy for stacking the A-side and B-side tables into one.  \n",
    "> Docs: https://pandas.pydata.org/docs/reference/api/pandas.concat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be7be9",
   "metadata": {
    "executionInfo": {
     "elapsed": 27703,
     "status": "ok",
     "timestamp": 1754949516871,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "Z7VkycO_ZVHY",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "do NOT use .copy() on large dataframes, it was cause RAM autograder errors\n",
    "battles_a = ..\n",
    "battles_b = ..\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "per_model_battles[...] = ...\n",
    "per_model_battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b957b7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6204e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1754949525877,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "p-TtEVXcrYyz",
    "outputId": "7f9c53dc-9cf5-495c-b99b-7aa5b5c37f15"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the structure\n",
    "# print(per_model_battles['conversation'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd8b62",
   "metadata": {
    "id": "snbYDBeXWR4u"
   },
   "source": [
    "Let's plot the response length for each model ordered by their rank and fit a trendline to see if there is any relation between rank and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68fe06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1754949531743,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "-nLUbYgOWR4u",
    "outputId": "bab1ff8f-681b-4c02-f481-83b9632f5f9d"
   },
   "outputs": [],
   "source": [
    "model_lineup = results_df.sort_values(\"Rank\")['Model'].tolist()\n",
    "avg_lengths = per_model_battles.groupby(\"model\")[\"response_length\"].mean().reset_index()\n",
    "avg_lengths[\"model\"] = pd.Categorical(avg_lengths[\"model\"], categories=model_lineup, ordered=True)\n",
    "avg_lengths = avg_lengths.sort_values(\"model\").reset_index(drop=True)\n",
    "\n",
    "# Add a numeric rank column for trendline fitting\n",
    "avg_lengths[\"rank\"] = avg_lengths.index + 1  # 1 = best, etc.\n",
    "\n",
    "# Fit a linear trendline (polyfit) to the response length vs. rank\n",
    "z = np.polyfit(avg_lengths[\"rank\"], avg_lengths[\"response_length\"], 1)\n",
    "p = np.poly1d(z)\n",
    "trendline = p(avg_lengths[\"rank\"])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=avg_lengths[\"model\"],\n",
    "    y=avg_lengths[\"response_length\"],\n",
    "    mode='lines+markers',\n",
    "    name='Avg Response Length'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=avg_lengths[\"model\"],\n",
    "    y=trendline,\n",
    "    mode='lines',\n",
    "    name='Trendline',\n",
    "    line=dict(dash='dash', color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average Response Length of Models (with Trendline)\",\n",
    "    xaxis_title=\"Model (sorted by performance)\",\n",
    "    yaxis_title=\"Average Response Length\",\n",
    "    xaxis_tickangle=45,\n",
    "    yaxis=dict(range=[0, max(avg_lengths[\"response_length\"].max(), trendline.max()) * 1.05])  # y-axis starts at 0\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3c60b",
   "metadata": {},
   "source": [
    "# **Question 8: Style Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510e00d",
   "metadata": {
    "id": "Q1PEDWwGi-XO"
   },
   "source": [
    "It looks like there is a trend: models with shorter responses tend to be ranked lower. While not a perfect analysis, if it could be true that people are preferring models which generate longer responses regardless of their other capabilities, then it would be useful to create a leaderboard which is *length agnostic*. Meaning, creating leaderboard model scores that control for certain stylistic properties of responses.\n",
    "\n",
    "So how can we control for these stylistic factors in our model rankings?\n",
    "\n",
    "In this question, you will implement a style feature ranking pipeline, starting with length as the only style feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1090d66",
   "metadata": {
    "id": "e7eLOGTJjAzy"
   },
   "source": [
    "1. Each row in `selected_battles_no_ties` represents a battle between model_a and model_b.\n",
    "\n",
    "2. Each battle contains `conv_metadata` with pre-computed style metrics, such as bold text counts, header counts, list counts, and token counts for each side.\n",
    "\n",
    "3. In the earlier question, each battle was converted to pairwise feature, where 1 denoted the model it belongs to, and -1 for the model that it was battling against. Now, our goal is to build on that data, including both the model identity indicators and the chosen style features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c41801",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UDiZ3-6ni4fw"
   },
   "source": [
    "## **Question 8a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074620d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tRAbgVzvjd90"
   },
   "source": [
    "We want to add style features other than length that will give us style feature aware ranks.\n",
    "\n",
    "\n",
    "**Task:**\n",
    "Implement the function `add_style_features` that reads stylistic metrics from `conv_metadata` between each model (model_a, model_b) for count of bold, header, list, and assistant tokens. Then, store the normalized differences in columns (with the designated names):\n",
    "\n",
    "1. style_bold_count\n",
    "2. style_header_count\n",
    "3. style_list_count\n",
    "4. style_sum_assistant_tokens\n",
    "\n",
    "The normalized differences would be following the formulation below:\n",
    "\n",
    "$$\n",
    "\\text{normdiff}(a, b) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } a + b = 0 \\\\[6pt]\n",
    "\\dfrac{a - b}{a + b} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "‚ö†Ô∏è **Important note**: Make sure you are not mutating the original DataFrame passed into your function. Work on a copy (df.copy()) and return that new DataFrame with the added columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c9e0a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jwdMDnr9j-W-"
   },
   "source": [
    "Let's take a took at `conv_metadata`. Essentially, we will be stacking up these elements for each style counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd3e57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1754949540997,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2e3UXdX4jmoC",
    "outputId": "09c6667c-7ed0-46ff-dac2-4135707ea1bc"
   },
   "outputs": [],
   "source": [
    "selected_battles_no_ties['conv_metadata'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e12ce",
   "metadata": {
    "executionInfo": {
     "elapsed": 2454,
     "status": "ok",
     "timestamp": 1754949546102,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "fC8wAZV_WR4u",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def add_style_features(df):\n",
    "    \"\"\"\n",
    "    Adds normalized style feature difference columns to the DataFrame.\n",
    "    The columns added are:\n",
    "      - style_bold_count\n",
    "      - style_header_count\n",
    "      - style_list_count\n",
    "      - style_sum_assistant_tokens\n",
    "    \"\"\"\n",
    "    def normdiff(a, b):\n",
    "        denom = a + b\n",
    "        return 0 if denom == 0 else (a - b) / denom\n",
    "\n",
    "    ...\n",
    "    style_bold = []\n",
    "    style_header = []\n",
    "    style_list = []\n",
    "    style_tokens = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ...\n",
    "      \n",
    "    df[\"style_bold_count\"] = style_bold\n",
    "    df[\"style_header_count\"] = style_header\n",
    "    df[\"style_list_count\"] = style_list\n",
    "    df[\"style_sum_assistant_tokens\"] = style_tokens\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "selected_battles_no_ties = add_style_features(selected_battles_no_ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173021f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd48d9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 8a Free Response Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2084464",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We‚Äôve now added stylistic features to each model comparison.  \n",
    "\n",
    "**Answer the following question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e5d8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER: How can integrating these features into the ranking pipeline create the effect of a ‚Äúlength-controlled‚Äù leaderboard, and why might this adjustment be useful?  \n",
    "\n",
    "Think about whether raw win/loss outcomes fully capture model quality, or whether stylistic inflation (e.g., longer answers, formatting tricks) can bias rankings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6cb1c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6bf828",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hXcAGuzmmyDI"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## **Question 8b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0217c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kGV8OzYEm_p_"
   },
   "source": [
    "Let's try to visualize and formulate the features we defined in the previous question in a neat way that we can see the direct relationship between the model battles and the style features.\n",
    "\n",
    "We now want a training table where each battle produces two rows, one for each ordering of the competitors (A‚ÜíB and B‚ÜíA).\n",
    "In other words, each row in the dataframe creates two entries in the new table.\n",
    "\n",
    "**Task:**\n",
    "Implement a function that creates the table described above. Each row should encode:\n",
    "1. The model identity vector **X** (+1 at the selected model in the row, ‚àí1 at its opponent, 0 elsewhere)\n",
    "2. The outcome y (win, lose)\n",
    "3. Set of style covariates capturing A vs B normalized differences (e.g., length)\n",
    "\n",
    "**NOTE:** Ensure that features are also antisymmetric, meaning they flipping the order of model should also flip the sign of each style feature.\n",
    "\n",
    "Below is an example of the desired table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628951e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lOpyU8W4njiN"
   },
   "source": [
    "| question_id                         | X                                           | y | direction | style_bold_count | style_header_count | style_list_count | style_sum_assistant_tokens |\n",
    "|--------------------------------------|----------------------------------------------|---|-----------|------------------|--------------------|------------------|----------------------------|\n",
    "| e8fe7c9f75ab4e528367cc7de625c475     | [0, 0, 0, 0, 0, 1, ...]  | 0 | A->B      | 1.0              | 0.0                | 1.0              | 0.07717                    |\n",
    "| e8fe7c9f75ab4e528367cc7de625c475     | [0, 0, 0, 0, 0, -1 ...] | 1 | B->A      | -1.0             | -0.0               | -1.0             | -0.07717                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa92477",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "executionInfo": {
     "elapsed": 7180,
     "status": "ok",
     "timestamp": 1754949561831,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "A1KeK9x-jXqY",
    "outputId": "1f7aff2f-036e-46a7-c002-ee478e64bc87",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def make_pairwise_feature_df(df, models, style_feature_cols):\n",
    "    \"\"\"\n",
    "    For each row in df, create two rows in the output:\n",
    "      - One for A->B (original direction)\n",
    "      - One for B->A (flipped direction)\n",
    "    Each row contains:\n",
    "      - question_id\n",
    "      - X: model indicator vector (1 for model_a, -1 for model_b, 0 otherwise)\n",
    "      - y: 1 if model_a wins, 0 if model_b wins\n",
    "      - style features (from style_feature_cols)\n",
    "      - direction: \"A->B\" or \"B->A\"\n",
    "    Returns a new DataFrame with only these columns.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # X vector for A->B and B->A\n",
    "        ...\n",
    "        ...\n",
    "        # y for A->B and B->A\n",
    "        ...\n",
    "        ...\n",
    "        # Style features for A->B, B->A\n",
    "\n",
    "\n",
    "        # Add A->B, B->A\n",
    "        \n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "style_feature_cols = [\n",
    "    \"style_bold_count\",\n",
    "    \"style_header_count\",\n",
    "    \"style_list_count\",\n",
    "    \"style_sum_assistant_tokens\"\n",
    "]\n",
    "\n",
    "pairwise_feature_df = make_pairwise_feature_df(selected_battles_no_ties, selected_models, style_feature_cols)\n",
    "pairwise_feature_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f6ed6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaed3933",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uZfDJ9HK02zq"
   },
   "source": [
    "## **Question 8c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8a6d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yclx2ftl1N2t"
   },
   "source": [
    "Amazing! üéâ Now that we've built our pairwise identity matrix X (which model is battling which) and our style feature matrix X_style (how A and B differ stylistically), lets's combine these two so that our logistic model can learn:\n",
    "\n",
    "1. The intrinsic strength of each model (controlling for style)\n",
    "2. The influence of each style feature on the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035201e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3wfxI8vk2K5i"
   },
   "source": [
    "**Task:**\n",
    "Now, implement the function `get_sc_category_results` that:\n",
    "1. Stacks these two matrices into one design matrix X_with_style, so the model can learn both intrinsic model strengths and style effects simultaneously.\n",
    "2. Uses `get_bootstrapped_score` to get the ranking results.\n",
    "3. Returns the results sorted by rank in ascending order. All style features should be assigned a rank of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218f535",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 23804,
     "status": "ok",
     "timestamp": 1754949659157,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "quY8yXMcWR4v",
    "outputId": "4651256d-6b12-4f5f-ac3f-07521e7bb209",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_sc_category_results(df, selected_models, filter_mask = None, category_name=\"Overall w/ Style Control\", n_bootstrap=25, style_features=style_feature_cols):\n",
    "    feature_labels = selected_models + style_features\n",
    "    ...\n",
    "    \n",
    "    #TODO\n",
    "    feature_labels = selected_models + style_features\n",
    "    ...\n",
    "\n",
    "    # rerank so the models are ranked and the style features are given a rank of -1\n",
    "    return results_df.sort_values(by=\"Rank\", ascending=True)\n",
    "results_df_style_control = get_sc_category_results(selected_battles_no_ties, selected_models, category_name=\"Overall w/ Style Control\", n_bootstrap=25)\n",
    "combined_results_df = pd.concat([results_df, results_df_style_control])\n",
    "fig = plot_rank_heatmap(combined_results_df, title=\"With and Without Style Control\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3943c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f92daf",
   "metadata": {
    "id": "Q4Ltt-gnWR4w"
   },
   "source": [
    "### Look at the impact of style\n",
    "\n",
    "Now let's visualize the style feature scores with confidence intervals using our premade `plot_style_features` in `plotting_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ff110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 1896,
     "status": "ok",
     "timestamp": 1754949608819,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2fEWg8eTWR4w",
    "outputId": "76784944-91da-484c-f5f7-ea63a0408c50"
   },
   "outputs": [],
   "source": [
    "# plot style feature coefficients\n",
    "fig = plot_style_features(results_df_style_control, selected_models)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207644ec",
   "metadata": {},
   "source": [
    "Here we see that length matters a LOT (in fact this coefficient is higher than the actual model coefficients), while things like bold and lists don't matter as much. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ddac6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "D-JQgjZC5vaE"
   },
   "source": [
    "## **Question 8d**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ba95e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FtUVFCXxWR4w"
   },
   "source": [
    "Let's add the stylistic features to our computation of per-category leaderboards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6214a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0FyXSnPPoJNV"
   },
   "source": [
    "**Goal:** Extend your category leaderboards from Q7d to the style-controlled setting.\n",
    "\n",
    "**Note:** This mirrors the merge pattern you used in Q7d. Reuse that approach, but start from the style-controlled baseline (not the plain baseline).\n",
    "\n",
    "**Task:**\n",
    "Now, re-using the function `get_sc_category_results` and `category_filters` that was previously defined, define `category_style_control_results_df` that has all the bound scores for the other categories (english, coding, creative, instruction_following, math, technical_accuracy). Specifically,\n",
    "\n",
    "1. Using `get_sc_category_results` and the provided category_filters, compute per-category results. \n",
    "\n",
    "3. Name the final DataFrame category_stle_control_results_df and sort by \"Rank\" ascending. This should be the same structure as the previous result `combined_results_df`."
   ]
  },
  {
   "cell_type": "raw",
   "id": "48bc1492",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "category_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58c454",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 52750,
     "status": "ok",
     "timestamp": 1754949725846,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "sY4a4JutnzXq",
    "outputId": "13b181a6-af1f-4d08-8e1f-2e4680c59146",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 1) Use the dictionary to store the returned dataframe from the get_sc_category_results function\n",
    "....\n",
    "category_style_control_results_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ba1a7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66369318",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1754627158675,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 240
    },
    "id": "S3e6Z7xsWR4x",
    "outputId": "db350b2e-5c8a-48eb-f540-49dd7eaa34cf"
   },
   "outputs": [],
   "source": [
    "fig = plot_rank_heatmap(category_style_control_results_df, title=\"With Style Control\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324de552",
   "metadata": {
    "id": "-H3bQx6hWR4y"
   },
   "source": [
    "Now let's look at the delta in rankings (shift of ranking) when we appy style control across all these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899dd84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 588,
     "status": "ok",
     "timestamp": 1754951617802,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "01aWwZE-WR4y",
    "outputId": "e9c05c88-a06e-46e3-f368-895b1f0aa989"
   },
   "outputs": [],
   "source": [
    "style_control_models = category_style_control_results_df[\n",
    "    category_style_control_results_df['Model'].isin(selected_models)\n",
    "].copy()\n",
    "\n",
    "baseline_models = category_results_df[\n",
    "    category_results_df['Model'].isin(selected_models)\n",
    "].copy()\n",
    "style_pivot = style_control_models.pivot(index='Model', columns='Category', values='Rank')\n",
    "baseline_pivot = baseline_models.pivot(index='Model', columns='Category', values='Rank')\n",
    "\n",
    "# Create mapping between style control and baseline categories\n",
    "category_mapping = {}\n",
    "for style_cat in style_pivot.columns:\n",
    "    baseline_cat = style_cat.replace(\" w/ Style Control\", \"\")\n",
    "    if baseline_cat in baseline_pivot.columns:\n",
    "        category_mapping[style_cat] = baseline_cat\n",
    "\n",
    "print(f\"Category mapping: {category_mapping}\")\n",
    "\n",
    "if not category_mapping:\n",
    "    print(\"No matching categories found between style control and baseline results\")\n",
    "else:\n",
    "    common_models = list(set(style_pivot.index) & set(baseline_pivot.index))\n",
    "    print(f\"Comparing {len(common_models)} models across {len(category_mapping)} categories\")\n",
    "    style_aligned = pd.DataFrame(index=common_models)\n",
    "    baseline_aligned = pd.DataFrame(index=common_models)\n",
    "    for style_cat, baseline_cat in category_mapping.items():\n",
    "        style_aligned[baseline_cat] = style_pivot.loc[common_models, style_cat]\n",
    "        baseline_aligned[baseline_cat] = baseline_pivot.loc[common_models, baseline_cat]\n",
    "    \n",
    "    # Compute rank deltas (baseline - style_control)\n",
    "    delta_data = baseline_aligned - style_aligned\n",
    "    if 'Overall' in delta_data.columns:\n",
    "        delta_data = delta_data.drop(columns=['Overall'])\n",
    "\n",
    "    heatmap_z = delta_data.values\n",
    "    heatmap_x = delta_data.columns.tolist()  # Categories\n",
    "    heatmap_y = delta_data.index.tolist()    # Models\n",
    "    avg_delta = delta_data.mean(axis=1).sort_values(ascending=False)\n",
    "    delta_data_sorted = delta_data.loc[avg_delta.index]\n",
    "    heatmap_z = delta_data_sorted.values\n",
    "    heatmap_y = delta_data_sorted.index.tolist()\n",
    "\n",
    "    annotations = []\n",
    "    for i, model in enumerate(heatmap_y):\n",
    "        for j, category in enumerate(heatmap_x):\n",
    "            value = heatmap_z[i][j]\n",
    "            if not pd.isna(value):\n",
    "                annotations.append(dict(x=category,y=model,text=f\"{int(value)}\",showarrow=False,font=dict(color=\"black\" if abs(int(value)) < 2 else \"white\", size=12)))\n",
    "    n_models = len(heatmap_y)\n",
    "    height = max(400, n_models * 30)\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(z=heatmap_z,x=heatmap_x,y=heatmap_y,colorscale=\"RdBu\",colorbar=dict(title=\"Rank Delta (Baseline - Style Control)\"),zmid=0))\n",
    "    fig.update_layout(\n",
    "        title=\"Delta in Model Rankings With Style Control (Category-Specific)\",\n",
    "        xaxis_title=\"Category\",\n",
    "        yaxis_title=\"Model\",\n",
    "        yaxis_autorange=\"reversed\",\n",
    "        annotations=annotations,\n",
    "        height=height\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8fc73",
   "metadata": {
    "id": "mLoHnkfYWR4y"
   },
   "source": [
    "Here we can quickly see which models are \"Style hacking\" - formatting their responses nicely but not necesarily being more capable models. It looks like gpt-4o-mini, llama-3.1-70b-instruct, llama-3.1-8b-instruct see a consistent drop in rankings while claude 3.5 sonnet, gemma-2-27b, and claude-3-haiku see a consistent rise in rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3c1cb",
   "metadata": {},
   "source": [
    "# **Question 9: Finding New Style Influences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f713f9",
   "metadata": {
    "id": "P66AjSVwWR4z"
   },
   "source": [
    "\n",
    "Earlier, we saw how model preference differs by looking at structural style features in model outputs (e.g., bold text count, header count, list count, token length).\n",
    "Now let's see if we can find new style features by inspecting the model responses to understand differences in models.\n",
    "Let's inpsect üîç the text ourselves for stylistic signals associated with wins. Your goal is to analyze assistant responses and identify phrases that differentiate winning from losing replies. This helps surface style features we might add to our ranking model later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dadc4",
   "metadata": {
    "id": "u09yRsQZWR4z"
   },
   "source": [
    "### Helper Functions\n",
    "Function to turn a conversations into plain text:\n",
    "*   convert_conversation_to_string\n",
    "*   convert_asst_conversation_to_string\n",
    "\n",
    "Function that compares two text with n-gram TF-IDF:\n",
    "*   tfidf_phrase_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737a1000",
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1754951655952,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "s2nTJLV9WR40"
   },
   "outputs": [],
   "source": [
    "def convert_conversation_to_string(conv):\n",
    "  ret = \"\"\n",
    "  for i in conv:\n",
    "    if i['role'] == 'user':\n",
    "      ret += \"User: \" + i['content'] + \"\\n\\n\"\n",
    "    else:\n",
    "      ret += \"Assistant: \" + i['content'] + \"\\n\\n\"\n",
    "  return ret\n",
    "\n",
    "def convert_asst_conversation_to_string(conv):\n",
    "  ret = \"\"\n",
    "  for i in conv:\n",
    "    if i['role'] == 'assistant':\n",
    "      ret += i['content'] + \"\\n\\n\"\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df112c4",
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1754951658812,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "FwVhzubyWR40"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def is_number_phrase(phrase):\n",
    "    # Remove phrases that are only numbers or contain only numbers and spaces/punctuation\n",
    "    # Also remove phrases that are just a number or start/end with a number\n",
    "    return bool(re.fullmatch(r\"[\\d\\s\\W]+\", phrase)) or bool(re.search(r\"\\b\\d+\\b\", phrase))\n",
    "\n",
    "def tfidf_phrase_diff(str_list_a, str_list_b, name_a=\"A\", name_b=\"B\", top_n=30, max_features=1000):\n",
    "    \"\"\"\n",
    "    Compute distinguishing ngram tfidf phrases between two sets of strings.\n",
    "    Returns two DataFrames: one for phrases more common in A, one for B.\n",
    "    \"\"\"\n",
    "    all_texts = str_list_a + str_list_b\n",
    "    labels = [name_a] * len(str_list_a) + [name_b] * len(str_list_b)\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', ngram_range=(2,4))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    n = len(str_list_a)\n",
    "    tfidf_a = tfidf_matrix[:n]\n",
    "    tfidf_b = tfidf_matrix[n:]\n",
    "    mean_a = np.asarray(tfidf_a.mean(axis=0)).flatten()\n",
    "    mean_b = np.asarray(tfidf_b.mean(axis=0)).flatten()\n",
    "    # Top phrases for A\n",
    "    a_scores = mean_a - mean_b\n",
    "    top_a_indices = np.argsort(a_scores)[::-1]\n",
    "    top_a_phrases = []\n",
    "    for i in top_a_indices:\n",
    "        phrase = feature_names[i]\n",
    "        if not is_number_phrase(phrase):\n",
    "            top_a_phrases.append((phrase, mean_a[i], mean_b[i]))\n",
    "        if len(top_a_phrases) >= top_n:\n",
    "            break\n",
    "    # Top phrases for B\n",
    "    b_scores = mean_b - mean_a\n",
    "    top_b_indices = np.argsort(b_scores)[::-1]\n",
    "    top_b_phrases = []\n",
    "    for i in top_b_indices:\n",
    "        phrase = feature_names[i]\n",
    "        if not is_number_phrase(phrase):\n",
    "            top_b_phrases.append((phrase, mean_b[i], mean_a[i]))\n",
    "        if len(top_b_phrases) >= top_n:\n",
    "            break\n",
    "    df_a = pd.DataFrame(top_a_phrases, columns=[\"phrase\", f\"{name_a}_tfidf\", f\"{name_b}_tfidf\"])\n",
    "    df_b = pd.DataFrame(top_b_phrases, columns=[\"phrase\", f\"{name_b}_tfidf\", f\"{name_a}_tfidf\"])\n",
    "    return df_a, df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de06ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1dNGoK9MYX7Izqa0k9_lOTfmpOHoj2RZI"
    },
    "executionInfo": {
     "elapsed": 12412,
     "status": "ok",
     "timestamp": 1754952070139,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "iwrbq-cjWR41",
    "outputId": "343c3ae9-9d0b-482a-899c-ab2f7aa60a1f"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model = \"llama-3.1-70b-instruct\"\n",
    "llama_battles = selected_battles_no_ties[\n",
    "    (selected_battles_no_ties['model_a'] == model) | (selected_battles_no_ties['model_b'] == model)\n",
    "].copy()\n",
    "\n",
    "llama_battles.loc[:, \"model_a_conversation_string\"] = llama_battles[\"conversation_a\"].apply(convert_asst_conversation_to_string)\n",
    "llama_battles.loc[:, \"model_b_conversation_string\"] = llama_battles[\"conversation_b\"].apply(convert_asst_conversation_to_string)\n",
    "llama_battles = llama_battles[llama_battles['language'] == 'English']\n",
    "\n",
    "str_list_a = llama_battles.apply(lambda x: x[\"model_a_conversation_string\"] if x[\"model_a\"] == model else x[\"model_b_conversation_string\"], axis=1).tolist()\n",
    "str_list_b = llama_battles.apply(lambda x: x[\"model_b_conversation_string\"] if x[\"model_a\"] == model else x[\"model_a_conversation_string\"], axis=1).tolist()\n",
    "# print(str_list_a)\n",
    "\n",
    "df_a, df_b = tfidf_phrase_diff(str_list_a, str_list_b, model, \"others\")\n",
    "\n",
    "print(f\"Top {model} phrases:\")\n",
    "# display(df_a)\n",
    "print(f\"Top others phrases:\")\n",
    "# display(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61960ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dcmdwwAM_Dvo"
   },
   "source": [
    "# **Question 9a: Key Phrases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a40a5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qn4owsN7_kjD"
   },
   "source": [
    "Let's analyze which phrases inherent in the text might be related to the win or lose of the battles. This is a similar idea to VibeCheck except (1) instead of comparing moel pairs we care comparing winning vs losing models and (2) instead of using LLM's to propose and validate vibes, we are going to be relying on keyword matching. \n",
    "\n",
    "Using the helper functions provided (`convert_asst_conversation_to_string`, `tfidf_phrase_diff`) and the reference example as guidance, implement a winning-vs-losing phrase analysis for assistant responses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task:** \n",
    "\n",
    "1. From selected_battles_no_ties, keep only rows in English.\n",
    "\n",
    "2. For each battle, extract assistant-only text using convert_asst_conversation_to_string.\n",
    "\n",
    "3. Build winning_responses: one assistant-only string for the winning side of each battle.\n",
    "\n",
    "4. Build losing_responses: one assistant-only string for the losing side of each battle.\n",
    "\n",
    "5. Use tfidf_phrase_diff to compare the two lists and construct df_win and df_lose. (we have set this up for you)\n",
    "\n",
    "6. Display the results to see the phrases most associated with winning vs. losing. (we have set this up for you)\n",
    "\n",
    "For your reference, a sample subset of outputs is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7edd5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Example: Top phrases in *winning* responses\n",
    "| phrase              | winning_tfidf | losing_tfidf |\n",
    "|---------------------|---------------|--------------|\n",
    "| let break           | 0.0127        | 0.0106       |\n",
    "| step step           | 0.0146        | 0.0126       |\\\n",
    "| ... |...       | ...     |\n",
    "\n",
    "### Example: Top phrases in *losing* responses\n",
    "| phrase                 | losing_tfidf | winning_tfidf |\n",
    "|------------------------|--------------|----------------|\n",
    "| let know               | 0.0278       | 0.0190         |\n",
    "| sorry assist           | 0.0054       | 0.0003         |\n",
    "| ...     | ...      | ...  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524c959",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 136950,
     "status": "ok",
     "timestamp": 1754953237487,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "rbBqPMiZWR41",
    "outputId": "c614b2d5-e369-49b2-facd-c6eb1fbf27ee",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TFIDF comparing winning responses to losing responses\n",
    "...\n",
    "# 1) Restrict to English Only and prepare assistant-only strings for A/B\n",
    "# Use 'convert_asst_conversation_to_string' helper function\n",
    "# Create a working copy to avoid modifying the main dataframe\n",
    "selected_battles_english = selected_battles_no_ties.copy()\n",
    "...\n",
    "\n",
    "# 2) Build lists of assistant-only winning/losing responses\n",
    "winning_responses = selected_battles_english.apply(...).tolist()\n",
    "\n",
    "losing_responses = selected_battles_english.apply(...).tolist()\n",
    "\n",
    "# 3) Compare phrases\n",
    "df_win, df_lose = tfidf_phrase_diff(winning_responses, losing_responses, \"winning\", \"losing\")\n",
    "# 4) Show results\n",
    "print(\"Top winning response phrases:\")\n",
    "# display(df_win)\n",
    "print(\"Top losing response phrases:\")\n",
    "# display(df_lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503926b9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b48111",
   "metadata": {},
   "source": [
    "## Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0cb37",
   "metadata": {
    "id": "64hdr6bMA6li"
   },
   "source": [
    "\n",
    "One thing we see from the TF-IDF results is that \"i'm sorry\" or \"i apologize\" appear often in losing models - when looking through these conversations you will see that these are often instances of **refusal**: where the model refuses to answer the question beacuse it violates ethical guidelines or is out of its domain of knowledge. Now let's turn this into a style feature to measure its impact on accuracy. \n",
    "\n",
    "\n",
    "Let's try capturing whether the assistant on side A which apologizes more than side B in a battle by calculating the normalized sorry_count_diff. Here we will just have a binary 1/0 for each conversation indicating if it contains or does not contain the word \"sorry\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e33e80",
   "metadata": {
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1754953063300,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "kfTRLsXsWR42"
   },
   "outputs": [],
   "source": [
    "def count_phrase_diff(row, phrase=[\"step by step\"]):\n",
    "    step_by_step_a = False\n",
    "    step_by_step_b = False\n",
    "    for i in row[\"conversation_a\"]:\n",
    "        if i[\"role\"] == \"assistant\" and any([p in i[\"content\"].lower().replace(\"-\", \" \") for p in phrase]):\n",
    "            step_by_step_a = True\n",
    "    for i in row[\"conversation_b\"]:\n",
    "        if i[\"role\"] == \"assistant\" and any([p in i[\"content\"].lower().replace(\"-\", \" \") for p in phrase]):\n",
    "            step_by_step_b = True\n",
    "    return int(step_by_step_a) - int(step_by_step_b)\n",
    "\n",
    "\n",
    "selected_battles_no_ties.loc[:, \"refusal_count\"] = selected_battles_no_ties.apply(\n",
    "    lambda row: count_phrase_diff(row, [\"sorry\", \"apologize\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e32a31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ys5ga1dCW2SM"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# **Question 9b: Discover Some Immaculate Vibes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b5580",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** \n",
    "Now, just like the `refusal_count` feature we created above, implement new functions that can extract any stylistic features from the conversations.  \n",
    "- Define a function that computes the normalized difference for a feature of your choice (e.g., presence of certain phrases, punctuation, formatting). You can check multiple different phrases if you want, they just to have a common \"theme\" - similar to the last problem of the previous part of this homework.   \n",
    "- Apply this function to each row in the dataset.  \n",
    "- Store the results in a new column of `selected_battles_no_ties[\"YOUR_FEATURE\"]`. \n",
    "- Plot the change in ranking and style coefficients and the existing style features along with your custom feature. **To get full points, your feature need to get a higher coefficient (Average Score) than `style_header_count`**. It is okay if the confidence intervasls overlap. \n",
    "- You cannot use a feature already explored or anything similar (e.g. you can't have an \"I refuse\" style feature or a word count style feature). \n",
    "\n",
    "This is open ended, you don't need to use the features you found above, get creative with it! Heck, you can even throw response pairs into your LLM of choice and ask it to come up with differences just like VibeCheck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed5c5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 14908,
     "status": "ok",
     "timestamp": 1754953532614,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "i386MD7lArDR",
    "outputId": "1c2be5a3-4c95-41c2-9a5a-fb97107c0a1e",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Design your own stylistic feature(s) that compare model A vs B on each row.\n",
    "# Keep it simple (boolean presence, counts, or normalized differences) or get creative.\n",
    "\n",
    "# Remember might want to normalize difference, if you compute counts\n",
    "\n",
    "# TODO: Define your feature function\n",
    "def YOUR_FUNC(row):\n",
    "    #Input: a row with 'conversation_a' and 'conversation_b' (each is a list of {role, content} dicts).\n",
    "    #Output: a single numeric feature comparing A vs B (e.g., -1/0/1, count diff, or normalized diff).\n",
    "    #Hint: You probably want to inspect only assistant messages.\n",
    "    # Example (placeholder): return 0\n",
    "    return ...\n",
    "\n",
    "# Avoid warnings by making copy\n",
    "selected_battles_no_ties = selected_battles_no_ties.copy()\n",
    "\n",
    "# TODO: Apply your function to create a new column\n",
    "# selected_battles_no_ties.loc[:, \"YOUR_FEATURE\"] = selected_battles_no_ties.apply(YOUR_FUNC, axis=1)\n",
    "\n",
    "# Choose which style features to control for in ranking.\n",
    "# Start from this list and add yours below.\n",
    "style_feature_cols = [\n",
    "    \"style_bold_count\",\n",
    "    \"style_header_count\",\n",
    "    \"style_list_count\",\n",
    "    \"style_sum_assistant_tokens\",\n",
    "    \"refusal_count\",\n",
    "    # \"YOUR_FEATURE\",   # <- uncomment after you create it\n",
    "\n",
    "combined_results_df_new = get_sc_category_results(selected_battles_no_ties,\n",
    "                                 selected_models, \n",
    "                                 category_name=\"Overall w/ Style Control\", \n",
    "                                 n_bootstrap=25, \n",
    "                                 style_features=style_feature_cols)\n",
    "\n",
    "fig = plot_rank_heatmap(combined_results_df_new, title=\"With and Without Style Control (New Features)\", selected_models=selected_models)\n",
    "fig.show()\n",
    "\n",
    "# Create and display the plot\n",
    "fig = plot_style_features(combined_results_df_new, selected_models)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6da8d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Memory cleanup: drop conversation columns (saves ~400-600 MB)\n",
    "# All features extracted from conversations have been computed\n",
    "selected_battles_no_ties = selected_battles_no_ties.drop(columns=['conversation_a', 'conversation_b'])\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ef53f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# **Question 9c: Reflection**\n",
    "\n",
    "**Answer the following questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ff1e1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "9c-1. Why did you decide to use the stylistic feature that you have implemented for Q9b? What ranking changes across models did you see when looking at the new style features you created? Why do you think that is the case?\n",
    "\n",
    "9c-2. Why might some models overuse or underuse stylistic markers (e.g., exclamation points, apologies, or explicit reasoning phrases), and how could that influence rankings?\n",
    "\n",
    "9c-3. How does including these stylistic features help control for length or formatting effects when building leaderboards?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231c9c3",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER 9c-1: Replace This\n",
    "YOUR ANSWER 9c-2: Replace This\n",
    "YOUR ANSWER 9c-3: Replace This\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc9d11c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00962e2c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12826425",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b7eee",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test, y_test = turn_into_features(selected_battles_no_ties, selected_models)\n>>> expected_rows = 2 * len(selected_battles_no_ties)\n>>> expected_cols = len(selected_models)\n>>> assert X_test.shape == (expected_rows, expected_cols), f'shape of X does not match ({expected_rows}, {expected_cols})'\n>>> assert y_test.shape == (expected_rows,), f'shape of y does not match ({expected_rows},)'\n>>> assert len(X) == 2 * len(selected_battles_no_ties), 'Each battle should generate exactly 2 examples'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert results_df.shape == (20, 2), 'result_df shape does not match (20,2)'\n>>> assert list(results_df.columns) == ['Model', 'Score'], 'results_df does not match column: [Model, Score]'\n>>> scores = results_df['Score'].to_numpy()\n>>> assert np.all(np.diff(scores) <= 1e-12), 'Scores must be sorted in descending order.'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_get_bootstrapped_score_outputs(X, y, selected_models):\n...     \"\"\"\n...       - Run get_bootstrapped_score on provided (X, y, selected_models).\n...       - Check shapes of outputs.\n...       - Check that results_df has expected columns and type.\n...       - Check that confidence intervals are valid (LB <= UB).\n...     \"\"\"\n...     np.random.seed(189)\n...     results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=25)\n...     assert isinstance(results_df, pd.DataFrame), 'results_df should be a DataFrame.'\n...     for c in ['Model', 'Average Score', 'Lower Bound', 'Upper Bound', 'Category']:\n...         assert c in results_df.columns, f'Missing column: {c}'\n...     n_models = len(selected_models)\n...     assert results_df.shape[0] == n_models, f'Expected {n_models} rows in results_df, got {results_df.shape[0]}'\n...     assert mean_scores.shape == (n_models,), f'mean_scores must be shape ({n_models},)'\n...     assert confidence_intervals.shape == (2, n_models), f'confidence_intervals must be (2, {n_models})'\n...     lb, ub = confidence_intervals\n...     assert (lb <= ub).all(), 'Every lower bound must be <= upper bound.'\n...     assert ((lb <= mean_scores) & (mean_scores <= ub)).all(), 'Each mean score should lie within its CI.'\n>>> test_get_bootstrapped_score_outputs(X, y, selected_models)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'results_df' in globals(), 'Expected a DataFrame named `results_df`.'\n>>> required_cols = {'Model', 'Lower Bound', 'Upper Bound', 'Rank', 'Category'}\n>>> assert required_cols.issubset(results_df.columns), f'Missing columns: {required_cols - set(results_df.columns)}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6a": {
     "name": "q6a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_get_category_results(selected_battles_no_ties, selected_models):\n...     selected_battles_no_ties_test = selected_battles_no_ties.copy()\n...     selected_battles_no_ties_test.loc[:, 'creative'] = selected_battles_no_ties_test['category_tag'].apply(lambda x: x['criteria_v0.1']['creativity'])\n...     selected_battles_no_ties_test.loc[:, 'technical_accuracy'] = selected_battles_no_ties_test['category_tag'].apply(lambda x: x['criteria_v0.1']['technical_accuracy'])\n...     selected_battles_no_ties_test.loc[:, 'instruction_following'] = selected_battles_no_ties_test['category_tag'].apply(lambda x: x['if_v0.1']['if'])\n...     selected_battles_no_ties_test.loc[:, 'math'] = selected_battles_no_ties_test['category_tag'].apply(lambda x: x['math_v0.1']['math'])\n...     test_filters = {'english': selected_battles_no_ties_test['language'] == 'English', 'coding': selected_battles_no_ties_test['is_code'] == True, 'creative': selected_battles_no_ties_test['creative'] == True, 'math': selected_battles_no_ties_test['math'] == True}\n...     required_cols = {'Model', 'Average Score', 'Lower Bound', 'Upper Bound', 'Rank', 'Category'}\n...     for name, mask in test_filters.items():\n...         res = get_category_results(selected_battles_no_ties_test, mask, selected_models, category_name=name, n_bootstrap=5)\n...         assert isinstance(res, pd.DataFrame), f'{name} result must be a DataFrame.'\n...         assert required_cols.issubset(res.columns), f'{name} result missing columns.'\n...         assert len(res) > 0, f'{name} result should not be empty.'\n...         assert set(res['Model']).issubset(set(selected_models)), f'{name} result has unexpected models.'\n...         assert res['Rank'].is_monotonic_increasing, f'{name} result not sorted by Rank.'\n>>> test_get_category_results(selected_battles_no_ties, selected_models)\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6b": {
     "name": "q6b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> required = ['creative', 'technical_accuracy', 'instruction_following', 'math']\n>>> for col in required:\n...     assert col in selected_battles_no_ties.columns, f'Missing column {col}'\n>>> for col in required:\n...     ser = selected_battles_no_ties[col]\n...     assert ser.notna().all(), f'{col} contains NaNs'\n...     assert (ser.astype(bool) == ser).all(), f'{col} should be boolean-valued'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6c": {
     "name": "q6c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_keys = {'english', 'coding', 'creative', 'instruction_following', 'math', 'technical_accuracy'}\n>>> assert set(category_filters.keys()) == expected_keys, 'category_filters has incorrect keys.'\n>>> for name, mask in category_filters.items():\n...     assert isinstance(mask, pd.Series), f\"Mask '{name}' must be a pandas Series.\"\n...     assert len(mask) == len(selected_battles_no_ties), f\"Mask '{name}' length must match the DataFrame.\"\n...     assert mask.notna().all(), f\"Mask '{name}' contains NaNs; expected strictly boolean.\"\n...     assert (mask.astype(bool) == mask).all(), f\"Mask '{name}' must be boolean-valued.\"\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6d": {
     "name": "q6d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_category_results_df(category_results_df, category_filters):\n...     \"\"\"Simple test for tidy format category_results_df\"\"\"\n...     assert set(category_results_df.columns) >= {'Model', 'Rank', 'Category'}, 'Missing required columns'\n...     expected_categories = {'Overall'} | set(category_filters.keys())\n...     actual_categories = set(category_results_df['Category'].unique())\n...     assert expected_categories == actual_categories, f'Category mismatch: expected {expected_categories}, got {actual_categories}'\n...     is_sorted = (category_results_df[['Rank', 'Category']] == category_results_df[['Rank', 'Category']].sort_values(['Rank', 'Category'])).all().all()\n...     assert is_sorted, 'DataFrame should be sorted by Rank, then Category'\n...     num_models = 20\n...     assert len(category_results_df) == num_models * (len(category_filters) + 1), f'category_results_df should have the same length as num_models * (len(category_filters) + 1) (current length: {len(category_results_df)}, expected length: {num_models * (len(category_filters) + 1)})'\n>>> test_category_results_df(category_results_df, category_filters)\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> conv_one_assistant = {'conversation': [{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': 'Hi there!'}]}\n>>> assert calculate_response_length(conv_one_assistant) == 3, 'error1: the joining did not work properly for one assistant'\n>>> conv_special = {'conversation': [{'role': 'assistant', 'content': 'Here is <|endoftext|> special'}]}\n>>> assert calculate_response_length(conv_special) == 10, 'the special token raises an error'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7b": {
     "name": "q7b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(per_model_battles.columns) == ['conversation', 'model', 'response_length'], 'does not have expected columns: [conversation, model, response_length]'\n>>> assert per_model_battles.shape[0] == 2 * len(selected_battles_no_ties), 'per_model_battles.shape[0] does not match expected (twice of original)'\n>>> assert (per_model_battles['response_length'] >= 0).all(), 'negative or undefined response_length'\n>>> assert np.issubdtype(per_model_battles['response_length'].dtype, np.integer), 'dtype or response_length has to be np.integer'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8a": {
     "name": "q8a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> min_expected_cols = 21\n>>> assert selected_battles_no_ties.shape[1] >= min_expected_cols, f'Expected at least {min_expected_cols} columns, got {selected_battles_no_ties.shape[1]}'\n>>> expected_cols = {'style_bold_count', 'style_header_count', 'style_list_count', 'style_sum_assistant_tokens'}\n>>> missing = expected_cols - set(selected_battles_no_ties.columns)\n>>> assert not missing, f'Missing expected style feature columns: {missing}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8b": {
     "name": "q8b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_rows = 2 * len(selected_battles_no_ties)\n>>> expected_num_cols = 4 + len(style_feature_cols)\n>>> assert pairwise_feature_df.shape == (expected_rows, expected_num_cols), f'Expected ({expected_rows}, {expected_num_cols}), got {pairwise_feature_df.shape}'\n>>> expected_cols = {'question_id', 'X', 'y', 'direction', *style_feature_cols}\n>>> assert set(pairwise_feature_df.columns) == expected_cols, 'Unexpected columns in output.'\n>>> for d in pairwise_feature_df['direction'].unique():\n...     assert d in ('A->B', 'B->A'), f'Unexpected direction: {d}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8c": {
     "name": "q8c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(results_df_style_control.columns) == ['Model', 'Category', 'Average Score', 'Lower Bound', 'Upper Bound', 'Rank'], 'results_df_style_control columns incorrect.'\n>>> assert list(combined_results_df.columns) == ['Model', 'Category', 'Average Score', 'Lower Bound', 'Upper Bound', 'Rank'], 'combined_results_df columns incorrect.'\n>>> n_models = 20\n>>> models = list(results_df['Model'].unique()) + ['style_sum_assistant_tokens', 'style_bold_count', 'style_header_count', 'style_list_count']\n>>> assert len(combined_results_df['Model'].unique()) == n_models + 4, f\"combined_results_df should contain exactly the models from results_df. ({n_models} models expected, got {len(combined_results_df['Model'].unique())})\"\n>>> assert len(combined_results_df) == n_models * 2 + 4, f'combined_results_df should contain {n_models * 2 + 4} rows, got {len(combined_results_df)}'\n>>> style_features = ['style_sum_assistant_tokens', 'style_bold_count', 'style_header_count', 'style_list_count']\n>>> assert np.all(combined_results_df.loc[combined_results_df['Model'].isin(style_features), 'Rank'].values.astype(int) == -1), 'Rank should be -1 for style features'\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8d": {
     "name": "q8d",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def sanity_check_tidy(selected_models, category_filters, category_style_control_results_df):\n...     \"\"\"\n...     Sanity check for tidy format category_style_control_results_df\n...     Expected structure: ['Model', 'Rank', 'Category'] with multiple rows per model\n...     \"\"\"\n...     required_cols = ['Model', 'Rank', 'Category']\n...     for col in required_cols:\n...         assert col in category_style_control_results_df.columns, f\"Missing required column '{col}'\"\n...     expected_categories = set([f'{cat} w/ Style Control' for cat in category_filters.keys()])\n...     if 'Overall w/ Style Control' in category_style_control_results_df['Category'].values:\n...         expected_categories.add('Overall w/ Style Control')\n...     actual_categories = set(category_style_control_results_df['Category'].unique())\n...     assert expected_categories.issubset(actual_categories), f'Missing categories: {expected_categories - actual_categories}'\n...     actual_models = category_style_control_results_df[category_style_control_results_df['Rank'] != -1]['Model'].unique()\n...     expected_models = set(selected_models)\n...     actual_models_set = set(actual_models)\n...     assert expected_models.issubset(actual_models_set), f'Missing models: {expected_models - actual_models_set}'\n...     model_category_counts = category_style_control_results_df[category_style_control_results_df['Rank'] != -1].groupby(['Model', 'Category']).size()\n...     assert (model_category_counts == 1).all(), 'Each model should appear exactly once per category'\n...     actual_model_data = category_style_control_results_df[category_style_control_results_df['Rank'] != -1]\n...     assert (actual_model_data['Rank'] > 0).all(), 'All actual model ranks should be positive'\n...     assert actual_model_data['Rank'].dtype in ['int64', 'int32'], 'Ranks should be integers'\n...     style_features = category_style_control_results_df[~category_style_control_results_df['Model'].isin(selected_models)]\n...     if len(style_features) > 0:\n...         assert (style_features['Rank'] == -1).all(), 'Style features should have Rank = -1'\n...     expected_sorted = category_style_control_results_df.sort_values(['Rank', 'Category'], ascending=[True, True]).reset_index(drop=True)\n...     actual_sorted = category_style_control_results_df.reset_index(drop=True)\n...     assert expected_sorted[['Model', 'Rank', 'Category']].equals(actual_sorted[['Model', 'Rank', 'Category']]), 'DataFrame should be sorted by Rank first, then Category'\n>>> sanity_check_tidy(selected_models, category_filters, category_style_control_results_df)\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9a": {
     "name": "q9a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_winning_losing_phrase_tfidf(selected_battles_no_ties, winning_responses, losing_responses, df_win, df_lose):\n...     english_only = selected_battles_no_ties[selected_battles_no_ties['language'] == 'English']\n...     assert len(winning_responses) == len(english_only), 'winning_responses length should match number of English battles.'\n...     assert len(losing_responses) == len(english_only), 'losing_responses length should match number of English battles.'\n...     assert all((isinstance(s, str) for s in winning_responses)), 'winning_responses must be strings.'\n...     assert all((isinstance(s, str) for s in losing_responses)), 'losing_responses must be strings.'\n...     if len(winning_responses) > 0:\n...         assert any((len(s.strip()) > 0 for s in winning_responses)), 'winning_responses seem empty.'\n...         assert any((len(s.strip()) > 0 for s in losing_responses)), 'losing_responses seem empty.'\n...     assert list(df_win.columns) == ['phrase', 'winning_tfidf', 'losing_tfidf'], \"df_win must have columns ['phrase','winning_tfidf','losing_tfidf'].\"\n...     assert list(df_lose.columns) == ['phrase', 'losing_tfidf', 'winning_tfidf'], \"df_lose must have columns ['phrase','losing_tfidf','winning_tfidf'].\"\n...     assert df_win['phrase'].map(lambda x: isinstance(x, str)).all(), 'df_win phrases must be strings.'\n...     assert df_lose['phrase'].map(lambda x: isinstance(x, str)).all(), 'df_lose phrases must be strings.'\n...     for col in ['winning_tfidf', 'losing_tfidf']:\n...         assert pd.to_numeric(df_win[col], errors='coerce').notna().all(), f'df_win.{col} must be numeric.'\n...         assert (df_win[col] >= 0).all(), f'df_win.{col} must be non-negative.'\n...     for col in ['losing_tfidf', 'winning_tfidf']:\n...         assert pd.to_numeric(df_lose[col], errors='coerce').notna().all(), f'df_lose.{col} must be numeric.'\n...         assert (df_lose[col] >= 0).all(), f'df_lose.{col} must be non-negative.'\n...     a, b = tfidf_phrase_diff(winning_responses, losing_responses, 'winning', 'losing', top_n=10, max_features=500)\n...     assert list(a.columns) == list(df_win.columns), 'Re-run df_win columns mismatch.'\n...     assert list(b.columns) == list(df_lose.columns), 'Re-run df_lose columns mismatch.'\n>>> test_winning_losing_phrase_tfidf(selected_battles_no_ties, winning_responses, losing_responses, df_win, df_lose)\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
