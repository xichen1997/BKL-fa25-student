{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw2\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"arena_warmup.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m0B0ZckWWNr"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Homework 02 – Welcome to the Arena (Warmup)</h1>\n",
    "\n",
    "CS 189, Fall 2025\n",
    "\n",
    "In this homework you will get more experience with logistic regression in two very different settings: creating leaderboards and predicting model responses.\n",
    "\n",
    "We will be taking real data from [LMArena](https://lmarena.ai/), a popular platform for crowsourcing evaluations of large language models and recreating their leaderboards, with a few fun extra steps along the way.\n",
    "\n",
    "The chats can be viewed interactively by accessing [ChatBot-Arena-Viewer](https://huggingface.co/spaces/BerkeleyML/Chatbot-Arena-Viewer) through hugging face. Much of the first half of this homework was first written by Prof Gonzalez back when his students first started the project, and now LMArena is a standard evaluation for large language models and turned into a company! Don't let anyone tell you logistic regression isn't valuable, it's worth at least $600 Million.\n",
    "\n",
    "---\n",
    "\n",
    "## Due Date: Saturday, October 4, 11:59 PM\n",
    "\n",
    "This assignment is due on **Saturday, October 4, at 11:59 PM**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "### Submission Tips\n",
    "- **Plan ahead**: We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early**: If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "\n",
    "<!-- ---\n",
    "\n",
    "### Assignment Overview\n",
    "\n",
    "This notebook contains a series of tasks designed to help you practice and apply concept you learned in class to a very practical task - Ranking LLM Models. You will complete all the TODOs in the notebook, which include both coding and written response questions. Some tasks are open-ended, which allows you to explore and experiment with different approaches.\n",
    "\n",
    "Question 1: Explore the data\n",
    "\n",
    "Question 2 : Analyze the prompts and their effects\n",
    "\n",
    "Question 3: Explore model ranking and win rates\n",
    "\n",
    "Question 4: Think about learning model strengths\n",
    "\n",
    "Question 5: Generate confidence intervals\n",
    "\n",
    "--- -->\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "1. Learn how to evaluate large language models (LLMs) using pairwise comparison data from LMArena\n",
    "2. Analyze battle distributions and compute win rates\n",
    "3. Find subsets of prompts which result in changes to the leaderboard\n",
    "    \n",
    "---\n",
    "\n",
    "### Collaboration Policy\n",
    "You are encouraged to discuss high-level concepts with your peers. However:\n",
    "- All submitted work must be written in your own words and code.\n",
    "- Do not share or copy solutions directly.\n",
    "- List any collaborators (students you worked with) in the line below. Include their name and SID:\n",
    "\n",
    "**Your Collaborators**: **TODO**\n",
    "\n",
    "### AI Tools Usage Disclosure\n",
    "We allow the use of AI tools (e.g., ChatGPT, Copilot) **only as support**, not as a replacement for your own reasoning. To ensure transparency, you must acknowledge any use of AI tools.\n",
    "\n",
    "Please answer with one of the following options:\n",
    "- **A) I did not use any AI tools for this homework.**\n",
    "- **B) I used AI tools in the following way(s):**  \n",
    "  (describe briefly, e.g., “Used ChatGPT to get hints for debugging a NumPy indexing error”)\n",
    "\n",
    "\n",
    "**Your Answer**: **TODO**\n",
    "    \n",
    "---\n",
    "\n",
    "### Grading Breakdown\n",
    "\n",
    "<!-- <div align=\"center\"> -->\n",
    "\n",
    "\n",
    "| Question  | Manual Grading? | Points |\n",
    "| --------- | --------------- | ------ |\n",
    "| q1a       | No              | 1      |\n",
    "| q1b       | No              | 1      |\n",
    "| q1c       | Yes             | 2      |\n",
    "| q2a       | No              | 2      |\n",
    "| q2b       | Yes             | 1      |\n",
    "| q2c       | Yes             | 3      |\n",
    "| q3a       | Yes             | 2      |\n",
    "| q3b       | No              | 2      |\n",
    "| q3c       | Yes             | 5      |\n",
    "| q3d       | Yes             | 2      |\n",
    "| q3e       | Yes             | 1      |\n",
    "| q3eFRQ    | Yes             | 3      |\n",
    "| **Total** |                 | **26** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3648,
     "status": "ok",
     "timestamp": 1754953676761,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "hdjcns4uWWNu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "#set fixed seed of 189\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJVRMez_WWNv"
   },
   "source": [
    "<div style=\"display: flex; align-items: flex-start;\">\n",
    "  <div style=\"flex: 1; padding-right: 24px;\">\n",
    "    <img src=\"https://i.imgur.com/tbrkWVX.png\" alt=\"LMArena\" style=\"width: 100%; max-width: 350px; display: block; margin: 0 auto;\">\n",
    "  </div>\n",
    "  <div style=\"flex: 2;\">\n",
    "    <h1 style=\"font-size: 2em; font-weight: bold; margin-bottom: 0.5em;\">What is LMArena?</h1>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "LMArena (previously known as Chatbot Arena) is a platform that evaluates generative models (e.g. chatbots or image generation models) through anonymous, crowd-sourced pairwise comparisons. Users enter prompts for two anonymous models to respond to and vote on the model that gave the better response, in which the model's identities are revealed (shown below). Users can also choose models to test themselves, but for the purposes of this homework we will only focus on the anonymous side-by-side comparisons, which we call **\"battles\"** -  since those are what are used to calcuate the leaderboard.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.imgur.com/rgv0jCb.png\" alt=\"LMArena\" style=\"display: block; margin-left: auto; margin-right: auto; width: 60%;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "In this homework we will investigate what these battles look like, how we can use these pairwise comparisons to get a leaderboard, and how we can find certain features of model responses that have an influence on preference.\n",
    "\n",
    "Although it is not required for this notebook, the [Chatbot Arena paper](https://arxiv.org/abs/2403.04132) can provide good intuition on how to answer the free response questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCKqMaQUWWNw"
   },
   "source": [
    "First, let's load a set of publicly released arena battles from [Hugging Face](https://huggingface.co/datasets/lmarena-ai/arena-human-preference-100k) — a popular website for sharing machine learning datasets and models. They also have a ton of great visualization tools with [Gradio](https://www.gradio.app/docs), which you will also get some experience with in this homework.\n",
    "\n",
    "\n",
    "**Note**: Before you get started with the homework, we reccomend you make a [huggingface account](https://huggingface.co/welcome) to play around with creating or modifying our data visualization apps! It is also a great general hub for downloading the majority of popular datasets in machine learning. Once you make the account, generate the token for login [huggingface token]\n",
    "(https://huggingface.co/docs/hub/security-tokens). This should be similar to how you generate a token for your git account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To log into huggingface, uncomment the line below and re-run the cell\n",
    "\n",
    "#! pip install ipywidgets\n",
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "27c71a72d88c4255ad9022229e400c06",
      "0cd940b9bfe04df18b056fd9610afbdc",
      "80a75b638ee5470f93be0b07cf5d09a7",
      "38dfd75cb0cb445cb033d8aa789c7f5e",
      "c492d0cf60934467a322d5cf7ee8c0bc",
      "5de644ffafb14b1da4b8d079986fd0a1",
      "d373d293000948ae82a60bc9a6ff67e6",
      "5e307e3cbd8147fc98fd305f554fbda1",
      "8c9028c345b040d58ebc180d0af96e22",
      "85135e416b8547359b05d425e0bcbda6",
      "0a975a38eaea4cec8ef94f6cc98b8968",
      "e73fc4365afd47e78df2dc19cdf6a882",
      "8dc1694d8be74bd68552192429fab064",
      "59ca363982484e709177a28f21c2fb67",
      "c6b6aa8c10c248f4996c6180e1180136",
      "4e18deedc5ac45308ab22a3f6f7abe0a",
      "31f3fe8fd37e4f1f8d906ee2d75af355",
      "f23a5fb259ae431cae4f04380d95180a",
      "da8c11d5d1474bdfa68aa39d84d3ecb2",
      "9139c4067fe3490898349652e6831d58",
      "2cc14c24261e40859c2161dfbac1b6d0",
      "8dc3052c99284190bb2584b1c560d2c5",
      "3f6dc15403824376beabb1b49cd81922",
      "216c481d29ed4949b534c26e5f811d36",
      "063db024f08c45c08ea4a995125fa265",
      "871c76f3d8ea471f8b458c952346191b",
      "c5db793651d540369832d01eff5a6f4b",
      "532c51c9b8564b35a2c667c29261b15f",
      "f13da946d4b54d0da90eeca0af76ce56",
      "79fdb1684a5f4a84803e796dc0d63c93",
      "196f8ecf0b5d48bb84b6f47b5c8f58bc",
      "ce86bc9028de4bc0bd9ad6e97fe200c2",
      "39122ccd2b3e46d2bad79228c2aaf702"
     ]
    },
    "executionInfo": {
     "elapsed": 33605,
     "status": "ok",
     "timestamp": 1754953717461,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "AEnadibuWWNx",
    "outputId": "9b397ff8-bc06-4731-8ca4-c9cd24f3d083"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset (this will take a few minutes to download)\n",
    "# if you don't have an account it may throw a warning to create a HF_TOKEN but you should still have access to the dataset if you skip this\n",
    "ds = load_dataset(\"lmarena-ai/arena-human-preference-100k\")\n",
    "battles = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_W3ILuFWWNx"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "Now let's look at the format of this data.\n",
    "\n",
    "Printing out the first row we see there are many fields with the most important being:\n",
    "\n",
    "<ul class=\"cal cal-list\">\n",
    "  <li><strong>question_id (str)</strong> - the ID of that battle</li>\n",
    "  <li><strong>model_a, model_b (str)</strong> - the models which participated in this battle</li>\n",
    "  <li><strong>winner (str)</strong> - which response the user preferred: can be <span class=\"cal-code-inline\">model_a</span>, <span class=\"cal-code-inline\">model_b</span>, <span class=\"cal-code-inline\">tie</span>, and <span class=\"cal-code-inline\">tie (bothbad)</span></li>\n",
    "  <li><strong>conversation_a (dict)</strong> - the conversation between the user and model a</li>\n",
    "  <li><strong>conversation_b (dict)</strong> - the conversation between the user and model b (note that the user turns in conversation_a and conversation_b are the same since this is a side by side comparison)</li>\n",
    "  <li><strong>turn (int)</strong> - number of turns in the conversation (1 turn means the user asked 1 question, 2 turns means the user asked a question, got an answer, then asked another question, got the response, then voted)</li>\n",
    "  <li><strong>language (str)</strong> - the language of the user prompt</li>\n",
    "</ul>\n",
    "\n",
    "We also have some other columns that may be useful for us later:\n",
    "\n",
    "<ul class=\"cal cal-list\">\n",
    "  <li><strong>is_code (bool)</strong> - either the prompt, the response, or both contains code</li>\n",
    "  <li><strong>is_refusal (bool)</strong> - one of the models refused to answer (usually this is because the model thinks it would be unethical to answer)</li>\n",
    "  <li><strong>dedup_tag (dict)</strong> - indicates whether the prompt appears very often (high_frequency) and if it does, whether it will be sampled (subsampled). We subsample these high frequency prompts so that common questions don't overly influence the leaderboard.</li>\n",
    "  <li><strong>category_tag (dict)</strong> - tags for question type (e.g. math and instruction following). These are assigned via an LLM labeler, more details on what the categories are in this <a href=\"https://blog.lmarena.ai/blog/2024/arena-category/\">blog post</a> (the criteria tags correspond to the hard prompts category described in the blog post).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1754953717534,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "29C-vOk1WWNy",
    "outputId": "0a2a96ca-6cd5-4f24-db06-d555dcaf3116"
   },
   "outputs": [],
   "source": [
    "print(\"Columns:\\n*\", \"\\n* \".join(battles.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1754953717689,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "6hN8yfWBWWNz",
    "outputId": "e3b51fdd-ef8c-4dce-d63e-c2895626d899"
   },
   "outputs": [],
   "source": [
    "battles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1754953717713,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "WOwS8T19WWNz",
    "outputId": "fe2411a6-df11-4410-9a7c-70d7293d5c5a"
   },
   "outputs": [],
   "source": [
    "example = battles.iloc[4]\n",
    "print(f\"Conversation A (model = {example['model_a']}):\")\n",
    "print(example['conversation_a'])\n",
    "print(f\"Conversation B (model = {example['model_b']}):\")\n",
    "print(example['conversation_b'])\n",
    "print(\"Category Tag:\")\n",
    "print(example['category_tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49X_FMb0WWN0"
   },
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "Before we get into leaderboard calculation, let's first conduct some basic exploratory analysis to highlight a few key properties and caveates with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 12343,
     "status": "ok",
     "timestamp": 1754953730155,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "cKUrjRaAWWN1",
    "outputId": "84787145-b090-4564-d267-84dc17743420"
   },
   "outputs": [],
   "source": [
    "battles.winner.hist(title=\"Counts of Battle Outcomes\", text_auto=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75fiCH5_df9K"
   },
   "source": [
    "#### NOTE: Notice how LMArena has two types of ties: tie (both bad) and just tie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca1R2-QsWWN1"
   },
   "source": [
    "### Battle Counts\n",
    "\n",
    "We see that certain models participate in more battles. This is due to two reasons:\n",
    "1. Several different matching and sampling algorithms were used. LMArena employs weighted sampling methods, which assign greater weights to better models.\n",
    "2. Since models are added to the arena when they come out, some models have been on the arena for many months while others have only been on for a few weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1754953730271,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "QMd80i0PWWN1",
    "outputId": "76a80035-f393-4244-d413-d8e9fff29646"
   },
   "outputs": [],
   "source": [
    "fig = pd.concat([battles[\"model_a\"], battles[\"model_b\"]]).value_counts().plot.bar(title=\"Battle Count for Each Model\", text_auto=True)\n",
    "fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Battle Count\", height=400, showlegend=False)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question 1: Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "TBpgU0o-ZIym"
   },
   "source": [
    "## **Question 1a**\n",
    "Since it can be hard to reason over that many models, we want to look at the top 20 models by battle count.\n",
    "\n",
    "**Task:** \n",
    "Return the a list of top 20 models by battle count for both `model_a` and `model_b` combined, SORTED by the battle count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1754953730381,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "MeujRZ-1WWN2",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "selected_models = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kuxNeJCKacnT"
   },
   "source": [
    "## **Question 1b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UEa3M8zQakWB"
   },
   "source": [
    "**Task:** \n",
    "Now, let's filter out and select the battles that are between the top 20 models we got from question 1a. Fill in the `subselect_battles` function to return the battles dataframe contatining only the selected models and the battles dataframe containing the selected models with ties removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uKeHh9yWdwg0"
   },
   "source": [
    "**Hint:** You may find it helpful to use a boolean array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754953789460,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "bP3Lb6U4dtXQ",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Set\n",
    "import pandas as pd\n",
    "\n",
    "def subselect_battles(\n",
    "    battles: pd.DataFrame, \n",
    "    selected_models: Set[str]\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Filters the battles DataFrame to only include battles between the selected models.\n",
    "    # Returns a tuple of the dataframe filtered by models and the dataframe filtered my models with ties removed.\n",
    "    ...\n",
    "    return selected_battles, selected_battles_no_ties\n",
    "  \n",
    "selected_battles, selected_battles_no_ties = subselect_battles(battles, selected_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to visualize how often each pair of models battled each other, similarly to the heatmap on the right side of **Figure 2** in the [LMArena paper](https://arxiv.org/pdf/2403.04132). We’ll sort models by their total number of battles and show the top **N** (default 30). Each cell of the heatmap shows the battle counts for the *(Model A, Model B)* match-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1754953791454,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "SnQLu3kfWWN2",
    "outputId": "bc521da1-9dc9-423c-8c58-7ab10b54928f"
   },
   "outputs": [],
   "source": [
    "def visualize_battle_count(battles, title, show_num_models=30):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        battles : pd.DataFrame with columns ['model_a','model_b', ...]\n",
    "        title   : str, title for the plot\n",
    "        show_num_models : int, how many top models (by total battle count) to display\n",
    "\n",
    "    Output:\n",
    "        fig : plotly.graph_objects.Figure heatmap of symmetric battle counts\n",
    "    \"\"\"\n",
    "    ptbl = pd.pivot_table(battles, index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "    battle_counts = ptbl + ptbl.T\n",
    "    ordering = battle_counts.sum().sort_values(ascending=False).index\n",
    "    ordering = ordering[:show_num_models]\n",
    "\n",
    "    fig = px.imshow(\n",
    "        battle_counts.loc[ordering, ordering],\n",
    "        title=title,\n",
    "        text_auto=True\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Model B\",\n",
    "        yaxis_title=\"Model A\",\n",
    "        xaxis_side=\"top\",\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "        title_y=0.07,\n",
    "        title_x=0.5,\n",
    "        font=dict(size=10)\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Count: %{z}<extra></extra>\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize selected_battles and selected_battles_no_ties subsequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Selected Battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_battle_count(selected_battles, title=\"Battle Count of Each Combination of Models\", show_num_models=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Selected Battles No Ties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 818
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1754953793690,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "lXHspIrTWWN3",
    "outputId": "d541b12f-de78-4d79-c56c-dcdc4f21cae6"
   },
   "outputs": [],
   "source": [
    "visualize_battle_count(selected_battles_no_ties, \"Battle Count for Each Combination of Models (without Ties)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 1c**\n",
    "We see many battles between top models (e.g., Claude, GPT, Gemini), while smaller models (e.g., Llama-3-8B) have fewer battles. This is because LMArena employs weighted sampling methods, which assign greater weights to better models.\n",
    "\n",
    "**Task:** Answer the question below (in the cell after the question cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "Q: Why might LMArena pair strong models vs. strong models more often than strong vs. smaller models? \n",
    "Think about statistical power, how quickly you can differentiate models, and ranking uncertainty among top-tier systems.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fWpyS1hmfmHr"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Languge Distribution\n",
    "Let’s explore which languages appear in LMArena battles. See how many language [LMArena](https://lmarena.ai/) is being used for! This is similar to the heatmap on the right side of **Figure 10** in the [LMArena paper](https://arxiv.org/pdf/2403.04132).\n",
    "\n",
    "The full distribution gives context, but the range across languages can be huge. Try zooming in by highlighting the region of the plot you want to view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1754953798690,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "pBNMXUFJWWN3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lang_counts_all = battles[\"language\"].value_counts()\n",
    "\n",
    "fig_lang_all = px.bar(\n",
    "    lang_counts_all,\n",
    "    title=\"Distribution of Languages\",\n",
    "    text_auto=True,\n",
    "    height=400\n",
    ")\n",
    "fig_lang_all.update_layout(\n",
    "    xaxis_title=\"Language\",\n",
    "    yaxis_title=\"Count\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig_lang_all.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Conversation Turns\n",
    "\n",
    "Now let's also try to explore conversation turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(battles[\"turn\"],\n",
    "             title=f\"Number of Conversation Turns\",\n",
    "             text_auto=True, height=400, log_y = True)\n",
    "fig.update_layout(xaxis_title=\"Turns\", yaxis_title=\"Count\", showlegend=False)\n",
    "fig.update_traces(marker_line_color='black', marker_line_width=1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction to Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing and Sharing Your Clusters\n",
    "\n",
    "Now we have plotted a lot of stats but what does this data actually *look* like? You can try printing out conversations in your notebook but reading large amounts of text can be a pain, so how can we better view our data? \n",
    "\n",
    "There are many different ways you can do this but we will show you one UI package which is fast to vibe code and sharable so you can show all these wonderful LLM conversations with your friends (or your future advisors and managers). \n",
    "\n",
    "#### Example: Gradio App\n",
    "\n",
    "One highly recommended tool for this purpose is **Gradio**. Gradio is a Python library for quickly building interactive web apps for your models and visualizations. These apps are easy to share—even with people who don’t have Python installed!\n",
    "\n",
    "- Gradio integrates with [Hugging Face Spaces](https://huggingface.co/spaces), a popular website for hosting and sharing machine learning demos, datasets, and models.\n",
    "\n",
    "One of our staff members, Lisa, **created an [app to go through random conversations](https://huggingface.co/spaces/BerkeleyML/Chatbot-Arena-Viewer) in the Chatbot Arena dataset.** She was able to code this up within 5 minutes (this is an opportunity for you to leverage LLMs to achieve useful tools). Also fun fact: LMArena was a gradio app up until a few months ago!\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.imgur.com/VvGs5nR.png\" alt=\"GradioConversation\" style=\"display: block; margin-left: auto; margin-right: auto; width: 60%;\">\n",
    "</div>\n",
    "\n",
    "Feel free to play around with Gradio experience to view raw chatbot arena conversations or for the cluster visualization you will do in a later problem in this homework.\n",
    "\n",
    "[Gradio documentation](https://www.gradio.app/docs) – reference for building your own apps. To publicly share your app, you may need to sign up for a free [Hugging Face account](https://huggingface.co/welcome) and accept their terms, and create a token for the app linked with your account. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question 2: Model Rankings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explored our data, let's consider how to use these pairwise battles to rank the models by preference. Our goal is to assign a \"strength\" parameter to each model that quantifies how likely it is to win against others.\n",
    "\n",
    "Given we analyzed $M=20$ models and $N=40k$ battles ($26k$ excluding ties), we want to estimate a skill parameter $S_m$ for each model $m \\in \\{1, \\ldots, M\\}$. This parameter $S_m$ should reflect the overall ability of model $m$ to be preferred over other models.\n",
    "\n",
    "Before we move on to more sophisticated probabilistic models that estimate these strength parameters, let’s build some intuition by starting with a simpler metric: the average win rate.\n",
    "\n",
    "Average win rate is simple: a model's average win rate is the proportion of battles they competed in which resulted in them winning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "FFA_WeXeyLE7"
   },
   "source": [
    "##  **Question 2a**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "r3dyvHoVWWN4"
   },
   "source": [
    "LMArena defines the win rate for a model as the average fraction of times it defeats another model across all its match-ups. \n",
    "\n",
    "**Task:**\n",
    "Implement the `compute_pairwise_win_fraction` function, which:\n",
    "\n",
    "1. Calculates the fraction of times each model beats each other model across all battles.\n",
    "\n",
    "2. Returns a square DataFrame where entry (i, j) is the fraction of times model i beats model j. The colmns should be the select model names as well as the index (similar to a confusion matrix). Any model pairings which do not have any battles should be given a NaN value. For instance, diagonal of `row_beats_col` should be NaN as no battles exist between a model and itself.\n",
    "\n",
    "3. The rows and columns of your `row_beats_col` dataframe should be ordered by their average win rate against all other models (i.e. order from strongest to weakest models)\n",
    "\n",
    "Tips:\n",
    "* Do not use your `selected_models` variable in your function, otherwise you may run into autograder issues. Instead define a variable which is the list of models in you input `battles` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1754954135998,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "Lmgw97W_WWN4",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_pairwise_win_fraction(battles):\n",
    "    #TODO\n",
    "\n",
    "    return row_beats_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "##  **Question 2b**\n",
    "\n",
    "\n",
    "Let’s visualize how often **Model A** beats **Model B** in non-tied battles. Below we have used your `compute_pairwise_win_fraction` to create heatmap where each cell `(A, B)` displays the **fraction of A’s wins** over B. No TODOs or code to fill in here, this question allows us to viusally inspect your function in the Coding PDF assignment on gradescope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_pairwise_win_fraction(battles, title):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        battles : pd.DataFrame of non-tied battles with ['model_a','model_b','winner', ...]\n",
    "        title   : str\n",
    "    Output:\n",
    "        fig : plotly Figure heatmap (cell (A,B) = fraction A beats B)\n",
    "    \"\"\"\n",
    "    row_beats_col = compute_pairwise_win_fraction(battles)\n",
    "    fig = px.imshow(\n",
    "        row_beats_col,\n",
    "        color_continuous_scale='RdBu',\n",
    "        text_auto=\".2f\",\n",
    "        title=title\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\" Model B: Loser\",\n",
    "        yaxis_title=\"Model A: Winner\",\n",
    "        xaxis_side=\"top\",\n",
    "        height=900,\n",
    "        width=900,\n",
    "        title_y=0.07,\n",
    "        title_x=0.5\n",
    "    )\n",
    "    fig.update_traces(\n",
    "        hovertemplate=\"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\"\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = visualize_pairwise_win_fraction(\n",
    "    selected_battles_no_ties,\n",
    "    title=\"Fraction of Model A Wins for All Non-tied A vs. B Battles\"\n",
    ")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "##  **Question 2c**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we can just average the win rate of model $i$ against all other models to get an estimate of strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1754954144653,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "AmD-nX_wWWN4",
    "outputId": "9f070e98-3695-40eb-de94-d9aa88ff9eb6"
   },
   "outputs": [],
   "source": [
    "def get_pairwise_win_fraction_plot(battles, title=\"\"):\n",
    "    row_beats_col_freq = compute_pairwise_win_fraction(battles)\n",
    "    pairwise_win_rate = row_beats_col_freq.mean(axis=1).reset_index()\n",
    "    pairwise_win_rate.columns = ['model', 'win rate']\n",
    "\n",
    "    # Rank (1 = best)\n",
    "    pairwise_win_rate[\"rank\"] = (\n",
    "        pairwise_win_rate[\"win rate\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "    )\n",
    "\n",
    "    # Sort for plotting and freeze that order on the x-axis\n",
    "    pairwise_win_rate = pairwise_win_rate.sort_values(by=\"win rate\", ascending=False)\n",
    "    model_order = pairwise_win_rate[\"model\"].tolist()\n",
    "\n",
    "    fig = px.bar(\n",
    "        pairwise_win_rate,\n",
    "        x=\"model\",\n",
    "        y=\"win rate\",\n",
    "        title=title,\n",
    "        text=\"win rate\",\n",
    "        hover_data=[\"rank\"]\n",
    "    )\n",
    "    fig.update_traces(texttemplate=\"%{text:.2f}\", hovertemplate=\"<b>%{x}</b><br>win rate=%{y:.3f}<br>rank=%{customdata}\")\n",
    "    fig.update_layout(\n",
    "        yaxis_title=\"Average Win Rate\",\n",
    "        xaxis_title=\"Model\",\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.update_xaxes(tickangle=45, categoryorder=\"array\", categoryarray=model_order)\n",
    "    return pairwise_win_rate, fig\n",
    "\n",
    "pairwise_win_rate, fig = get_pairwise_win_fraction_plot(selected_battles_no_ties, title=\"Average Win Rate Against All Other Models (No Ties)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we’ve computed and visualized the average win rate of each model against all others, we can start reasoning about the results.\n",
    "In the chart above, we see that some models have very similar average win rates. For example, some GPT, Claude, and Llama variants sit close together. On the other hand, smaller models like llama-3-8b and gemma-2-9b fall noticeably behind.\n",
    "\n",
    "**Task: Answer the following questions in the cell below the question cell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```otter\n",
    "1. Identify at least two models that appear to have similar average win rates.\n",
    "2. Compare their parameter sizes of each model (you can google their parameter counts, and if its a closed source model which does not list its parameter size assume its >100 billion parameters). Is there a relationship between parameter size and performance? Are there any models which stick out as unusally good or bad for its size?\n",
    "3. Why might computing win-rate in this manner misrepresent model strength? Think about the distribution of battles per model pair and how an imbalance in model pairing counts could result in one model ranking high or lower than it should. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# **Question 3: Prompt Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have explored the general features of this dataset. When evaluating models, it’s also often useful to understand the types of questions that are asked. By grouping similar prompts together, we can analyze which models perform well on certain categories and poorly on others. This helps uncover biases in leaderboards (e.g., a model may excel at coding questions but struggle with creative writing).\n",
    "\n",
    "**First, let's identify the most frequent prompts.**\n",
    "\n",
    "In the code cell below, we've already done the following:\n",
    "*   Extracted the first user message (prompt) from conversation_a\n",
    "*   Filtered out only the battles in English and only kept the select models using your `subselect_battles` function from 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def first_user_text(conv):\n",
    "    return (conv[0].get(\"content\") or \"\").strip()\n",
    "\n",
    "battles['prompt'] = battles['conversation_a'].apply(first_user_text).fillna(\"\")\n",
    "eng_battles = battles[(battles['language'] == 'English') | (battles['language'] == 'unknown')]\n",
    "eng_battles, eng_battles_no_ties = subselect_battles(eng_battles, selected_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's view the top 10 promiment prompts for battles in English.\n",
    "Note that there is a caveat for LMArena data, where some of the lanauges are labeled as 'unknown'. This is the reason why we have `battles['language'] == 'unknown'` as part our filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 most common prompts along with their count and percentage of total prompts\n",
    "top_prompts = eng_battles[\"prompt\"].value_counts().head(10)\n",
    "for i, (prompt, count) in enumerate(top_prompts.items(), 1):\n",
    "    print(f\"Rank {i}: {count} samples ({round(100 * count/len(eng_battles), 2)}%)\\n{prompt}\\n\")\n",
    "\n",
    "# print the total percentage of prompts that are 1 of the top 10 prompts\n",
    "top_10_percentage = sum(top_prompts) / len(eng_battles)\n",
    "print(f\"Total percentage of prompts that are 1 of the top 10 prompts: {round(100 * top_10_percentage, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 3a**\n",
    "\n",
    "When evaluating models, it’s useful to understand which prompt types are over-represented and how these popular prompts can influence the leaderboard. Below we have plotted the leaderboard when all of the top 10 prompts are removed. \n",
    "\n",
    "**Task:**  Compare the leaderboard generated from `get_pairwise_win_fraction_plot(eng_no_top_prompts_battles)` to the leaderboard you generated in the previous problem. In 2-3 sentences, answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "1. Is there a difference between the two leaderabords? List any models in the top 5 that have changed position\n",
    "2. What is a potential reason that this leaderboard has shifted? We are looking for an answer which relates to potential model behavior, not just \"prompts were removed thus the leaderboard changed\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# remove top prompts top_prompts from eng_battles_no_ties\n",
    "eng_battles_no_ties_no_top_prompts = eng_battles_no_ties[~eng_battles_no_ties[\"prompt\"].isin(top_prompts.index)]\n",
    "\n",
    "pairwise_win_rate, fig = get_pairwise_win_fraction_plot(eng_battles_no_ties_no_top_prompts, title=\"Average Win Rate Against All Other Models (No Top Prompts)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## **Question 3b**\n",
    "LMArena also provides more detailed **category labels** inside the columns `is_code` and nested `category_tag` column.\n",
    "We have already extracted the following boolean Series for you. Feel free to refer to these columns when you are analyzing your clusters for Question 3c.\n",
    "\n",
    "**Task:** \n",
    "1. Make a bar chart showing these proportion for each category (i.e. the fraction of battles where the category is True)\n",
    "2. Using `get_pairwise_win_fraction_plot`, compute the pairwise win rate for each category and use the `plot_category_rank_heatmap` to visualize the results. Ensure that your data is in *tidy format*, containing columns `model`, `category`, and `win_rate`. You should have each of the categories above as well as an 'overall' category which is the `pairwise_win_rate` you computed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_category_rank_heatmap(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Plot a heatmap of model ranks by category.\n",
    "    \"\"\"\n",
    "    assert \"overall\" in df[\"category\"].unique(), \"'overall' was not found as a category in your dataframe\"\n",
    "    rank_table = df.pivot(index=\"model\", columns=\"category\", values=\"rank\")\n",
    "\n",
    "    if \"overall\" in rank_table.columns:\n",
    "        cols = [\"overall\"] + [c for c in rank_table.columns if c != \"overall\"]\n",
    "        rank_table = rank_table[cols]\n",
    "    # sort models by overall rank\n",
    "    rank_table = rank_table.sort_values(\"overall\", ascending=True)\n",
    "    px.imshow(\n",
    "        rank_table,\n",
    "        text_auto=True,\n",
    "        color_continuous_scale=\"Viridis_r\",\n",
    "        labels=dict(x=\"Category\", y=\"Model\", color=\"Rank (1=best)\"),\n",
    "        zmin=1, zmax=rank_table.max().max(),\n",
    "        aspect=\"auto\"\n",
    "    ).update_layout(\n",
    "        title=\"Overall and Per-Category Ranks\",\n",
    "        width=950,\n",
    "        height=400 + 12 * len(rank_table),\n",
    "        xaxis_side=\"top\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# LMArena also provides more detailed category labels inside the columns `is_code`, `is_refusal`,\n",
    "# and the nested `category_tag` column.\n",
    "# We have already extracted the following boolean Series for you:\n",
    "\n",
    "# GIVEN (do not modify)\n",
    "expected_creative = eng_battles_no_ties['category_tag'].apply(lambda x: x['criteria_v0.1']['creativity'])\n",
    "expected_tech = eng_battles_no_ties['category_tag'].apply(lambda x: x['criteria_v0.1']['technical_accuracy'])\n",
    "expected_if = eng_battles_no_ties['category_tag'].apply(lambda x: x['if_v0.1']['if'])\n",
    "expected_math = eng_battles_no_ties['category_tag'].apply(lambda x: x['math_v0.1']['math'])\n",
    "expected_code = (eng_battles_no_ties['is_code'] == True)\n",
    "\n",
    "# Task:\n",
    "# 1) Make a bar chart showing these proportion for each category (i.e. the fraction of battles where the category is True)\n",
    "# 2) Compute the pairwise win rate for each category and use the plot_category_rank_heatmap to visualize the results\n",
    "\n",
    "# TODO: plot a bar chart of the proportions\n",
    "...\n",
    "\n",
    "# TODO: compute the pairwise win rate for each category and use the plot_category_rank_heatmap to visualize the results\n",
    "# Ensure that your data is tidy format (i.e. has the columns model, category, and win_rate). We have added the category column to the pairwise_win_rate dataframe below.\n",
    "\n",
    "pairwise_win_rate['category'] = 'overall' \n",
    "for category in category_dataframes:\n",
    "    ...\n",
    "\n",
    "rank_dataframes = ...\n",
    "plot_category_rank_heatmap(rank_dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "You might notice that the leaderboards can change quite a bit! This is because different model developers often put more emphasis on ceratin tasks in training their LLMs to better cater to their audience. Many of these models are also limited by the amount of data and compute available, which can further force specialization as some tasks are much harder to learn (esoecially if the model is on the smaller side). \n",
    "\n",
    "We will explore these ideas more in part 2 of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 3c**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The category labels provide a clean way to divide problems but there are likely other ways to group prompts which can reveal other common use cases. \n",
    "\n",
    "We'll now dig deeper by discovering prompt topics via K-Means clustering on the **prompt text**. Since we are interested in seeing what kinda of questions people are asking, use the **no-top-prompts** subset from earlier (`eng_battles_no_ties_no_top_prompts`) to reduce noise. We have sampled 8,000 battles for reasonable runtime and provided a helper function for clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:**\n",
    "\n",
    "1. We already provided you eng_battles_no_ties_no_top_prompts with the 8000 samples\n",
    "2.  Build TF-IDF features X from eng_battles_no_ties_no_top_prompts[\"prompt\"] (default max_features=500)\n",
    "3.  Sweep K over [4, 6, 8, 10, 12] collecting times and inertias\n",
    "4.  Plot runtime vs K and elbow (inertia vs K)\n",
    "5.  Choose a best_K (e.g., 8) from elbow\n",
    "6. Assign labels to eng_battles_no_ties_no_top_prompts[\"cluster\"]\n",
    "7. Visualize the clusters using a 2D projection (e.g., SVD - see [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)), with points colored by their cluster.\n",
    "8.  Save the clustered df to clustered_prompts_no_top_prompts.csv\n",
    "\n",
    "#### More Gradio Visualizations!\n",
    "\n",
    "We have provided another [gradio app](https://huggingface.co/spaces/BerkeleyML/Arena-Cluster-Viewer) for yall to view prompts in each cluster, but feel free to modify it if you want to view different subsets of conversations!\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://i.imgur.com/r9b0xxQ.png\" alt=\"GradioApp\" style=\"display: block; margin-left: auto; margin-right: auto; width: 60%;\">\n",
    "</div>\n",
    "\n",
    "Try it yourself:  You can use Gradio to build an app and easily share your visualization—just like in the staff example! This is a great way to help others conveniently view the clusters you’ve discovered (and delve into the wonderful world of UI vibe coding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Since we're focusing on prompts again, use the no-power-user subset from earlier:\n",
    "eng_battles_no_ties_no_top_prompts = eng_battles_no_ties_no_top_prompts.sample(n=8000, random_state=42)\n",
    "\n",
    "texts = np.array(eng_battles_no_ties_no_top_prompts[\"prompt\"])\n",
    "\n",
    "# Fit the TfidfVectorizer and transform texts in one step, storing the result in X\n",
    "# (feel free to experiment with max_features later in 2c follow-up)\n",
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1,1), min_df=5)\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# General KMeans for any embedding\n",
    "def kmeans_cluster_prompts(features: np.ndarray, prompts: np.ndarray, k: int, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform k-means clustering on features and return:\n",
    "      - dataframe with prompts and their cluster assignments\n",
    "      - model inertia (float)\n",
    "      - elapsed runtime (seconds)\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
    "    cluster_labels = km.fit_predict(features)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    df = pd.DataFrame({\"prompt\": prompts, \"cluster\": cluster_labels})\n",
    "    return df, km.inertia_, elapsed\n",
    "\n",
    "\n",
    "# Example usage of the function (updated to unpack df, inertia, elapsed)\n",
    "clustered_prompts_df, inertia, elapsed = kmeans_cluster_prompts(X, texts, k=5)\n",
    "\n",
    "print(\"Clustered prompts dataframe shape:\", clustered_prompts_df.shape)\n",
    "print(\"Inertia:\", inertia)\n",
    "print(\"Elapsed time (s):\", elapsed)\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(clustered_prompts_df.head())\n",
    "\n",
    "print(\"\\nCluster distribution:\")\n",
    "print(clustered_prompts_df['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Save to CSV\n",
    "clustered_prompts_df.to_csv('clustered_prompts_no_top_prompts.csv', index=False)\n",
    "print(\"\\nSaved to clustered_prompts_no_top_prompts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Since we're focusing on the prompt text and removing power-user bias,\n",
    "# use the no top prompts subset `eng_no_top_prompts_battles`:\n",
    "\n",
    "# 1) We already provided you eng_battles_sample with the 8000 samples\n",
    "# 2) Build TF-IDF features X from eng_battles_sample[\"prompt\"] (default max_features=500)\n",
    "# 3) You can use kmeans_cluster_prompts(features, prompts, k, random_state) to return:\n",
    "#       - DataFrame ['prompt','cluster'], inertia (float), elapsed seconds (float)\n",
    "# 4) Sweep K over [4, 6, 8, 10, 12] collecting times and inertias\n",
    "# 5) Plot runtime vs K and elbow (inertia vs K)\n",
    "# 6) Choose a best_K (e.g., 8) from elbow\n",
    "# 7) Assign labels to eng_battles_sample[\"cluster\"]\n",
    "# 8) Visualize with 2D projection (ex: using SVD) colored by cluster\n",
    "# 9) Save the clustered df to clustered_prompts_no_top_prompts.csv\n",
    "\n",
    "...\n",
    "# Save to CSV\n",
    "out_path = f\"clustered_prompts_no_top_prompts_k{best_K}.csv\"\n",
    "YOUR_DF.to_csv(out_path, index=False)\n",
    "print(f\"Saved -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 3d**\n",
    "\n",
    "We’ll now move from training to interpretation. Using the cluster labels from your selected K, briefly characterize what at least one distinct cluster seems to contain, or explain why patterns are unclear and how you might improve them.\n",
    "\n",
    "For more clear visulization, we suggest you to explore gradio by working with the clustered_prompts_no_top_prompts.csv that was mentioned above.\n",
    "\n",
    "**Task: Answer the questions below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "In 2–3 sentences, use the labels from your clustering to do the following:\n",
    "1. Briefly explain the type(s) of question you see in one of the distinct/similar clusters.\n",
    "2. If you don’t see a clear pattern, describe likely limitations and potential improvements we can make.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 3e**\n",
    "\n",
    "In this homework we have seen how different prompts can lead to leaderboard changes, now its your time to find other prompt types that cause leaderboard shift!\n",
    "\n",
    "**Task:** Find a specific subset of battles in `eng_battles_no_ties_no_top_prompts` that, when removed, causes the **top 5 models to change rankings**. You must create a clear, simple rule for selecting which battles to remove.\n",
    "\n",
    "To get points for this question, you must give the following deliverables:\n",
    "1. Provide your code for selecting the battles to be removed and save your final dataframe with your subset of battles filtered out to a variable called `filtered_battles` \n",
    "2. Produce the leaderboards for the overall leaderboard and the leaderboard of your split using `plot_category_rank_heatmap` to show rank change\n",
    "2. Write 1-3 sentences explaining your solution and why you suspect the leaderboard has shifted. Your solution should have a clear \"rule\" of how you selected your prompts (e.g. I removed all the prompts which contained the word `banana').\n",
    "\n",
    "-------\n",
    "\n",
    "You are free to use any of the columns in `eng_battles_no_ties_no_top_prompts` or create a new column with your custom \"rule\" (e.g. \"contains the word banana\"). For your solution to be valid it **must** adhere to the following properties:\n",
    "* The subset of prompts you remove **must be <=20% of the total prompts** in `eng_battles_no_ties_no_top_prompts` and must follow a clear rule. For instance, randomly removing 20% of prompts from a prompt cluster is invalid because you are subsampling - you need to define a group where removing all of the prompts removes less than 20% of the overall prompts. Similarly, having a casade of several unrelated rules is also invalid. \n",
    "* Each model in `selected_models` **must appear in at least 100 battles** in your final `filtered_battles` dataframe\n",
    "* You cannot simply replicate work from 3a or 3b. For example, you may not remove an entire question category or part of one, nor just delete all “hi” and “hello” prompts. \n",
    "* Your solution must select battles **based on the inputs (either the prompts or the users), not the models**. For example, removing the `is_refusal` category is an invalid solution as refusal is a property of the model response and ot the prompt itself. \n",
    "\n",
    "This intentionally very open ended so don't overthink it! As long as you can clearly describe what types of prompts you removed and why their removal might result in a ranking change you will get full points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_out_battles(eng_battles_no_ties_no_top_prompts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Selects and removes a subset of battles containing a specific type of input.\n",
    "\n",
    "    Args:\n",
    "        eng_battles_no_ties_no_top_prompts (pd.DataFrame): DataFrame containing battles. \n",
    "            Must include a 'judge_hash' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with <=20% of the total prompts removed.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    return filtered_battles\n",
    "\n",
    "filtered_battles = filter_out_battles(eng_battles_no_ties_no_top_prompts)\n",
    "custom_leaderboard, _ = get_pairwise_win_fraction_plot(filtered_battles)\n",
    "custom_leaderboard[\"category\"] = \"custom\"\n",
    "plot_category_rank_heatmap(pd.concat([custom_leaderboard, pairwise_win_rate]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Free response:** Write 1-3 sentences explaining your solution and why you suspect the leaderboard has shifted. Your solution should have a clear \"rule\" of how you selected your prompts (e.g. I removed all the prompts which contained the word `banana')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cs189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(selected_models, list), '`selected_models` should be a list of model names.'\n>>> assert len(selected_models) == 20\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_selected_battles_filter(battles, selected_models, selected_battles, selected_battles_no_ties):\n...     assert isinstance(battles, pd.DataFrame), '`battles` should be a pandas DataFrame.'\n...     expected_selected = battles[battles['model_a'].isin(selected_models) & battles['model_b'].isin(selected_models)]\n...     expected_no_ties = expected_selected[~expected_selected['winner'].astype(str).str.contains('tie')]\n...     pd.testing.assert_frame_equal(selected_battles.reset_index(drop=True), expected_selected.reset_index(drop=True), check_dtype=False)\n...     pd.testing.assert_frame_equal(selected_battles_no_ties.reset_index(drop=True), expected_no_ties.reset_index(drop=True), check_dtype=False)\n...     assert not selected_battles_no_ties['winner'].astype(str).str.contains('tie', case=False, na=False).any(), '`selected_battles_no_ties` must not contain any ties.'\n...     assert set(selected_battles.columns) == set(battles.columns), '`selected_battles` should preserve the same columns as `battles`.'\n...     assert set(selected_battles_no_ties.columns) == set(battles.columns), '`selected_battles_no_ties` should preserve the same columns as `battles`.'\n>>> test_selected_battles_filter(battles, selected_models, selected_battles, selected_battles_no_ties)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_compute_pairwise_win_fraction():\n...     df = compute_pairwise_win_fraction(battles)\n...     assert isinstance(df, pd.DataFrame), 'Output must be a pandas DataFrame.'\n...     assert len(df.index) == len(df.columns) > 0, 'Output must be a non-empty square matrix.'\n...     assert (df.index == df.columns).all(), 'Row and column labels must match (same models in same order).'\n...     vals = df.values\n...     finite_mask = np.isfinite(vals)\n...     assert np.all((vals[finite_mask] >= 0) & (vals[finite_mask] <= 1)), 'All finite entries must be within [0, 1].'\n>>> test_compute_pairwise_win_fraction()\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(proportions, dict)\n>>> for val in proportions.values():\n...     assert 0 <= val <= 1\n>>> assert len(rank_dataframes) == 120\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'filtered_battles' in locals(), 'filtered_battles does not exist'\n>>> assert len(filtered_battles) / len(eng_battles_no_ties_no_top_prompts) >= 0.8, f'removed too many prompts - filtered battles are {len(filtered_battles) / len(eng_battles_no_ties_no_top_prompts) * 100}% of total prompts, should be >= 80%'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
