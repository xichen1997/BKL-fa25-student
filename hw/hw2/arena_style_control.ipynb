{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT: On Colab, we expect your homework to be in the cs189 folder\n",
    "## Please contact staff if you encounter any problems with installing dependencies\n",
    "import sys\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "if IS_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/cs189/hw/hw2\n",
    "    %pip install -r ./requirements.txt\n",
    "    !pip install -U kaleido plotly\n",
    "    import kaleido\n",
    "    kaleido.get_chrome_sync()\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = pio.renderers.default + \"+png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25c9e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"arena_style_control.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40ce34",
   "metadata": {
    "id": "S5acmOY9WR4j"
   },
   "source": [
    "<link rel=\"stylesheet\" href=\"berkeley.css\">\n",
    "\n",
    "<h1 class=\"cal cal-h1\">Homework 02 ‚Äì Welcome to the Arena (Style Control)</h1>\n",
    "\n",
    "CS 189, Fall 2025\n",
    "\n",
    "In this homework you will get more experience with logistic regression to create model leaderboards.\n",
    "\n",
    "We will be taking real data from [LMArena](https://lmarena.ai/), a popular platform for crowdsourcing evaluations of large language models and recreating their leaderboards, with a few fun extra steps along the way.\n",
    "\n",
    "The chats can be viewed interactively by accessing [ChatBot-Arena-Viewer](https://huggingface.co/spaces/BerkeleyML/Chatbot-Arena-Viewer) through hugging face. Much of the first half of this homework was first written by Prof Gonzalez back when his students first started the project, and now LMArena is a standard evaluation for large language models and turned into a company! Don't let anyone tell you logistic regression isn't valuable, it's worth at least $600 Million.\n",
    "    \n",
    "---\n",
    "\n",
    "\n",
    "## Due Date: Friday, October 17, 11:59 PM\n",
    "\n",
    "This assignment is due on **Friday, October 17, 11:59 PM**. You must submit your work to Gradescope by this deadline. Please refer to the syllabus for the [Slip Day policy](https://eecs189.org/fa25/syllabus/#slip-days). No late submissions will be accepted beyond the details outlined in the Slip Day policy.\n",
    "\n",
    "### Submission Tips\n",
    "- **Plan ahead**: We strongly encourage you to submit your work several hours before the deadline. This will give you ample time to address any submission issues.\n",
    "- **Reach out for help early**: If you encounter difficulties, contact course staff well before the deadline. While we are happy to assist with submission issues, we cannot guarantee responses to last-minute requests.\n",
    "      \n",
    "<!-- --- -->\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "In this homework you will build on the previous warmup section, implementing the Bradley-Terry ranking used in the actual Arena and taking account of style controls for model rank. In particular, you will:\n",
    "1. Apply the Bradley‚ÄìTerry model to build leaderboards\n",
    "2. Practice analyzing conversational data and extracting stylistic features  \n",
    "3. Build custom features (length, punctuation, phrase presence, etc.) and integrate them into ranking models  \n",
    "4. Explore confounding stylistic variables in LLM evaluation (style vs. content)  \n",
    "5. Apply pairwise evaluation methods to understand how style affects outcomes  \n",
    "  \n",
    "---\n",
    "\n",
    "### Collaboration Policy\n",
    "You are encouraged to discuss high-level concepts with your peers. However:\n",
    "- All submitted work must be written in your own words and code.\n",
    "- Do not share or copy solutions directly.\n",
    "- List any collaborators (students you worked with) in the line below:\n",
    "\n",
    "**Your Collaborators**: **TODO**\n",
    "\n",
    "### AI Tools Usage Disclosure\n",
    "We allow the use of AI tools (e.g., ChatGPT, Copilot) **only as support**, not as a replacement for your own reasoning. To ensure transparency, you must acknowledge any use of AI tools.\n",
    "\n",
    "Please complete one of the following:\n",
    "- **A) I did not use any AI tools for this homework.**\n",
    "- **B) I used AI tools in the following way(s):**  \n",
    "  (describe briefly, e.g., ‚ÄúUsed ChatGPT to get hints for debugging a NumPy indexing error‚Äù)\n",
    "\n",
    "\n",
    "**Your Answer**: **TODO**\n",
    "    \n",
    "---\n",
    "\n",
    "### Grading Breakdown\n",
    "\n",
    "| Question | Manual Grading? | Points |\n",
    "|----------|-----------------|--------|\n",
    "| q4a      | No              | 2      |\n",
    "| q4b      | No              | 2      |\n",
    "| q4c      | Yes             | 2      |\n",
    "| q5a      | No              | 2      |\n",
    "| q5b      | No              | 2      |\n",
    "| q7a      | No              | 2      |\n",
    "| q7b      | No              | 2      |\n",
    "| q8a      | No              | 2      |\n",
    "| q8afrq   | Yes             | 2      |\n",
    "| q8b      | No              | 2      |\n",
    "| q8c      | No              | 2      |\n",
    "| q9a      | No              | 2      |\n",
    "| q9b      | Yes             | 4      |\n",
    "| q9c      | Yes             | 6      |\n",
    "| **Total**|                 | **34** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcb4e3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1661,
     "status": "ok",
     "timestamp": 1754949252732,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "Fjrjh-brWR4n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotting_utils import plot_rank_heatmap, plot_style_features\n",
    "#set fixed seed of 189\n",
    "np.random.seed(189)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377be9b",
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1754949269115,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2BHL7ichWR4o"
   },
   "outputs": [],
   "source": [
    "# ! pip install ipywidgets\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856324d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "ee9bd84d37b74df599bb04a0674d96ae",
      "f999e16b1f03445f819a2d780482f8fd",
      "924710c922e04900bbf39d5c0148184a",
      "3b29af24732d4ce09978703f5819620e",
      "b2090f9ca74d446b80879d3e7bb8fa0e",
      "7fe95e7a459c4b188f911ec6a2a869d1",
      "6b77a7fe1f434937bab915ab291d84c0",
      "1f03c4dc981047348492cd1ee93844b4",
      "a34a2b4628534adeb1275f54d9f856fd",
      "7269d9a850684affadbb845fac333fd6",
      "40c695f88e114b8a804305936f15e34a",
      "f44ab2de0a66487b99385b06ca3e0927",
      "6da5f82604ae425f9a15d0d4a036001e",
      "e5727bca7f8c4e64abe2271add37c3c4",
      "515d41fc4015482c9f3e1de36ec964a5",
      "799bb3928ea24eff837684dc6f84becf",
      "7c0d795d30ba4782b0ebff647f6376f0",
      "5c7e225b4ebd4e65be7d7315a9dd8168",
      "663a003ffbc84a4787ddf7ba13ce1d7e",
      "21f023f65970482e84f7daa78dc16b34",
      "86d5261e767645d088bafd4f616603f2",
      "e0bb2688693a4ab1815d897cb67ada37",
      "49400e054cde42759dab2d5a519c71c7",
      "5396890a6a2e47c785a9c6b410e7bac7",
      "96273c7dae504597875d36bdcb81e7c2",
      "a5a749f2d39048089df69d673dece032",
      "d116ee9efee24cad9b7ae54c4ccc95ea",
      "c19dac25e6044be194c44e7ecb87ef44",
      "361a0ef284604dbea5fb4c499a4b2d74",
      "8e92ffcb211b4a43847b14d240f6f006",
      "99105de4cb444e9ca2291b9a9fed5c14",
      "cb90820ae343424494f84181b496aca3",
      "096e39f01bfd4eb889d988253db708ff"
     ]
    },
    "executionInfo": {
     "elapsed": 30891,
     "status": "ok",
     "timestamp": 1754949300115,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "VpHcbudcWR4o",
    "outputId": "e64834e3-ba19-41c6-fc8f-8e61c63adb4c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset (this will take a few minutes to download)\n",
    "ds = load_dataset(\"lmarena-ai/arena-human-preference-100k\")\n",
    "battles = ds['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11eff00",
   "metadata": {},
   "source": [
    "## Initialize with HW2 Warmup\n",
    "\n",
    "Fill in the cells in this section with your implementations from part 1 of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea669ad2",
   "metadata": {},
   "source": [
    "#### FILL IN: HW2 Part 1 Question 1B Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924da4f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754949300357,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "OJdE_ePCWR4p"
   },
   "outputs": [],
   "source": [
    "# #TODO: FILL IN from your solution for Question 1B\n",
    "# models = ...\n",
    "# selected_models = ...\n",
    "# selected_battles = ...\n",
    "# selected_battles_no_ties = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before dedup: \", len(selected_battles_no_ties))\n",
    "selected_battles_no_ties = selected_battles_no_ties[selected_battles_no_ties[\"dedup_tag\"].apply(lambda x: x.get(\"sampled\", False))]\n",
    "print(\"After dedup: \", len(selected_battles_no_ties))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc068f2",
   "metadata": {},
   "source": [
    "##  **Question 4: Model Strengths**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cef763",
   "metadata": {},
   "source": [
    "In the earlier part of the homework, we calculated the Average Model Win-Rate.\n",
    "\n",
    "However, this method is not ideal for our use case where battle counts per model are not equal. For instance, if ChatGPT-4o-latest battled more often with weaker models, it would have a high win rate without being an actually stronger model. Now let's explore how we can instead *learn* these model strengths.\n",
    "\n",
    "**To recap,** we want to construct a leaderboard by assigning a strength score $S_m$ to each model $m \\in \\{1,...,M\\}$, such that:\n",
    "- The ranking reflects the probability of one model winning against another.\n",
    "- For any pair of models A and B, the probability that A beats B, should depend on the *difference* in their strengths: $S_A - S_B$. Why the difference? Since we are measuring pairwise preference, there is no absolute measure of strength but rather a model's strength *relative* to other models.\n",
    "\n",
    "**Formally, we want a function $f$ such that**\n",
    "- For models A and B with scores $S_A$ and $S_B$, we want:\n",
    "  $$ P(\\text{A beats B}) = f(S_A - S_B) $$\n",
    "- The function $f$ should be increasing (bigger skill gap, higher win chance), and always output a probability between 0 and 1.\n",
    "\n",
    "At this point, a natural question is: what should we choose for the function $f$? A standard and effective choice is the logistic (sigmoid) function:\n",
    "\n",
    "$$ P(\\text{A beats B}) = \\sigma(S_A - S_B) = \\frac{1}{1 + e^{-(S_A - S_B)}} $$\n",
    "\n",
    "Notice that this is exactly the same form as logistic regression, where the model scores are the parameters to be learned. In other words, learning model strengths from pairwise outcomes is equivalent to fitting a logistic regression model to the data.\n",
    "\n",
    "So, we can use logistic regression to learn the model strengths that best explain the observed battle outcomes. The higher a model's score, the more likely it is to win against others. The methodology is called the [Bradley-Terry](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model) model and is the underlying theory to other common scoring systems like ELO ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5e966",
   "metadata": {},
   "source": [
    "#### Step 1: Understanding the formulation of the problem as features\n",
    "\n",
    "To learn these model strengths, we need to prepare our data in a form suitable for logistic regression. Recall that each battle involves two models: A and B. One of them wins (for simplicity we still start by removing any battles that end in ties).\n",
    "\n",
    "We want to convert this into:\n",
    "\n",
    "1. A feature vector indicating which two models were involved.\n",
    "2. A label representing the winner.\n",
    "\n",
    "Each row produces two training examples:\n",
    "\n",
    "1. One with model A as +1 and model B as ‚Äì1, and a label denoting if model A wins\n",
    "2. Another with model B as +1 and model A as ‚Äì1, and a label denoting if model B wins\n",
    "\n",
    "Let's take a look an example:\n",
    "\n",
    "`row = {'model_a': 'gpt-4o-2024-05-13', 'model_b': 'claude-3-opus-20240229', 'winner': 'model_a'}`\n",
    "\n",
    "This generates two features, which are...\n",
    "\n",
    "**Feature 1:** [1, -1]\n",
    "* +1 at index 0 (gpt-4o)\n",
    "* ‚Äì1 at index 1 (claude-3-opus)\n",
    "* Label: 1 because model A (gpt-4o) won\n",
    "\n",
    "**Feature 2:** [-1, 1]\n",
    "* -1 at index 0 (gpt-4o)\n",
    "* +1 at index 1 (claude-3-opus)\n",
    "* Label: 0 because model B (claude-3-opus) lost\n",
    "\n",
    "Why do we have to do this?\n",
    "This lets the model take account for both ways:\n",
    "\n",
    "\n",
    "$$ P(\\text{GPT-4o beats Claude-3-opus}) = \\sigma(S_{\\text{GPT-4o}}  - S_{\\text{Claude}})$$\n",
    "\n",
    "\n",
    "\n",
    "$$ P(\\text{Claude-3-opus beats GPT-4o}) = \\sigma(S_{\\text{Claude}} - S_{\\text{GPT-4o}}  )$$\n",
    "\n",
    "\n",
    "**Think:** When we have more than two models, how should we handle the models that were not involved in the battle?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fada783",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Step 2: Constructing generalized features and labels\n",
    "\n",
    "Cool, now we want to generalize this formulation to the pair of not only GPT-4o and Claude-3-opus, but all the models.\n",
    "\n",
    "Once we have turned all battle outcomes into feature vectors, we can organize them into a **feature matrix** $\\mathbf{X}$ and a **label vector** $\\mathbf{y}$.\n",
    "\n",
    "We have the model strengths we want to learn:\n",
    "\\begin{bmatrix}\n",
    "S_A \\\\\n",
    "S_B \\\\\n",
    "S_C\n",
    "\\end{bmatrix}\n",
    "\n",
    "And we want our model to predict:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\sigma(S_A - S_C) \\\\\n",
    "\\sigma(S_B - S_A) \\\\\n",
    "\\sigma(S_B - S_C)\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "\n",
    "As a recap...\n",
    "\n",
    "- $\\mathbf{X}$ encodes **who played whom** and in what direction.\n",
    "- $\\mathbf{S}$ are the model strengths we are trying to learn.\n",
    "- $\\mathbf{y} = \\sigma(\\mathbf{X} \\cdot \\mathbf{S})$ gives us the predicted win probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400a7e6",
   "metadata": {},
   "source": [
    "Now let's try to actually featurize these battles and labels!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd87a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 4a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f63f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In order to train our model, we should first featurize our battles as discussed before.\n",
    "\n",
    "**Task:** Implement the function below to transform `selected_battles_no_ties` and `selected_models` into feature vectors and labels. This will allow us to represent each battle as input-output pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32130eee",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def turn_into_features(df, models):\n",
    "    '''\n",
    "    Convert pairwise battle results into feature matrix X and label vector y\n",
    "    suitable for logistic regression based on the Bradley-Terry model\n",
    "    '''\n",
    "    # TODO:\n",
    "    # 1. Iterate through each row in the DataFrame.\n",
    "    # 2. For each battle, create a feature vector:\n",
    "    #    - Assign +1 to the column corresponding to 'model_a'.\n",
    "    #    - Assign -1 to the column corresponding to 'model_b'.\n",
    "    #    - All other columns should be 0.\n",
    "    # 3. Append the label:\n",
    "    #    - 1 if 'model_a' is the winner.\n",
    "    #    - 0 if 'model_b' is the winner.\n",
    "    # 4. Return the feature matrix X and label vector y as numpy arrays.\n",
    "    ...\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = turn_into_features(selected_battles_no_ties, selected_models)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1744de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47245a53",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 4b**\n",
    "Now that we have extracted out the features from the previous question, let's now dive into actually building the model. Note that for Bradley-Terry logistic regression, we do *not* want an intercept in your model. \n",
    "\n",
    "**Task:** \n",
    "Train the model with the features and labels created in Question 4a, and store the strengths, sorted in the order of scores in `results_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908f89b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = ...\n",
    "scores = ...\n",
    "\n",
    "results = {\"Model\": selected_models, \"Score\": scores}\n",
    "results_df = pd.DataFrame(results).sort_values(\"Score\", ascending=False).reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdb759",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c9781",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 4c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ebb9f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's think about an important aspect of our formulation.\n",
    "\n",
    "**Task:** Answer the following question: Why we don't need an intercept for the logistic regression formulation above?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2519bb4",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c2864",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# **Question 5: Confidence Intervals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df3f73",
   "metadata": {},
   "source": [
    "From the previous question, we were able to train the model and obtain the scores of the models. \n",
    "\n",
    "However, when comparing model scores, it's important to understand not just the average performance, but also how much uncertainty there is in our estimates. Our rankings are based on a finite sample of battles, and if we had collected a different set of match-ups, the resulting scores could be different. This sampling variability means that our estimated model strengths are subject to noise.\n",
    "\n",
    "Bootstrapping is a powerful, intuitive way to assess this uncertainty without making strong assumptions about the underlying data. By repeatedly resampling our observed battles (with replacement) and retraining the model on each resampled dataset, we simulate what might have happened if we had observed a slightly different set of battles. For each resample, we get a new set of model scores. By looking at the distribution of these bootstrapped scores, we can estimate confidence intervals for each model's strength.\n",
    "\n",
    "In short, bootstrapping helps us answer: \"If we repeated this evaluation process many times, how much could each model's score vary just due to random chance in which battles we happened to observe?\" This gives us a more honest sense of which differences in model scores are robust, and which might just be due to luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9733a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 5a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9410e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let's implement a function that returns these scores and confidence intervals after bootstrapping.\n",
    "\n",
    "**Task:** \n",
    "* Bootstrap the samples to train a new logistic regression model.\n",
    "* Store each set of coefficients (or learned model strengths).\n",
    "* Compute the mean and percentiles (2.5th and 97.5th) to obtain the 95% confidence intervals.\n",
    "* Return i) results_df, ii) mean_scores, iii) confidence_intervals\n",
    "\n",
    "An example outout of results_df is below.\n",
    "\n",
    "| Model                   | Average Score | Lower Bound | Upper Bound | Category | Rank |\n",
    "|-------------------------|--------------:|------------:|------------:|----------|-----:|\n",
    "| chatgpt-4o-latest       |     0.888888  |   0.777777  |   0.999999  | Overall  |    1 |\n",
    "| gemini-1.5-pro-exp-0801 |     0.777777  |   0.666666  |   0.888888  | Overall  |    2 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c29a44d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_bootstrapped_score(X, y, models, category_name=\"Overall\", n_bootstrap=10):\n",
    "    \"\"\"\n",
    "    Bootstraps logistic regression model scores to estimate confidence intervals.\n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        models: List of model names (order matches columns of X)\n",
    "        n_bootstrap: Number of bootstrap samples\n",
    "    Returns:\n",
    "        results_df: DataFrame with Model, Average Score, Lower Bound, Upper Bound\n",
    "        mean_scores: Mean of bootstrapped scores (np.array)\n",
    "        confidence_intervals: 2.5 and 97.5 percentiles (np.array shape [2, n_models])\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "    np.random.seed(189)  # for reproducibility\n",
    "    bootstrap_scores = []\n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        ...\n",
    "        model = ...\n",
    "        model.fit(...)\n",
    "        ...\n",
    "    ...\n",
    "    mean_scores = ...\n",
    "    ...\n",
    "    results = {\n",
    "        \"Model\": ...\n",
    "        \"Average Score\": ...\n",
    "        \"Lower Bound\": ...\n",
    "        \"Upper Bound\": ...\n",
    "        \"Category\": ...\n",
    "    }\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, mean_scores, confidence_intervals\n",
    "results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=10)\n",
    "\n",
    "# Test that confidence intervals make sense\n",
    "assert (confidence_intervals[0] <= confidence_intervals[1]).all(), \"Every lower bound must be <= upper bound.\"\n",
    "assert ((confidence_intervals[0] <= mean_scores) & (mean_scores <= confidence_intervals[1])).all(), \"Each mean score should lie within its CI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67d219",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec72c2",
   "metadata": {},
   "source": [
    "### Now let's visualize the intervals! *üßô*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc5dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=10)\n",
    "fig = go.Figure()\n",
    "\n",
    "# Use the sorted values from results_df for plotting\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_df[\"Model\"],\n",
    "    y=results_df[\"Average Score\"],\n",
    "    mode='markers',\n",
    "    name='Model Scores',\n",
    "    marker=dict(size=5, color='blue'),\n",
    "    error_y=dict(\n",
    "        type='data',\n",
    "        array=results_df[\"Upper Bound\"] - results_df[\"Average Score\"],   # Upper error\n",
    "        arrayminus=results_df[\"Average Score\"] - results_df[\"Lower Bound\"],  # Lower error\n",
    "        visible=True\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Scores with 95% Confidence Intervals (Sorted by Mean Score)',\n",
    "    xaxis_title='Models',\n",
    "    yaxis_title='Score',\n",
    "    xaxis=dict(tickangle=45),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d299f9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **Question 5b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec5c31",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now that we have confidence intervals, we can assign a rank to each model. We want the rank of model $i$ to represent the number of models that are **confidently better** than model $i$.\n",
    "\n",
    "When we say model A is **confidently better** than model B, it will mean that model A's lower bound is still greater than model B's upper bound. Remember that greater rank means that there are more models that perform better than the current model.\n",
    "\n",
    "**Task:**\n",
    "Implement the `assign_rank` function below that assigns rank to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311144c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def assign_rank(row, df=results_df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        row : pd.Series\n",
    "            A row of the DataFrame (representing a model‚Äôs metrics).\n",
    "        df : pd.DataFrame (default = results_df)\n",
    "            DataFrame containing model performance with 'Lower Bound' and 'Upper Bound'.\n",
    "\n",
    "    Output:\n",
    "        int : The rank of the model, defined as (# of models confidently better) + 1.\n",
    "    \"\"\"\n",
    "\n",
    "    count = ...\n",
    "    return ...\n",
    "\n",
    "\n",
    "results_df['Rank'] = results_df.apply(lambda r: assign_rank(r, results_df), axis=1)\n",
    "results_df = results_df.sort_values(by=\"Rank\", ascending=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459f839",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3492a33f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Example (non-graded): visualize the leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8fa3d0",
   "metadata": {},
   "source": [
    "Now that you have your leaderboard, let's use our premade `plot_rank_heatmap` in `plotting_utils.py` to visualize the results. This is an example cell and not a graded question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (for the current case: overall rank only)\n",
    "category_names = ['Overall']\n",
    "fig = plot_rank_heatmap(results_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5100f",
   "metadata": {},
   "source": [
    "> **NOTICE BEFORE YOUR PROGRESS**\n",
    "> - If you accidentally modify `selected_battles_no_ties` in a way that breaks later parts, double check and **reset it** using the initial block of code you have placed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdbd19",
   "metadata": {},
   "source": [
    "# **Question 7: Ranking Influences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046aa0d2",
   "metadata": {
    "id": "BqCtCfqjWR4u"
   },
   "source": [
    "One thing that has been known to affect user preference is response length: people (and LLM's) tend to prefer longer answers. A recurring observation in human grading and UX is that **longer responses are often preferred**. For example, analyses from the SAT essay reported that **essay length strongly correlated with higher scores‚Äîeven when errors were present** ([New York Times, 2005](https://www.nytimes.com/2005/05/04/education/sat-essay-test-rewards-length-and-ignores-errors.html)). \n",
    "\n",
    "In the context of LLM evaluations, this motivates a core question: **does response length systematically tilt battle outcomes and model rankings?**\n",
    "\n",
    "\n",
    "Let's investigate whether length plays a role in model rankings. First let's do some quick analysis on the response length per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7244f791",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9ubVgwtsWcd0"
   },
   "source": [
    "## **Question 7a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fb6e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CTjSPxR-YZew"
   },
   "source": [
    "We want to analyze whether **response length (in tokens)** is related to model rankings.\n",
    "\n",
    "In `per_model_battles` (which is what you would implement in Q7b), the **`conversation`** column contains, for each row, a *single exchange* between a user and a model (one battle). It is represented as a **list of message dictionaries**. These dictionaries are representing a full exchange between a user and a model in a single battle. The number of turns is `len(row['conversation']) // 2`.\n",
    "\n",
    "**Each message dictionary contains (as provided):**\n",
    "1. `\"content\"` ‚Äì the text of the message  \n",
    "3. `\"role\"` ‚Äì either `\"user\"` or `\"assistant\"`. In our question we will be focusing on `\"assistant\"`\n",
    "\n",
    "---\n",
    "\n",
    "#### What exactly is `conv`?\n",
    "\n",
    "For this question, assume your function will receive **`conv`**, which is a **dictionary** with a single key `\"conversation\"` mapping to a **list of message dictionaries**:\n",
    "\n",
    "```python\n",
    "conv_example = {\n",
    "    \"conversation\": [\n",
    "        {\"role\": \"user\", \"content\": \"How do I sum a list in Python?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Use the built-in function: sum(your_list).\"}\n",
    "    ]\n",
    "}\n",
    "# conv_example[\"conversation\"]  -> list of message dicts, ordered by turns\n",
    "# Each dict has:\n",
    "#   - \"role\": \"user\" or \"assistant\"\n",
    "#   - \"content\": str (message text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab15a8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-pzu_xZcWhl_"
   },
   "source": [
    "**Task:**\n",
    "Implement a function `calculate_response_length` that, given a conversation `conv`, returns the total number of GPT-2 tokens in the concatenation of all **assistant messages** in that conversation.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Use the tiktoken library with the \"gpt2\" encoding. Import tiktoken.\n",
    "\n",
    "2. Concatenate only the assistant messages and count tokens.\n",
    "\n",
    "3. Call `enc.encode(..., disallowed_special=())` to allow all special tokens (avoids ValueError, e.g., for <|endoftext|>).\n",
    "\n",
    "4. Concatenate all assistant role messages separated by two newlines (\"\\n\\n\") before counting tokens (join each them by this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17843dd5",
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1754949482422,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "tBHBaRLDWR4u",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def calculate_response_length(conv):\n",
    "    ...\n",
    "    ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5200d334",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc333e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c4pSBvMCZW4N"
   },
   "source": [
    "## **Question 7b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124bd70f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "J6UGwlkkc_rc"
   },
   "source": [
    "In the previous question (Q7a), you wrote a function to compute the **token length** of a model's reply from a conversation. We‚Äôll now **reshape** the battle-level data so that each row corresponds to a **single model‚Äôs response in a single battle** (instead of one row per battle).\n",
    "\n",
    "In the `selected_battles_no_ties` DataFrame, each row represents a battle between two models, with:\n",
    "\n",
    "*   conversation_a = the conversation for model_a in that battle\n",
    "*   conversation_b = the conversation for model_b in that battle\n",
    "\n",
    "\n",
    "To analyze response length per model, we would want a table where each row corresponds to a single model's response in a single battle (rather than one row per battle).\n",
    "\n",
    "Specifically, our goal is to turn each battle row into **two rows** (tidy format):\n",
    "1. one for `model_a` using `conversation_a`\n",
    "2. one for `model_b` using `conversation_b`\n",
    "\n",
    "**Task:**\n",
    "Using the function defined in Question 7a, create a DataFrame named `per_model_battles ` with columns:\n",
    "\n",
    "1. **`conversation`** ‚Äî the list of message dicts for that model‚Äôs side of the battle  \n",
    "2. **`model`** ‚Äî the model name  \n",
    "3. **`response_length`** ‚Äî integer token count of all assistant messages concatenated (computed via **`calculate_response_length`** from Q7a)\n",
    "> **Hint:** `pd.concat` might be handy for stacking the A-side and B-side tables into one.  \n",
    "> Docs: https://pandas.pydata.org/docs/reference/api/pandas.concat.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22195627",
   "metadata": {
    "executionInfo": {
     "elapsed": 27703,
     "status": "ok",
     "timestamp": 1754949516871,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "Z7VkycO_ZVHY",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: \n",
    "battles_a = ..\n",
    "battles_b = ..\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "per_model_battles[...] = ...\n",
    "per_model_battles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9f521",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef5573",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1754949525877,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "p-TtEVXcrYyz",
    "outputId": "7f9c53dc-9cf5-495c-b99b-7aa5b5c37f15"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the structure\n",
    "# print(per_model_battles['conversation'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802b7fa",
   "metadata": {
    "id": "snbYDBeXWR4u"
   },
   "source": [
    "Let's plot the response length for each model ordered by their rank and fit a trendline to see if there is any relation between rank and length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74336c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1754949531743,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "-nLUbYgOWR4u",
    "outputId": "bab1ff8f-681b-4c02-f481-83b9632f5f9d"
   },
   "outputs": [],
   "source": [
    "model_lineup = results_df.sort_values(\"Rank\")['Model'].tolist()\n",
    "avg_lengths = per_model_battles.groupby(\"model\")[\"response_length\"].mean().reset_index()\n",
    "avg_lengths[\"model\"] = pd.Categorical(avg_lengths[\"model\"], categories=model_lineup, ordered=True)\n",
    "avg_lengths = avg_lengths.sort_values(\"model\").reset_index(drop=True)\n",
    "\n",
    "# Add a numeric rank column for trendline fitting\n",
    "avg_lengths[\"rank\"] = avg_lengths.index + 1  # 1 = best, etc.\n",
    "\n",
    "# Fit a linear trendline (polyfit) to the response length vs. rank\n",
    "z = np.polyfit(avg_lengths[\"rank\"], avg_lengths[\"response_length\"], 1)\n",
    "p = np.poly1d(z)\n",
    "trendline = p(avg_lengths[\"rank\"])\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=avg_lengths[\"model\"],\n",
    "    y=avg_lengths[\"response_length\"],\n",
    "    mode='lines+markers',\n",
    "    name='Avg Response Length'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=avg_lengths[\"model\"],\n",
    "    y=trendline,\n",
    "    mode='lines',\n",
    "    name='Trendline',\n",
    "    line=dict(dash='dash', color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Average Response Length of Models (with Trendline)\",\n",
    "    xaxis_title=\"Model (sorted by performance)\",\n",
    "    yaxis_title=\"Average Response Length\",\n",
    "    xaxis_tickangle=45,\n",
    "    yaxis=dict(range=[0, max(avg_lengths[\"response_length\"].max(), trendline.max()) * 1.05])  # y-axis starts at 0\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e795f0",
   "metadata": {},
   "source": [
    "# **Question 8: Style Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9a81d",
   "metadata": {
    "id": "Q1PEDWwGi-XO"
   },
   "source": [
    "It looks like there is a trend: models with shorter responses tend to be ranked lower. While not a perfect analysis, if it could be true that people are preferring models which generate longer responses regardless of their other capabilities, then it would be useful to create a leaderboard which is *length agnostic*. Meaning, creating leaderboard model scores that control for certain stylistic properties of responses.\n",
    "\n",
    "So how can we control for these stylistic factors in our model rankings?\n",
    "\n",
    "In this question, you will implement a style feature ranking pipeline, starting with length as the only style feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11fd49f",
   "metadata": {
    "id": "e7eLOGTJjAzy"
   },
   "source": [
    "1. Each row in `selected_battles_no_ties` represents a battle between model_a and model_b.\n",
    "\n",
    "2. Each battle contains `conv_metadata` with pre-computed style metrics, such as bold text counts, header counts, list counts, and token counts for each side.\n",
    "\n",
    "3. In the earlier question, each battle was converted to pairwise feature, where 1 denoted the model it belongs to, and -1 for the model that it was battling against. Now, our goal is to build on that data, including both the model identity indicators and the chosen style features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179c749",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UDiZ3-6ni4fw"
   },
   "source": [
    "## **Question 8a**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc3f8e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tRAbgVzvjd90"
   },
   "source": [
    "We want to add style features other than length that will give us style feature aware ranks.\n",
    "\n",
    "\n",
    "**Task:**\n",
    "Implement the function `add_style_features` that reads stylistic metrics from `conv_metadata` between each model (model_a, model_b) for count of bold, header, list, and assistant tokens. Then, store the normalized differences in columns (with the designated names):\n",
    "\n",
    "1. style_bold_count\n",
    "2. style_header_count\n",
    "3. style_list_count\n",
    "4. style_sum_assistant_tokens\n",
    "\n",
    "The normalized differences would be following the formulation below:\n",
    "\n",
    "$$\n",
    "\\text{normdiff}(a, b) =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } a + b = 0 \\\\[6pt]\n",
    "\\dfrac{a - b}{a + b} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbd028",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jwdMDnr9j-W-"
   },
   "source": [
    "Let's take a look at `conv_metadata`. Essentially, we will be stacking up these elements for each style count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee95388",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1754949540997,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2e3UXdX4jmoC",
    "outputId": "09c6667c-7ed0-46ff-dac2-4135707ea1bc"
   },
   "outputs": [],
   "source": [
    "selected_battles_no_ties['conv_metadata'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0eb3e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2454,
     "status": "ok",
     "timestamp": 1754949546102,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "fC8wAZV_WR4u",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def add_style_features(df):\n",
    "    \"\"\"\n",
    "    Adds normalized style feature difference columns to the DataFrame.\n",
    "    The columns added are:\n",
    "      - style_bold_count\n",
    "      - style_header_count\n",
    "      - style_list_count\n",
    "      - style_sum_assistant_tokens\n",
    "    \"\"\"\n",
    "    def normdiff(a, b):\n",
    "        denom = a + b\n",
    "        return 0 if denom == 0 else (a - b) / denom\n",
    "\n",
    "    ...\n",
    "    style_bold = []\n",
    "    style_header = []\n",
    "    style_list = []\n",
    "    style_tokens = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ...\n",
    "      \n",
    "    df[\"style_bold_count\"] = style_bold\n",
    "    df[\"style_header_count\"] = style_header\n",
    "    df[\"style_list_count\"] = style_list\n",
    "    df[\"style_sum_assistant_tokens\"] = style_tokens\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "selected_battles_no_ties = add_style_features(selected_battles_no_ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8644c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c69902",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## **Question 8a Free Response Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc83fb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We‚Äôve now added stylistic features to each model comparison.  \n",
    "\n",
    "**Answer the following question**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d7325",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "QUESTION: How can integrating these features into the ranking pipeline create the effect of a ‚Äúlength-controlled‚Äù leaderboard, and why might this adjustment be useful?  \n",
    "\n",
    "Think about whether raw win/loss outcomes fully capture model quality, or whether stylistic inflation (e.g., longer answers, formatting tricks) can bias rankings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d89539",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1b2b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "hXcAGuzmmyDI"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## **Question 8b**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de450df9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kGV8OzYEm_p_"
   },
   "source": [
    "Let's try to visualize and formulate the features we defined in the previous question in a neat way that we can see the direct relationship between the model battles and the style features.\n",
    "\n",
    "We now want a training table where each battle produces two rows, one for each ordering of the competitors (A‚ÜíB and B‚ÜíA).\n",
    "In other words, each row in the dataframe creates two entries in the new table.\n",
    "\n",
    "**Task:**\n",
    "Implement a function that creates the table described above. Each row should encode:\n",
    "1. The model identity vector **X** (+1 at the selected model in the row, ‚àí1 at its opponent, 0 elsewhere)\n",
    "2. The outcome y (win, lose)\n",
    "3. Set of style covariates capturing A vs B normalized differences (e.g., length)\n",
    "\n",
    "**NOTE:** Ensure that features are also antisymmetric, meaning they flipping the order of model should also flip the sign of each style feature.\n",
    "\n",
    "Below is an example of the desired table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902d7f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lOpyU8W4njiN"
   },
   "source": [
    "| question_id                         | X                                           | y | direction | style_bold_count | style_header_count | style_list_count | style_sum_assistant_tokens |\n",
    "|--------------------------------------|----------------------------------------------|---|-----------|------------------|--------------------|------------------|----------------------------|\n",
    "| e8fe7c9f75ab4e528367cc7de625c475     | [0, 0, 0, 0, 0, 1, ...]  | 0 | A->B      | 1.0              | 0.0                | 1.0              | 0.07717                    |\n",
    "| e8fe7c9f75ab4e528367cc7de625c475     | [0, 0, 0, 0, 0, -1 ...] | 1 | B->A      | -1.0             | -0.0               | -1.0             | -0.07717                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83957bc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "executionInfo": {
     "elapsed": 7180,
     "status": "ok",
     "timestamp": 1754949561831,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "A1KeK9x-jXqY",
    "outputId": "1f7aff2f-036e-46a7-c002-ee478e64bc87",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def make_pairwise_feature_df(df, models, style_feature_cols):\n",
    "    \"\"\"\n",
    "    For each row in df, create two rows in the output:\n",
    "      - One for A->B (original direction)\n",
    "      - One for B->A (flipped direction)\n",
    "    Each row contains:\n",
    "      - question_id\n",
    "      - X: model indicator vector (1 for model_a, -1 for model_b, 0 otherwise)\n",
    "      - y: 1 if model_a wins, 0 if model_b wins\n",
    "      - style features (from style_feature_cols)\n",
    "      - direction: \"A->B\" or \"B->A\"\n",
    "    Returns a new DataFrame with only these columns.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # X vector for A->B and B->A\n",
    "        ...\n",
    "        ...\n",
    "        # y for A->B and B->A\n",
    "        ...\n",
    "        ...\n",
    "        # Style features for A->B, B->A\n",
    "\n",
    "\n",
    "        # Add A->B, B->A\n",
    "        \n",
    "        \n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "style_feature_cols = [\n",
    "    \"style_bold_count\",\n",
    "    \"style_header_count\",\n",
    "    \"style_list_count\",\n",
    "    \"style_sum_assistant_tokens\"\n",
    "]\n",
    "\n",
    "pairwise_feature_df = make_pairwise_feature_df(selected_battles_no_ties, selected_models, style_feature_cols)\n",
    "pairwise_feature_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40704b1d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7000a78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uZfDJ9HK02zq"
   },
   "source": [
    "## **Question 8c**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864379b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yclx2ftl1N2t"
   },
   "source": [
    "Amazing! üéâ Now that we've built our pairwise identity matrix X (which model is battling which) and our style feature matrix X_style (how A and B differ stylistically), let's combine these two so that our logistic model can learn:\n",
    "\n",
    "1. The intrinsic strength of each model (controlling for style)\n",
    "2. The influence of each style feature on the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9977e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3wfxI8vk2K5i"
   },
   "source": [
    "**Task:**\n",
    "Now, implement the function `get_sc_category_results` that:\n",
    "1. Stacks these two matrices into one design matrix X_with_style, so the model can learn both intrinsic model strengths and style effects simultaneously.\n",
    "2. Uses `get_bootstrapped_score` to get the ranking results.\n",
    "3. Returns the results sorted by rank in ascending order. All style features should be assigned a rank of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a94ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 23804,
     "status": "ok",
     "timestamp": 1754949659157,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "quY8yXMcWR4v",
    "outputId": "4651256d-6b12-4f5f-ac3f-07521e7bb209",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_sc_category_results(df, selected_models, filter_mask = None, category_name=\"Overall w/ Style Control\", n_bootstrap=10, style_features=style_feature_cols):\n",
    "    #TODO\n",
    "    feature_labels = selected_models + style_features\n",
    "    ...\n",
    "\n",
    "    # rerank so the models are ranked and the style features are given a rank of -1\n",
    "    return results_df.sort_values(by=\"Rank\", ascending=True)\n",
    "results_df_style_control = get_sc_category_results(selected_battles_no_ties, selected_models, category_name=\"Overall w/ Style Control\", n_bootstrap=10)\n",
    "combined_results_df = pd.concat([results_df, results_df_style_control])\n",
    "fig = plot_rank_heatmap(combined_results_df, title=\"With and Without Style Control\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba9842",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76b413",
   "metadata": {
    "id": "Q4Ltt-gnWR4w"
   },
   "source": [
    "### Look at the impact of style\n",
    "\n",
    "Now let's visualize the style feature scores with confidence intervals using our premade `plot_style_features` in `plotting_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60257612",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 1896,
     "status": "ok",
     "timestamp": 1754949608819,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "2fEWg8eTWR4w",
    "outputId": "76784944-91da-484c-f5f7-ea63a0408c50"
   },
   "outputs": [],
   "source": [
    "# plot style feature coefficients\n",
    "fig = plot_style_features(results_df_style_control, selected_models)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5cf53a",
   "metadata": {},
   "source": [
    "Here we see that length matters a LOT (in fact this coefficient is higher than the actual model coefficients), while things like bold and lists don't matter as much. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bca54",
   "metadata": {
    "id": "mLoHnkfYWR4y"
   },
   "source": [
    "Here we can quickly see which models are \"Style hacking\" - formatting their responses nicely but not necessarily being more capable models. It looks like gpt-4o-mini and llama-3.1-70b-instruct see a consistent drop in rankings while models like claude 3.5 sonnet see a consistent rise in rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2555507f",
   "metadata": {},
   "source": [
    "# **Question 9: Finding New Style Influences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12cf387",
   "metadata": {
    "id": "P66AjSVwWR4z"
   },
   "source": [
    "\n",
    "Earlier, we saw how model preference differs by looking at structural style features in model outputs (e.g., bold text count, header count, list count, token length).\n",
    "Now let's see if we can find new style features by inspecting the model responses to understand differences in models.\n",
    "Let's inspect üîç the text ourselves for stylistic signals associated with wins. Your goal is to analyze assistant responses and identify phrases that differentiate winning from losing replies. This helps surface style features we might add to our ranking model later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa9f98",
   "metadata": {
    "id": "u09yRsQZWR4z"
   },
   "source": [
    "### Helper Functions\n",
    "Function to turn a conversations into plain text:\n",
    "*   convert_conversation_to_string\n",
    "*   convert_asst_conversation_to_string\n",
    "\n",
    "Function that compares two text with n-gram TF-IDF:\n",
    "*   tfidf_phrase_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f2d5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1754951655952,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "s2nTJLV9WR40"
   },
   "outputs": [],
   "source": [
    "def convert_conversation_to_string(conv):\n",
    "  ret = \"\"\n",
    "  for i in conv:\n",
    "    if i['role'] == 'user':\n",
    "      ret += \"User: \" + i['content'] + \"\\n\\n\"\n",
    "    else:\n",
    "      ret += \"Assistant: \" + i['content'] + \"\\n\\n\"\n",
    "  return ret\n",
    "\n",
    "def convert_asst_conversation_to_string(conv):\n",
    "  ret = \"\"\n",
    "  for i in conv:\n",
    "    if i['role'] == 'assistant':\n",
    "      ret += i['content'] + \"\\n\\n\"\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dae933",
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1754951658812,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "FwVhzubyWR40"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def is_number_phrase(phrase):\n",
    "    # Remove phrases that are only numbers or contain only numbers and spaces/punctuation\n",
    "    # Also remove phrases that are just a number or start/end with a number\n",
    "    return bool(re.fullmatch(r\"[\\d\\s\\W]+\", phrase)) or bool(re.search(r\"\\b\\d+\\b\", phrase))\n",
    "\n",
    "def tfidf_phrase_diff(str_list_a, str_list_b, name_a=\"A\", name_b=\"B\", top_n=30, max_features=1000):\n",
    "    \"\"\"\n",
    "    Compute distinguishing ngram tfidf phrases between two sets of strings.\n",
    "    Returns two DataFrames: one for phrases more common in A, one for B.\n",
    "    \"\"\"\n",
    "    all_texts = str_list_a + str_list_b\n",
    "    labels = [name_a] * len(str_list_a) + [name_b] * len(str_list_b)\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english', ngram_range=(2,4))\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    n = len(str_list_a)\n",
    "    tfidf_a = tfidf_matrix[:n]\n",
    "    tfidf_b = tfidf_matrix[n:]\n",
    "    mean_a = np.asarray(tfidf_a.mean(axis=0)).flatten()\n",
    "    mean_b = np.asarray(tfidf_b.mean(axis=0)).flatten()\n",
    "    # Top phrases for A\n",
    "    a_scores = mean_a - mean_b\n",
    "    top_a_indices = np.argsort(a_scores)[::-1]\n",
    "    top_a_phrases = []\n",
    "    for i in top_a_indices:\n",
    "        phrase = feature_names[i]\n",
    "        if not is_number_phrase(phrase):\n",
    "            top_a_phrases.append((phrase, mean_a[i], mean_b[i]))\n",
    "        if len(top_a_phrases) >= top_n:\n",
    "            break\n",
    "    # Top phrases for B\n",
    "    b_scores = mean_b - mean_a\n",
    "    top_b_indices = np.argsort(b_scores)[::-1]\n",
    "    top_b_phrases = []\n",
    "    for i in top_b_indices:\n",
    "        phrase = feature_names[i]\n",
    "        if not is_number_phrase(phrase):\n",
    "            top_b_phrases.append((phrase, mean_b[i], mean_a[i]))\n",
    "        if len(top_b_phrases) >= top_n:\n",
    "            break\n",
    "    df_a = pd.DataFrame(top_a_phrases, columns=[\"phrase\", f\"{name_a}_tfidf\", f\"{name_b}_tfidf\"])\n",
    "    df_b = pd.DataFrame(top_b_phrases, columns=[\"phrase\", f\"{name_b}_tfidf\", f\"{name_a}_tfidf\"])\n",
    "    return df_a, df_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea8e3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1dNGoK9MYX7Izqa0k9_lOTfmpOHoj2RZI"
    },
    "executionInfo": {
     "elapsed": 12412,
     "status": "ok",
     "timestamp": 1754952070139,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "iwrbq-cjWR41",
    "outputId": "343c3ae9-9d0b-482a-899c-ab2f7aa60a1f"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "model = \"llama-3.1-70b-instruct\"\n",
    "llama_battles = selected_battles_no_ties[\n",
    "    ((selected_battles_no_ties['model_a'] == model) | (selected_battles_no_ties['model_b'] == model)) & \n",
    "    (selected_battles_no_ties['language'] == 'English')\n",
    "].copy()\n",
    "\n",
    "llama_battles.loc[:, \"model_a_conversation_string\"] = llama_battles[\"conversation_a\"].apply(convert_asst_conversation_to_string)\n",
    "llama_battles.loc[:, \"model_b_conversation_string\"] = llama_battles[\"conversation_b\"].apply(convert_asst_conversation_to_string)\n",
    "\n",
    "str_list_a = llama_battles.apply(lambda x: x[\"model_a_conversation_string\"] if x[\"model_a\"] == model else x[\"model_b_conversation_string\"], axis=1).tolist()\n",
    "str_list_b = llama_battles.apply(lambda x: x[\"model_b_conversation_string\"] if x[\"model_a\"] == model else x[\"model_a_conversation_string\"], axis=1).tolist()\n",
    "# print(str_list_a)\n",
    "\n",
    "df_a, df_b = tfidf_phrase_diff(str_list_a, str_list_b, model, \"others\")\n",
    "\n",
    "print(f\"Top {model} phrases:\")\n",
    "# display(df_a)\n",
    "print(f\"Top others phrases:\")\n",
    "# display(df_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15260449",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dcmdwwAM_Dvo"
   },
   "source": [
    "# **Question 9a: Key Phrases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b827ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qn4owsN7_kjD"
   },
   "source": [
    "Let's analyze which phrases inherent in the text might be related to the win or lose of the battles. This is a similar idea to VibeCheck except (1) instead of comparing model pairs we are comparing winning vs losing models and (2) instead of using LLMs to propose and validate vibes, we are going to be relying on keyword matching. \n",
    "\n",
    "Using the helper functions provided (`convert_asst_conversation_to_string`, `tfidf_phrase_diff`) and the reference example as guidance, implement a winning-vs-losing phrase analysis for assistant responses.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Task:** \n",
    "\n",
    "1. From selected_battles_no_ties, keep only rows in English.\n",
    "\n",
    "2. For each battle, extract assistant-only text using convert_asst_conversation_to_string.\n",
    "\n",
    "3. Build winning_responses: one assistant-only string for the winning side of each battle.\n",
    "\n",
    "4. Build losing_responses: one assistant-only string for the losing side of each battle.\n",
    "\n",
    "5. Use tfidf_phrase_diff to compare the two lists and construct df_win and df_lose. (we have set this up for you)\n",
    "\n",
    "6. Display the results to see the phrases most associated with winning vs. losing. (we have set this up for you)\n",
    "\n",
    "For your reference, a sample subset of outputs is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94bc89",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Example: Top phrases in *winning* responses\n",
    "| phrase              | winning_tfidf | losing_tfidf |\n",
    "|---------------------|---------------|--------------|\n",
    "| let break           | 0.0127        | 0.0106       |\n",
    "| step step           | 0.0146        | 0.0126       |\\\n",
    "| ... |...       | ...     |\n",
    "\n",
    "### Example: Top phrases in *losing* responses\n",
    "| phrase                 | losing_tfidf | winning_tfidf |\n",
    "|------------------------|--------------|----------------|\n",
    "| let know               | 0.0278       | 0.0190         |\n",
    "| sorry assist           | 0.0054       | 0.0003         |\n",
    "| ...     | ...      | ...  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b998c99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 136950,
     "status": "ok",
     "timestamp": 1754953237487,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "rbBqPMiZWR41",
    "outputId": "c614b2d5-e369-49b2-facd-c6eb1fbf27ee",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TFIDF comparing winning responses to losing responses\n",
    "...\n",
    "# 1) Restrict to English Only and prepare assistant-only strings for A/B\n",
    "# Use 'convert_asst_conversation_to_string' helper function\n",
    "selected_battles_english = selected_battles_no_ties[selected_battles_no_ties['language'] == 'English']\n",
    "...\n",
    "\n",
    "# 2) Build lists of assistant-only winning/losing responses\n",
    "winning_responses = selected_battles_english.apply(...).tolist()\n",
    "\n",
    "losing_responses = selected_battles_english.apply(...).tolist()\n",
    "\n",
    "# 3) Compare phrases\n",
    "df_win, df_lose = tfidf_phrase_diff(winning_responses, losing_responses, \"winning\", \"losing\")\n",
    "# 4) Show results\n",
    "print(\"Top winning response phrases:\")\n",
    "# display(df_win)\n",
    "print(\"Top losing response phrases:\")\n",
    "# display(df_lose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23288a07",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050d061c",
   "metadata": {},
   "source": [
    "## Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c861",
   "metadata": {
    "id": "64hdr6bMA6li"
   },
   "source": [
    "\n",
    "New let's look at how we can add in a new style feature. Below we have an example of adding a `refusal_count` feature. Phrases like \"i'm sorry\" or \"i apologize\" appear more often in losing models - when looking through these conversations you will see that these are often instances of **refusal**: where the model refuses to answer the question because it violates ethical guidelines or is out of its domain of knowledge. Now let's turn this into a style feature to measure its impact on accuracy.\n",
    "\n",
    "\n",
    "Let's try capturing whether the assistant on side A which apologizes more than side B in a battle by calculating the normalized sorry_count_diff. Here we will just have a binary 1/0 for each conversation indicating if it contains or does not contain the word \"sorry\" or \"apologize\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cddcb",
   "metadata": {
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1754953063300,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "kfTRLsXsWR42"
   },
   "outputs": [],
   "source": [
    "def count_phrase_diff(row, phrase=[\"step by step\"]):\n",
    "    step_by_step_a = False\n",
    "    step_by_step_b = False\n",
    "    for i in row[\"conversation_a\"]:\n",
    "        if i[\"role\"] == \"assistant\" and any([p in i[\"content\"].lower().replace(\"-\", \" \") for p in phrase]):\n",
    "            step_by_step_a = True\n",
    "    for i in row[\"conversation_b\"]:\n",
    "        if i[\"role\"] == \"assistant\" and any([p in i[\"content\"].lower().replace(\"-\", \" \") for p in phrase]):\n",
    "            step_by_step_b = True\n",
    "    return int(step_by_step_a) - int(step_by_step_b)\n",
    "\n",
    "\n",
    "selected_battles_no_ties.loc[:, \"refusal_count\"] = selected_battles_no_ties.apply(\n",
    "    lambda row: count_phrase_diff(row, [\"sorry\", \"apologize\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f8ea4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ys5ga1dCW2SM"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# **Question 9b: Discover Some Immaculate Vibes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74132817",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** \n",
    "Now, just like the `refusal_count` feature we created above, implement new functions that can extract any stylistic features from the conversations.  \n",
    "- Define a function that computes the normalized difference for a feature of your choice (e.g., presence of certain phrases, punctuation, formatting). You can check multiple different phrases if you want, they just to have a common \"theme\" - similar to the last problem of the previous part of this homework.   \n",
    "- Apply this function to each row in the dataset.  \n",
    "- Store the results in a new column of `selected_battles_no_ties[\"YOUR_FEATURE\"]`. \n",
    "- Plot the change in ranking and style coefficients and the existing style features along with your custom feature. **To get full points, your feature needs to get a higher magnitude coefficient [abs(Average Score)] than `style_header_count`**. It is okay if the confidence intervals overlap or if your coeffieinct is negative, as long as the absolute value is higher. \n",
    "- You cannot use a feature already explored or anything similar (e.g. you can't have an \"I refuse\" style feature or a word count style feature). \n",
    "\n",
    "This is open ended, you don't need to use the features you found above, get creative with it! Heck, you can even throw response pairs into your LLM of choice and ask it to come up with differences just like VibeCheck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124abc8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 14908,
     "status": "ok",
     "timestamp": 1754953532614,
     "user": {
      "displayName": "Terry Kim",
      "userId": "03457895093288389637"
     },
     "user_tz": 420
    },
    "id": "i386MD7lArDR",
    "outputId": "1c2be5a3-4c95-41c2-9a5a-fb97107c0a1e",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Design your own stylistic feature(s) that compare model A vs B on each row.\n",
    "# Keep it simple (boolean presence, counts, or normalized differences) or get creative.\n",
    "\n",
    "# Remember might want to normalize difference, if you compute counts\n",
    "\n",
    "# TODO: Define your feature function\n",
    "def YOUR_FUNC(row):\n",
    "    #Input: a row with 'conversation_a' and 'conversation_b' (each is a list of {role, content} dicts).\n",
    "    #Output: a single numeric feature comparing A vs B (e.g., -1/0/1, count diff, or normalized diff).\n",
    "    #Hint: You probably want to inspect only assistant messages.\n",
    "    # Example (placeholder): return 0\n",
    "    return ...\n",
    "\n",
    "# TODO: Apply your function to create a new column\n",
    "# selected_battles_no_ties.loc[:, \"YOUR_FEATURE\"] = selected_battles_no_ties.apply(YOUR_FUNC, axis=1)\n",
    "\n",
    "# Choose which style features to control for in ranking.\n",
    "# Start from this list and add yours below.\n",
    "style_feature_cols = [\n",
    "    \"style_bold_count\",\n",
    "    \"style_header_count\",\n",
    "    \"style_list_count\",\n",
    "    \"style_sum_assistant_tokens\",\n",
    "    \"refusal_count\",\n",
    "    # \"YOUR_FEATURE\",   # <- uncomment after you create it\n",
    "]\n",
    "\n",
    "combined_results_df_new = get_sc_category_results(selected_battles_no_ties,\n",
    "                                 selected_models, \n",
    "                                 category_name=\"Overall w/ Style Control\", \n",
    "                                 n_bootstrap=10, \n",
    "                                 style_features=style_feature_cols)\n",
    "\n",
    "fig = plot_rank_heatmap(pd.concat([results_df, combined_results_df_new]), title=\"With and Without Style Control (New Features)\", selected_models=selected_models)\n",
    "fig.show()\n",
    "\n",
    "# Create and display the plot\n",
    "fig = plot_style_features(combined_results_df_new, selected_models)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d3977a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# **Question 9c: Reflection**\n",
    "\n",
    "**Answer the following questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd172c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "```otter\n",
    "9c-1. Why did you decide to use the stylistic feature that you have implemented for Q9b? What ranking changes across models did you see when looking at the new style features you created? Why do you think that is the case?\n",
    "\n",
    "9c-2. Why might some models overuse or underuse stylistic markers (e.g., exclamation points, apologies, or explicit reasoning phrases), and how could that influence rankings?\n",
    "\n",
    "9c-3. How does including these stylistic features help control for length or formatting effects when building leaderboards?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6cb66b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```otter\n",
    "YOUR ANSWER 9c-1: Replace This\n",
    "YOUR ANSWER 9c-2: Replace This\n",
    "YOUR ANSWER 9c-3: Replace This\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c64f4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece2186",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell if you are running the notebook in Google Colab to install the necessary dependencies, this may take a few minutes\n",
    "if IS_COLAB:\n",
    "    !apt-get install -y texlive texlive-xetex pandoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d7aca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bea8b6",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "cs189",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q4a": {
     "name": "q4a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_test, y_test = turn_into_features(selected_battles_no_ties, selected_models)\n>>> expected_rows = 2 * len(selected_battles_no_ties)\n>>> expected_cols = len(selected_models)\n>>> assert X_test.shape == (expected_rows, expected_cols), f'shape of X does not match ({expected_rows}, {expected_cols})'\n>>> assert y_test.shape == (expected_rows,), f'shape of y does not match ({expected_rows},)'\n>>> assert len(X) == 2 * len(selected_battles_no_ties), 'Each battle should generate exactly 2 examples'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert results_df.shape == (20, 2), 'result_df shape does not match (20,2)'\n>>> assert list(results_df.columns) == ['Model', 'Score'], 'results_df does not match column: [Model, Score]'\n>>> scores = results_df['Score'].to_numpy()\n>>> assert np.all(np.diff(scores) <= 1e-12), 'Scores must be sorted in descending order.'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5a": {
     "name": "q5a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_get_bootstrapped_score_outputs(X, y, selected_models):\n...     \"\"\"\n...       - Run get_bootstrapped_score on provided (X, y, selected_models).\n...       - Check shapes of outputs.\n...       - Check that results_df has expected columns and type.\n...       - Check that confidence intervals are valid (LB <= UB).\n...     \"\"\"\n...     np.random.seed(189)\n...     results_df, mean_scores, confidence_intervals = get_bootstrapped_score(X, y, selected_models, n_bootstrap=10)\n...     assert isinstance(results_df, pd.DataFrame), 'results_df should be a DataFrame.'\n...     for c in ['Model', 'Average Score', 'Lower Bound', 'Upper Bound', 'Category']:\n...         assert c in results_df.columns, f'Missing column: {c}'\n...     n_models = len(selected_models)\n...     assert results_df.shape[0] == n_models, f'Expected {n_models} rows in results_df, got {results_df.shape[0]}'\n...     assert mean_scores.shape == (n_models,), f'mean_scores must be shape ({n_models},)'\n...     assert confidence_intervals.shape == (2, n_models), f'confidence_intervals must be (2, {n_models})'\n...     lb, ub = confidence_intervals\n...     assert (lb <= ub).all(), 'Every lower bound must be <= upper bound.'\n...     assert ((lb <= mean_scores) & (mean_scores <= ub)).all(), 'Each mean score should lie within its CI.'\n>>> test_get_bootstrapped_score_outputs(X, y, selected_models)\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5b": {
     "name": "q5b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert 'results_df' in globals(), 'Expected a DataFrame named `results_df`.'\n>>> required_cols = {'Model', 'Lower Bound', 'Upper Bound', 'Rank', 'Category'}\n>>> assert required_cols.issubset(results_df.columns), f'Missing columns: {required_cols - set(results_df.columns)}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7a": {
     "name": "q7a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> conv_one_assistant = {'conversation': [{'role': 'user', 'content': 'Hello'}, {'role': 'assistant', 'content': 'Hi there!'}]}\n>>> assert calculate_response_length(conv_one_assistant) == 3, 'error1: the joining did not work properly for one assistant'\n>>> conv_special = {'conversation': [{'role': 'assistant', 'content': 'Here is <|endoftext|> special'}]}\n>>> assert calculate_response_length(conv_special) == 10, 'the special token raises an error'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7b": {
     "name": "q7b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(per_model_battles.columns) == ['conversation', 'model', 'response_length'], 'does not have expected columns: [conversation, model, response_length]'\n>>> assert per_model_battles.shape[0] == 2 * len(selected_battles_no_ties), 'per_model_battles.shape[0] does not match expected (twice of original)'\n>>> assert (per_model_battles['response_length'] >= 0).all(), 'negative or undefined response_length'\n>>> assert np.issubdtype(per_model_battles['response_length'].dtype, np.integer), 'dtype or response_length has to be np.integer'\n",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> def test_per_model_battles_expected_solution():\n...     expected_df = pd.DataFrame({'conversation': pd.concat([selected_battles_no_ties['conversation_a'], selected_battles_no_ties['conversation_b']], ignore_index=True), 'model': pd.concat([selected_battles_no_ties['model_a'], selected_battles_no_ties['model_b']], ignore_index=True)})\n...     expected_df['response_length'] = expected_df['conversation'].map(calculate_response_length).astype(int)\n...     expected_df = expected_df[['conversation', 'model', 'response_length']].reset_index(drop=True)\n...     student_df = per_model_battles[['conversation', 'model', 'response_length']].reset_index(drop=True)\n...     def make_sortable_key(row):\n...         conv_str = str(row['conversation']) if hasattr(row['conversation'], '__iter__') else str(row['conversation'])\n...         return (conv_str, row['model'], row['response_length'])\n...     expected_keys = [make_sortable_key(row) for _, row in expected_df.iterrows()]\n...     student_keys = [make_sortable_key(row) for _, row in student_df.iterrows()]\n...     expected_keys.sort()\n...     student_keys.sort()\n...     assert len(student_keys) == len(expected_keys), f'Length mismatch: expected {len(expected_keys)}, got {len(student_keys)}'\n...     for i, (student_key, expected_key) in enumerate(zip(student_keys, expected_keys)):\n...         assert student_key == expected_key, f'Row {i} mismatch after sorting'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8a": {
     "name": "q8a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_cols = {'style_bold_count', 'style_header_count', 'style_list_count', 'style_sum_assistant_tokens'}\n>>> missing = expected_cols - set(selected_battles_no_ties.columns)\n>>> assert not missing, f'Missing expected style feature columns: {missing}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8b": {
     "name": "q8b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> expected_rows = 2 * len(selected_battles_no_ties)\n>>> expected_num_cols = 4 + len(style_feature_cols)\n>>> assert pairwise_feature_df.shape == (expected_rows, expected_num_cols), f'Expected ({expected_rows}, {expected_num_cols}), got {pairwise_feature_df.shape}'\n>>> expected_cols = {'question_id', 'X', 'y', 'direction', *style_feature_cols}\n>>> assert set(pairwise_feature_df.columns) == expected_cols, 'Unexpected columns in output.'\n>>> for d in pairwise_feature_df['direction'].unique():\n...     assert d in ('A->B', 'B->A'), f'Unexpected direction: {d}'\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8c": {
     "name": "q8c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert list(results_df_style_control.columns) == ['Model', 'Category', 'Average Score', 'Lower Bound', 'Upper Bound', 'Rank'], 'results_df_style_control columns incorrect.'\n>>> assert list(combined_results_df.columns) == ['Model', 'Category', 'Average Score', 'Lower Bound', 'Upper Bound', 'Rank'], 'combined_results_df columns incorrect.'\n>>> n_models = 20\n>>> feature_labels = list(results_df['Model'].unique()) + ['style_sum_assistant_tokens', 'style_bold_count', 'style_header_count', 'style_list_count']\n>>> assert len(combined_results_df['Model'].unique()) == n_models + 4, f\"combined_results_df should contain exactly the models from results_df. ({n_models} models expected, got {len(combined_results_df['Model'].unique())})\"\n>>> assert len(combined_results_df) == n_models * 2 + 4, f'combined_results_df should contain {n_models * 2 + 4} rows, got {len(combined_results_df)}'\n>>> style_features = ['style_sum_assistant_tokens', 'style_bold_count', 'style_header_count', 'style_list_count']\n>>> assert np.all(combined_results_df.loc[combined_results_df['Model'].isin(style_features), 'Rank'].values.astype(int) == -1), 'Rank should be -1 for style features'\n",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9a": {
     "name": "q9a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> def test_winning_losing_phrase_tfidf(english_count, winning_responses, losing_responses, df_win, df_lose):\n...     assert len(winning_responses) == english_count, 'winning_responses length should match number of English battles.'\n...     assert len(losing_responses) == english_count, 'losing_responses length should match number of English battles.'\n...     assert all((isinstance(s, str) for s in winning_responses)), 'winning_responses must be strings.'\n...     assert all((isinstance(s, str) for s in losing_responses)), 'losing_responses must be strings.'\n...     if len(winning_responses) > 0:\n...         assert any((len(s.strip()) > 0 for s in winning_responses)), 'winning_responses seem empty.'\n...         assert any((len(s.strip()) > 0 for s in losing_responses)), 'losing_responses seem empty.'\n...     assert list(df_win.columns) == ['phrase', 'winning_tfidf', 'losing_tfidf'], \"df_win must have columns ['phrase','winning_tfidf','losing_tfidf'].\"\n...     assert list(df_lose.columns) == ['phrase', 'losing_tfidf', 'winning_tfidf'], \"df_lose must have columns ['phrase','losing_tfidf','winning_tfidf'].\"\n...     assert df_win['phrase'].map(lambda x: isinstance(x, str)).all(), 'df_win phrases must be strings.'\n...     assert df_lose['phrase'].map(lambda x: isinstance(x, str)).all(), 'df_lose phrases must be strings.'\n...     for col in ['winning_tfidf', 'losing_tfidf']:\n...         assert pd.to_numeric(df_win[col], errors='coerce').notna().all(), f'df_win.{col} must be numeric.'\n...         assert (df_win[col] >= 0).all(), f'df_win.{col} must be non-negative.'\n...     for col in ['losing_tfidf', 'winning_tfidf']:\n...         assert pd.to_numeric(df_lose[col], errors='coerce').notna().all(), f'df_lose.{col} must be numeric.'\n...         assert (df_lose[col] >= 0).all(), f'df_lose.{col} must be non-negative.'\n...     a, b = tfidf_phrase_diff(winning_responses, losing_responses, 'winning', 'losing', top_n=10, max_features=500)\n...     assert list(a.columns) == list(df_win.columns), 'Re-run df_win columns mismatch.'\n...     assert list(b.columns) == list(df_lose.columns), 'Re-run df_lose columns mismatch.'\n>>> test_winning_losing_phrase_tfidf(len(selected_battles_no_ties[selected_battles_no_ties['language'] == 'English']), winning_responses, losing_responses, df_win, df_lose)\n",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
